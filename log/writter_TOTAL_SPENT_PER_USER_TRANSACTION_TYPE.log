25/05/02 08:34:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 08:34:05,997 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Lancement de l'application Spark Streaming...
25/05/02 08:34:06 INFO SparkContext: Running Spark version 3.5.0
25/05/02 08:34:06 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 08:34:06 INFO SparkContext: Java version 11.0.24
25/05/02 08:34:06 INFO ResourceUtils: ==============================================================
25/05/02 08:34:06 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 08:34:06 INFO ResourceUtils: ==============================================================
25/05/02 08:34:06 INFO SparkContext: Submitted application: KafkaConsumer
25/05/02 08:34:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 08:34:06 INFO ResourceProfile: Limiting resource is cpu
25/05/02 08:34:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 08:34:06 INFO SecurityManager: Changing view acls to: root
25/05/02 08:34:06 INFO SecurityManager: Changing modify acls to: root
25/05/02 08:34:06 INFO SecurityManager: Changing view acls groups to: 
25/05/02 08:34:06 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 08:34:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 08:34:07 INFO Utils: Successfully started service 'sparkDriver' on port 34563.
25/05/02 08:34:07 INFO SparkEnv: Registering MapOutputTracker
25/05/02 08:34:07 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 08:34:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 08:34:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 08:34:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 08:34:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1f0035a9-6623-4876-95de-4c38637bd1b0
25/05/02 08:34:07 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 08:34:07 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 08:34:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 08:34:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/05/02 08:34:08 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://505ebd009d1d:34563/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746174846128
25/05/02 08:34:08 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://505ebd009d1d:34563/jars/kafka-clients-3.3.1.jar with timestamp 1746174846128
25/05/02 08:34:08 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://505ebd009d1d:34563/jars/commons-pool2-2.11.1.jar with timestamp 1746174846128
25/05/02 08:34:08 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://505ebd009d1d:34563/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746174846128
25/05/02 08:34:08 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://505ebd009d1d:34563/jars/postgresql-42.2.23.jar with timestamp 1746174846128
25/05/02 08:34:08 INFO Executor: Starting executor ID driver on host 505ebd009d1d
25/05/02 08:34:08 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 08:34:08 INFO Executor: Java version 11.0.24
25/05/02 08:34:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 08:34:08 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@e246d94 for default.
25/05/02 08:34:08 INFO Executor: Fetching spark://505ebd009d1d:34563/jars/kafka-clients-3.3.1.jar with timestamp 1746174846128
25/05/02 08:34:08 INFO TransportClientFactory: Successfully created connection to 505ebd009d1d/172.21.0.11:34563 after 96 ms (0 ms spent in bootstraps)
25/05/02 08:34:08 INFO Utils: Fetching spark://505ebd009d1d:34563/jars/kafka-clients-3.3.1.jar to /tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/fetchFileTemp16725233071684730384.tmp
25/05/02 08:34:09 INFO Executor: Adding file:/tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/kafka-clients-3.3.1.jar to class loader default
25/05/02 08:34:09 INFO Executor: Fetching spark://505ebd009d1d:34563/jars/postgresql-42.2.23.jar with timestamp 1746174846128
25/05/02 08:34:09 INFO Utils: Fetching spark://505ebd009d1d:34563/jars/postgresql-42.2.23.jar to /tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/fetchFileTemp17624030590447770697.tmp
25/05/02 08:34:09 INFO Executor: Adding file:/tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/postgresql-42.2.23.jar to class loader default
25/05/02 08:34:09 INFO Executor: Fetching spark://505ebd009d1d:34563/jars/commons-pool2-2.11.1.jar with timestamp 1746174846128
25/05/02 08:34:09 INFO Utils: Fetching spark://505ebd009d1d:34563/jars/commons-pool2-2.11.1.jar to /tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/fetchFileTemp8520554733184158598.tmp
25/05/02 08:34:09 INFO Executor: Adding file:/tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/commons-pool2-2.11.1.jar to class loader default
25/05/02 08:34:09 INFO Executor: Fetching spark://505ebd009d1d:34563/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746174846128
25/05/02 08:34:09 INFO Utils: Fetching spark://505ebd009d1d:34563/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/fetchFileTemp1601722584678469742.tmp
25/05/02 08:34:09 INFO Executor: Adding file:/tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 08:34:09 INFO Executor: Fetching spark://505ebd009d1d:34563/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746174846128
25/05/02 08:34:09 INFO Utils: Fetching spark://505ebd009d1d:34563/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/fetchFileTemp13742876415698616777.tmp
25/05/02 08:34:09 INFO Executor: Adding file:/tmp/spark-d2cb45c0-cb8a-4ca7-8bd6-194d361bee7a/userFiles-0834fef5-7f93-451c-a7da-3974b9ca0dae/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 08:34:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39997.
25/05/02 08:34:09 INFO NettyBlockTransferService: Server created on 505ebd009d1d:39997
25/05/02 08:34:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 08:34:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 505ebd009d1d, 39997, None)
25/05/02 08:34:09 INFO BlockManagerMasterEndpoint: Registering block manager 505ebd009d1d:39997 with 1007.8 MiB RAM, BlockManagerId(driver, 505ebd009d1d, 39997, None)
25/05/02 08:34:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 505ebd009d1d, 39997, None)
25/05/02 08:34:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 505ebd009d1d, 39997, None)
2025-05-02 08:34:10,225 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Session Spark créée.
2025-05-02 08:34:10,226 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Schéma du message défini pour les données flattened.
2025-05-02 08:34:10,226 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic 'transaction_log'.
25/05/02 08:34:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 08:34:10 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 08:34:13,919 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Connexion à Kafka réussie. Lecture des messages en streaming.
2025-05-02 08:34:15,296 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Transformation JSON des messages terminée. Schéma résultant :
root
 |-- KSQL_COL_0: string (nullable = true)
 |-- TOTAL_SPENT: double (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

25/05/02 08:34:15 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 08:34:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 08:34:15 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28 resolved to file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28.
25/05/02 08:34:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 08:34:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/metadata using temp file file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/.metadata.cfffb67e-5b66-4679-b54a-cf75b4bf862a.tmp
25/05/02 08:34:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/.metadata.cfffb67e-5b66-4679-b54a-cf75b4bf862a.tmp to file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/metadata
25/05/02 08:34:16 INFO MicroBatchExecution: Starting [id = 164a455a-1f44-4204-8889-91f3a1af759c, runId = db32d9b8-c2f5-4cb2-ae6d-5af27da8b1a5]. Use file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28 to store the query checkpoint.
2025-05-02 08:34:16,247 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 08:34:16,247 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Initialisation de l'écriture en Parquet...
25/05/02 08:34:16 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6d7cb770] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@16c5fe23]
25/05/02 08:34:16 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 08:34:16 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b resolved to file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b.
25/05/02 08:34:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 08:34:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/metadata using temp file file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/.metadata.a454b034-2a49-4823-b24b-580820a351b7.tmp
25/05/02 08:34:16 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 08:34:16 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 08:34:16 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 08:34:16 INFO MicroBatchExecution: Stream started from {}
25/05/02 08:34:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/.metadata.a454b034-2a49-4823-b24b-580820a351b7.tmp to file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/metadata
25/05/02 08:34:16 INFO MicroBatchExecution: Starting [id = 112de11a-a714-4cda-8d97-b6b11f6cb530, runId = c5cd319d-8020-4fe4-a288-dea023547618]. Use file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b to store the query checkpoint.
25/05/02 08:34:16 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6d7cb770] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@16c5fe23]
25/05/02 08:34:16 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 08:34:16 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 08:34:16 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 08:34:16 INFO MicroBatchExecution: Stream started from {}
25/05/02 08:34:17 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 08:34:17 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 08:34:18 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 08:34:18 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 08:34:18 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 08:34:18 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 08:34:18 INFO AppInfoParser: Kafka startTimeMs: 1746174858095
25/05/02 08:34:18 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 08:34:18 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 08:34:18 INFO AppInfoParser: Kafka startTimeMs: 1746174858095
25/05/02 08:34:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/sources/0/0 using temp file file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/sources/0/.0.62ef87de-e4ed-4559-88a4-a5fcbc7cd01e.tmp
25/05/02 08:34:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/sources/0/0 using temp file file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/sources/0/.0.97cdf1f9-a7e2-4bfb-8c31-a44678ecdd1e.tmp
25/05/02 08:34:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/sources/0/.0.97cdf1f9-a7e2-4bfb-8c31-a44678ecdd1e.tmp to file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/sources/0/0
25/05/02 08:34:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/sources/0/.0.62ef87de-e4ed-4559-88a4-a5fcbc7cd01e.tmp to file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/sources/0/0
25/05/02 08:34:19 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_SPENT_PER_USER_TRANSACTION_TYPE":{"0":0}}
25/05/02 08:34:19 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_SPENT_PER_USER_TRANSACTION_TYPE":{"0":0}}
25/05/02 08:34:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/offsets/0 using temp file file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/offsets/.0.274b6804-2025-45d0-964a-de712f259ceb.tmp
25/05/02 08:34:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/offsets/0 using temp file file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/offsets/.0.229e9b2b-bdb7-49b5-a0fa-346c21cc3dfd.tmp
25/05/02 08:34:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/offsets/.0.274b6804-2025-45d0-964a-de712f259ceb.tmp to file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/offsets/0
25/05/02 08:34:19 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746174859205,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 08:34:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/offsets/.0.229e9b2b-bdb7-49b5-a0fa-346c21cc3dfd.tmp to file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/offsets/0
25/05/02 08:34:19 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746174859205,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 08:34:20 INFO IncrementalExecution: Current batch timestamp = 1746174859205
25/05/02 08:34:20 INFO IncrementalExecution: Current batch timestamp = 1746174859205
25/05/02 08:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:21 INFO IncrementalExecution: Current batch timestamp = 1746174859205
25/05/02 08:34:21 INFO IncrementalExecution: Current batch timestamp = 1746174859205
25/05/02 08:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:22 INFO IncrementalExecution: Current batch timestamp = 1746174859205
25/05/02 08:34:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 08:34:23 INFO CodeGenerator: Code generated in 812.220106 ms
25/05/02 08:34:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]]. The input RDD has 1 partitions.
25/05/02 08:34:24 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 08:34:24 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/05/02 08:34:24 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/05/02 08:34:24 INFO DAGScheduler: Parents of final stage: List()
25/05/02 08:34:24 INFO DAGScheduler: Missing parents: List()
25/05/02 08:34:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at start at <unknown>:0), which has no missing parents
25/05/02 08:34:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 22.4 KiB, free 1007.8 MiB)
25/05/02 08:34:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 1007.8 MiB)
25/05/02 08:34:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 505ebd009d1d:39997 (size: 10.4 KiB, free: 1007.8 MiB)
25/05/02 08:34:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/05/02 08:34:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 08:34:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/05/02 08:34:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (505ebd009d1d, executor driver, partition 0, PROCESS_LOCAL, 9086 bytes) 
25/05/02 08:34:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/05/02 08:34:26 INFO CodeGenerator: Code generated in 92.325003 ms
25/05/02 08:34:26 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 08:34:26 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
25/05/02 08:34:26 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
25/05/02 08:34:26 INFO DAGScheduler: Parents of final stage: List()
25/05/02 08:34:26 INFO DAGScheduler: Missing parents: List()
25/05/02 08:34:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at start at <unknown>:0), which has no missing parents
25/05/02 08:34:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 35.4 KiB, free 1007.8 MiB)
25/05/02 08:34:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 1007.8 MiB)
25/05/02 08:34:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 505ebd009d1d:39997 (size: 15.9 KiB, free: 1007.8 MiB)
25/05/02 08:34:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
25/05/02 08:34:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 08:34:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/05/02 08:34:27 INFO CodeGenerator: Code generated in 108.104822 ms
25/05/02 08:34:27 INFO CodeGenerator: Code generated in 83.345279 ms
25/05/02 08:34:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 fromOffset=0 untilOffset=130, for query queryId=164a455a-1f44-4204-8889-91f3a1af759c batchId=0 taskId=0 partitionId=0
25/05/02 08:34:27 INFO CodeGenerator: Code generated in 25.449109 ms
25/05/02 08:34:27 INFO CodeGenerator: Code generated in 91.659828 ms
25/05/02 08:34:27 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 08:34:27 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 08:34:27 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 08:34:27 INFO AppInfoParser: Kafka startTimeMs: 1746174867828
25/05/02 08:34:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor-1, groupId=spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor] Assigned to partition(s): TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 08:34:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor-1, groupId=spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor] Seeking to offset 0 for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 08:34:27 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor-1, groupId=spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor] Resetting the last seen epoch of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to 0 since the associated topicId changed from null to 1NA8zRCeQuCDE7KOavBGoQ
25/05/02 08:34:27 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor-1, groupId=spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor] Cluster ID: M7rQ1QLCSVqd8VDtHXyJXQ
25/05/02 08:34:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor-1, groupId=spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor] Seeking to earliest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 08:34:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor-1, groupId=spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 08:34:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor-1, groupId=spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor] Seeking to latest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 08:34:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor-1, groupId=spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=130, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 08:34:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 08:34:28 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/05/02 08:34:28 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 groupId=spark-kafka-source-31241c9a-df47-45a1-afde-3b1b67d4b415-2023498268-executor read 130 records through 1 polls (polled  out 130 records), taking 640180477 nanos, during time span of 1154255687 nanos.
25/05/02 08:34:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 9542 bytes result sent to driver
25/05/02 08:34:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (505ebd009d1d, executor driver, partition 0, PROCESS_LOCAL, 9087 bytes) 
25/05/02 08:34:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
25/05/02 08:34:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3389 ms on 505ebd009d1d (executor driver) (1/1)
25/05/02 08:34:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/05/02 08:34:29 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 4.725 s
25/05/02 08:34:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 08:34:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/05/02 08:34:29 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 5.194074 s
25/05/02 08:34:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/02 08:34:29 INFO CodeGenerator: Code generated in 83.486244 ms
25/05/02 08:34:29 INFO CodeGenerator: Code generated in 9.949379 ms
25/05/02 08:34:33 INFO CodeGenerator: Code generated in 15.522432 ms
25/05/02 08:34:33 INFO CodeGenerator: Code generated in 88.623619 ms
25/05/02 08:34:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 fromOffset=0 untilOffset=130, for query queryId=112de11a-a714-4cda-8d97-b6b11f6cb530 batchId=0 taskId=1 partitionId=0
+----------+------------------+-----------------------+--------------+
|KSQL_COL_0|TOTAL_SPENT       |ingestion_time         |ingestion_date|
+----------+------------------+-----------------------+--------------+
|NULL      |746.9712000000001 |2025-05-02 08:34:19.205|2025-05-02    |
|NULL      |169.48499999999999|2025-05-02 08:34:19.205|2025-05-02    |
|NULL      |1.14499           |2025-05-02 08:34:19.205|2025-05-02    |
|NULL      |122.655           |2025-05-02 08:34:19.205|2025-05-02    |
|NULL      |448.33799999999997|2025-05-02 08:34:19.205|2025-05-02    |
|NULL      |51.59             |2025-05-02 08:34:19.205|2025-05-02    |
|NULL      |245.83020000000002|2025-05-02 08:34:19.205|2025-05-02    |
|NULL      |5.75659           |2025-05-02 08:34:19.205|2025-05-02    |
|NULL      |119.15            |2025-05-02 08:34:19.205|2025-05-02    |
|NULL      |941.5764000000001 |2025-05-02 08:34:19.205|2025-05-02    |
+----------+------------------+-----------------------+--------------+
only showing top 10 rows

25/05/02 08:34:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] committed.
25/05/02 08:34:33 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 08:34:33 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 08:34:33 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 08:34:33 INFO AppInfoParser: Kafka startTimeMs: 1746174873296
25/05/02 08:34:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor-2, groupId=spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor] Assigned to partition(s): TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 08:34:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor-2, groupId=spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor] Seeking to offset 0 for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 08:34:33 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor-2, groupId=spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor] Resetting the last seen epoch of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to 0 since the associated topicId changed from null to 1NA8zRCeQuCDE7KOavBGoQ
25/05/02 08:34:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/commits/0 using temp file file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/commits/.0.e5c1d29c-0abd-4f37-a597-e328e56fee4a.tmp
25/05/02 08:34:33 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor-2, groupId=spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor] Cluster ID: M7rQ1QLCSVqd8VDtHXyJXQ
25/05/02 08:34:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor-2, groupId=spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor] Seeking to earliest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 08:34:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/commits/.0.e5c1d29c-0abd-4f37-a597-e328e56fee4a.tmp to file:/tmp/temporary-08912a8b-ea57-4142-b934-e5f75ce36b28/commits/0
25/05/02 08:34:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "164a455a-1f44-4204-8889-91f3a1af759c",
  "runId" : "db32d9b8-c2f5-4cb2-ae6d-5af27da8b1a5",
  "name" : null,
  "timestamp" : "2025-05-02T08:34:16.423Z",
  "batchId" : 0,
  "numInputRows" : 130,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 7.615253939429441,
  "durationMs" : {
    "addBatch" : 11792,
    "commitOffsets" : 206,
    "getBatch" : 32,
    "latestOffset" : 2689,
    "queryPlanning" : 1829,
    "triggerExecution" : 17071,
    "walCommit" : 281
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_SPENT_PER_USER_TRANSACTION_TYPE]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 130
      }
    },
    "latestOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 130
      }
    },
    "numInputRows" : 130,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 7.615253939429441,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@641d7673",
    "numOutputRows" : 130
  }
}
25/05/02 08:34:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor-2, groupId=spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 08:34:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor-2, groupId=spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor] Seeking to latest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 08:34:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor-2, groupId=spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=130, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 08:34:34 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 groupId=spark-kafka-source-abd1150e-70e8-4245-9852-1e69a0b24ee4--2040148185-executor read 130 records through 1 polls (polled  out 130 records), taking 597812460 nanos, during time span of 1287302226 nanos.
25/05/02 08:34:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1774 bytes result sent to driver
25/05/02 08:34:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5503 ms on 505ebd009d1d (executor driver) (1/1)
25/05/02 08:34:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/05/02 08:34:34 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 7.802 s
25/05/02 08:34:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 08:34:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/05/02 08:34:34 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 7.816278 s
25/05/02 08:34:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/commits/0 using temp file file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/commits/.0.3a659670-3e05-4380-a1d2-45e28569380c.tmp
25/05/02 08:34:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/commits/.0.3a659670-3e05-4380-a1d2-45e28569380c.tmp to file:/tmp/temporary-2eebe801-4d72-4613-9128-743a662e4b5b/commits/0
25/05/02 08:34:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "112de11a-a714-4cda-8d97-b6b11f6cb530",
  "runId" : "c5cd319d-8020-4fe4-a288-dea023547618",
  "name" : null,
  "timestamp" : "2025-05-02T08:34:16.807Z",
  "batchId" : 0,
  "numInputRows" : 130,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 7.139326706573672,
  "durationMs" : {
    "addBatch" : 13467,
    "commitOffsets" : 125,
    "getBatch" : 32,
    "latestOffset" : 2392,
    "queryPlanning" : 1829,
    "triggerExecution" : 18209,
    "walCommit" : 280
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_SPENT_PER_USER_TRANSACTION_TYPE]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 130
      }
    },
    "latestOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 130
      }
    },
    "numInputRows" : 130,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 7.139326706573672,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
25/05/02 08:34:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:34:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:34:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:34:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:35:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:36:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 08:37:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
