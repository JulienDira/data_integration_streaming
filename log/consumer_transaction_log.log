:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
org.apache.kafka#kafka-clients added as a dependency
org.apache.commons#commons-pool2 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-fb68f4f4-492f-40f8-b0dc-9b9c0e0e5c08;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (249ms)
downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
	[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (154ms)
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (105ms)
downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
	[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (5102ms)
downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
	[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (75ms)
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (15054ms)
downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
	[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (1779ms)
downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
	[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (315ms)
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (8712ms)
downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
	[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (61ms)
:: resolution report :: resolve 5688ms :: artifacts dl 31618ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	:: evicted modules:
	org.apache.kafka#kafka-clients;3.3.1 by [org.apache.kafka#kafka-clients;3.4.1] in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   12  |   10  |   10  |   1   ||   11  |   10  |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-fb68f4f4-492f-40f8-b0dc-9b9c0e0e5c08
	confs: [default]
	11 artifacts copied, 0 already retrieved (56767kB/66ms)
25/04/30 13:53:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30 13:53:36,028 - transaction_log - INFO - Lancement de l'application Spark Streaming...
25/04/30 13:53:36 INFO SparkContext: Running Spark version 3.5.0
25/04/30 13:53:36 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/04/30 13:53:36 INFO SparkContext: Java version 11.0.24
25/04/30 13:53:36 INFO ResourceUtils: ==============================================================
25/04/30 13:53:36 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/30 13:53:36 INFO ResourceUtils: ==============================================================
25/04/30 13:53:36 INFO SparkContext: Submitted application: KafkaConsumer
25/04/30 13:53:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/30 13:53:36 INFO ResourceProfile: Limiting resource is cpu
25/04/30 13:53:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/30 13:53:36 INFO SecurityManager: Changing view acls to: root
25/04/30 13:53:36 INFO SecurityManager: Changing modify acls to: root
25/04/30 13:53:36 INFO SecurityManager: Changing view acls groups to: 
25/04/30 13:53:36 INFO SecurityManager: Changing modify acls groups to: 
25/04/30 13:53:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/04/30 13:53:36 INFO Utils: Successfully started service 'sparkDriver' on port 37597.
25/04/30 13:53:36 INFO SparkEnv: Registering MapOutputTracker
25/04/30 13:53:36 INFO SparkEnv: Registering BlockManagerMaster
25/04/30 13:53:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/30 13:53:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/30 13:53:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/30 13:53:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-23dbab40-c73d-4aa3-9a70-2195ddb16161
25/04/30 13:53:36 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/04/30 13:53:36 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/30 13:53:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/04/30 13:53:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://65de2eec9cb2:37597/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://65de2eec9cb2:37597/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://65de2eec9cb2:37597/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://65de2eec9cb2:37597/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://65de2eec9cb2:37597/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://65de2eec9cb2:37597/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://65de2eec9cb2:37597/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://65de2eec9cb2:37597/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://65de2eec9cb2:37597/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://65de2eec9cb2:37597/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://65de2eec9cb2:37597/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.commons_commons-pool2-2.11.1.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.kafka_kafka-clients-3.4.1.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/com.google.code.findbugs_jsr305-3.0.0.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.lz4_lz4-java-1.8.0.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.xerial.snappy_snappy-java-1.1.10.3.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.slf4j_slf4j-api-2.0.7.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.hadoop_hadoop-client-api-3.3.4.jar
25/04/30 13:53:37 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/commons-logging_commons-logging-1.1.3.jar
25/04/30 13:53:37 INFO Executor: Starting executor ID driver on host 65de2eec9cb2
25/04/30 13:53:37 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/04/30 13:53:37 INFO Executor: Java version 11.0.24
25/04/30 13:53:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/opt/spark/jars/*,file:/app/*'
25/04/30 13:53:37 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@632d3344 for default.
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.hadoop_hadoop-client-api-3.3.4.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/com.google.code.findbugs_jsr305-3.0.0.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.xerial.snappy_snappy-java-1.1.10.3.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.slf4j_slf4j-api-2.0.7.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.commons_commons-pool2-2.11.1.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/commons-logging_commons-logging-1.1.3.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.lz4_lz4-java-1.8.0.jar
25/04/30 13:53:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.kafka_kafka-clients-3.4.1.jar
25/04/30 13:53:37 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO TransportClientFactory: Successfully created connection to 65de2eec9cb2/172.21.0.11:37597 after 47 ms (0 ms spent in bootstraps)
25/04/30 13:53:37 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp2937965903610096393.tmp
25/04/30 13:53:37 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp2937965903610096393.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.kafka_kafka-clients-3.4.1.jar
25/04/30 13:53:37 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
25/04/30 13:53:37 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1746021216119
25/04/30 13:53:37 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp610035855180544154.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp610035855180544154.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.hadoop_hadoop-client-api-3.3.4.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
25/04/30 13:53:38 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1746021216119
25/04/30 13:53:38 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp17954810239621167691.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp17954810239621167691.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/commons-logging_commons-logging-1.1.3.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/commons-logging_commons-logging-1.1.3.jar to class loader default
25/04/30 13:53:38 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1746021216119
25/04/30 13:53:38 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp12039311193051419889.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp12039311193051419889.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/com.google.code.findbugs_jsr305-3.0.0.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
25/04/30 13:53:38 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1746021216119
25/04/30 13:53:38 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp1333556564868546037.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp1333556564868546037.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.commons_commons-pool2-2.11.1.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
25/04/30 13:53:38 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746021216119
25/04/30 13:53:38 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp14084366776140659096.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp14084366776140659096.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/04/30 13:53:38 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1746021216119
25/04/30 13:53:38 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp15389947298124840061.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp15389947298124840061.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
25/04/30 13:53:38 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1746021216119
25/04/30 13:53:38 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp8686328139691938574.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp8686328139691938574.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.slf4j_slf4j-api-2.0.7.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.slf4j_slf4j-api-2.0.7.jar to class loader default
25/04/30 13:53:38 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1746021216119
25/04/30 13:53:38 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp826858566537891537.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp826858566537891537.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.lz4_lz4-java-1.8.0.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.lz4_lz4-java-1.8.0.jar to class loader default
25/04/30 13:53:38 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746021216119
25/04/30 13:53:38 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp3120118785431929565.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp3120118785431929565.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/04/30 13:53:38 INFO Executor: Fetching spark://65de2eec9cb2:37597/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1746021216119
25/04/30 13:53:38 INFO Utils: Fetching spark://65de2eec9cb2:37597/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp2104950652213120928.tmp
25/04/30 13:53:38 INFO Utils: /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/fetchFileTemp2104950652213120928.tmp has been previously copied to /tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.xerial.snappy_snappy-java-1.1.10.3.jar
25/04/30 13:53:38 INFO Executor: Adding file:/tmp/spark-4106d834-20b9-4be8-bd1c-3903afb7eb4e/userFiles-2ddc707f-c089-42cc-9c52-4077c44161a7/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
25/04/30 13:53:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34727.
25/04/30 13:53:38 INFO NettyBlockTransferService: Server created on 65de2eec9cb2:34727
25/04/30 13:53:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/30 13:53:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 65de2eec9cb2, 34727, None)
25/04/30 13:53:38 INFO BlockManagerMasterEndpoint: Registering block manager 65de2eec9cb2:34727 with 1007.8 MiB RAM, BlockManagerId(driver, 65de2eec9cb2, 34727, None)
25/04/30 13:53:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 65de2eec9cb2, 34727, None)
25/04/30 13:53:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 65de2eec9cb2, 34727, None)
2025-04-30 13:53:39,120 - transaction_log - INFO - Session Spark créée.
2025-04-30 13:53:39,120 - transaction_log - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic 'transaction_log'.
25/04/30 13:53:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/04/30 13:53:39 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-04-30 13:53:41,847 - transaction_log - INFO - Connexion à Kafka réussie. Lecture des messages en streaming.
2025-04-30 13:53:42,237 - transaction_log - INFO - Transformation JSON des messages terminée. Schéma résultant :
root
 |-- json_value: string (nullable = true)
 |-- key: string (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

25/04/30 13:53:42 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/04/30 13:53:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a5407594-c612-4861-a237-c3672d99b468. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/04/30 13:53:42 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-a5407594-c612-4861-a237-c3672d99b468 resolved to file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468.
25/04/30 13:53:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/04/30 13:53:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/metadata using temp file file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/.metadata.ac3123e3-8598-4dc1-90d1-663ba72da47a.tmp
25/04/30 13:53:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/.metadata.ac3123e3-8598-4dc1-90d1-663ba72da47a.tmp to file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/metadata
25/04/30 13:53:42 INFO MicroBatchExecution: Starting [id = bd65091d-fb34-4721-8152-1f039da29f29, runId = 1b00316b-5f64-425c-b998-60b16293e236]. Use file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468 to store the query checkpoint.
2025-04-30 13:53:42,919 - transaction_log - INFO - Démarrage de l'écriture en console pour le debug.
2025-04-30 13:53:42,919 - transaction_log - INFO - Initialisation de l'écriture en Parquet...
25/04/30 13:53:42 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@58cdc293] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@43e29aee]
25/04/30 13:53:43 INFO OffsetSeqLog: BatchIds found from listing: 
25/04/30 13:53:43 INFO OffsetSeqLog: BatchIds found from listing: 
25/04/30 13:53:43 INFO MicroBatchExecution: Starting new streaming query.
25/04/30 13:53:43 INFO ResolveWriteToStream: Checkpoint root checkpoints/transaction_log resolved to file:/app/checkpoints/transaction_log.
25/04/30 13:53:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/04/30 13:53:43 INFO MicroBatchExecution: Stream started from {}
25/04/30 13:53:43 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/metadata using temp file file:/app/checkpoints/transaction_log/.metadata.d65f0bb9-478e-43a5-bbc3-91464792921d.tmp
25/04/30 13:53:43 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/.metadata.d65f0bb9-478e-43a5-bbc3-91464792921d.tmp to file:/app/checkpoints/transaction_log/metadata
25/04/30 13:53:43 INFO MicroBatchExecution: Starting [id = 4d301ec5-a389-4f81-b29f-7582bd67fdc7, runId = 06be4f4e-c1cb-439e-9313-f7109c03bab6]. Use file:/app/checkpoints/transaction_log to store the query checkpoint.
2025-04-30 13:53:43,137 - transaction_log - INFO - L'écriture en Parquet a démarré. En attente des messages...
25/04/30 13:53:43 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@58cdc293] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@43e29aee]
25/04/30 13:53:43 INFO OffsetSeqLog: BatchIds found from listing: 
25/04/30 13:53:43 INFO OffsetSeqLog: BatchIds found from listing: 
25/04/30 13:53:43 INFO MicroBatchExecution: Starting new streaming query.
25/04/30 13:53:43 INFO MicroBatchExecution: Stream started from {}
25/04/30 13:53:43 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/04/30 13:53:43 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/04/30 13:53:43 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/04/30 13:53:43 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/04/30 13:53:43 INFO AppInfoParser: Kafka version: 3.4.1
25/04/30 13:53:43 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
25/04/30 13:53:43 INFO AppInfoParser: Kafka startTimeMs: 1746021223923
25/04/30 13:53:43 INFO AppInfoParser: Kafka version: 3.4.1
25/04/30 13:53:43 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
25/04/30 13:53:43 INFO AppInfoParser: Kafka startTimeMs: 1746021223923
25/04/30 13:53:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/sources/0/0 using temp file file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/sources/0/.0.5c9f2b15-be10-4ad3-aedc-95b75ac066c9.tmp
25/04/30 13:53:44 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/sources/0/0 using temp file file:/app/checkpoints/transaction_log/sources/0/.0.2aeec310-1fc3-4e64-a247-dc23132b5f25.tmp
25/04/30 13:53:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/sources/0/.0.5c9f2b15-be10-4ad3-aedc-95b75ac066c9.tmp to file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/sources/0/0
25/04/30 13:53:44 INFO KafkaMicroBatchStream: Initial offsets: {"transaction_log":{"0":0}}
25/04/30 13:53:44 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/sources/0/.0.2aeec310-1fc3-4e64-a247-dc23132b5f25.tmp to file:/app/checkpoints/transaction_log/sources/0/0
25/04/30 13:53:44 INFO KafkaMicroBatchStream: Initial offsets: {"transaction_log":{"0":0}}
25/04/30 13:53:44 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/offsets/0 using temp file file:/app/checkpoints/transaction_log/offsets/.0.38321cd7-b3c4-4857-946c-416aec7f2e8b.tmp
25/04/30 13:53:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/offsets/0 using temp file file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/offsets/.0.ccaada79-446e-41d7-840c-26a32f1ccba6.tmp
25/04/30 13:53:44 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/offsets/.0.38321cd7-b3c4-4857-946c-416aec7f2e8b.tmp to file:/app/checkpoints/transaction_log/offsets/0
25/04/30 13:53:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/offsets/.0.ccaada79-446e-41d7-840c-26a32f1ccba6.tmp to file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/offsets/0
25/04/30 13:53:44 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746021224738,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/04/30 13:53:44 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746021224738,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/04/30 13:53:45 INFO IncrementalExecution: Current batch timestamp = 1746021224738
25/04/30 13:53:45 INFO IncrementalExecution: Current batch timestamp = 1746021224738
25/04/30 13:53:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:45 INFO IncrementalExecution: Current batch timestamp = 1746021224738
25/04/30 13:53:45 INFO IncrementalExecution: Current batch timestamp = 1746021224738
25/04/30 13:53:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:46 INFO IncrementalExecution: Current batch timestamp = 1746021224738
25/04/30 13:53:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 13:53:46 INFO FileStreamSinkLog: BatchIds found from listing: 0
25/04/30 13:53:46 INFO FileStreamSink: Skipping already committed batch 0
25/04/30 13:53:46 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/commits/0 using temp file file:/app/checkpoints/transaction_log/commits/.0.145b6e49-9e7d-41a4-80b6-e05902efde2d.tmp
25/04/30 13:53:46 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/commits/.0.145b6e49-9e7d-41a4-80b6-e05902efde2d.tmp to file:/app/checkpoints/transaction_log/commits/0
25/04/30 13:53:46 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4d301ec5-a389-4f81-b29f-7582bd67fdc7",
  "runId" : "06be4f4e-c1cb-439e-9313-f7109c03bab6",
  "name" : null,
  "timestamp" : "2025-04-30T13:53:43.139Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 353,
    "commitOffsets" : 101,
    "getBatch" : 99,
    "latestOffset" : 1596,
    "queryPlanning" : 813,
    "triggerExecution" : 3082,
    "walCommit" : 102
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[transaction_log]]",
    "startOffset" : null,
    "endOffset" : {
      "transaction_log" : {
        "0" : 148
      }
    },
    "latestOffset" : {
      "transaction_log" : {
        "0" : 148
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "FileSink[/app/data_lake/transaction_log]",
    "numOutputRows" : -1
  }
}
25/04/30 13:53:46 INFO CodeGenerator: Code generated in 396.129606 ms
25/04/30 13:53:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
25/04/30 13:53:46 INFO SparkContext: Starting job: start at <unknown>:0
25/04/30 13:53:47 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/04/30 13:53:47 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/04/30 13:53:47 INFO DAGScheduler: Parents of final stage: List()
25/04/30 13:53:47 INFO DAGScheduler: Missing parents: List()
25/04/30 13:53:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at start at <unknown>:0), which has no missing parents
25/04/30 13:53:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 11.2 KiB, free 1007.8 MiB)
25/04/30 13:53:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1007.8 MiB)
25/04/30 13:53:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 65de2eec9cb2:34727 (size: 5.0 KiB, free: 1007.8 MiB)
25/04/30 13:53:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/04/30 13:53:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/04/30 13:53:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/30 13:53:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (65de2eec9cb2, executor driver, partition 0, PROCESS_LOCAL, 10608 bytes) 
25/04/30 13:53:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/04/30 13:53:47 INFO CodeGenerator: Code generated in 95.705977 ms
25/04/30 13:53:47 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=transaction_log-0 fromOffset=0 untilOffset=148, for query queryId=bd65091d-fb34-4721-8152-1f039da29f29 batchId=0 taskId=0 partitionId=0
25/04/30 13:53:48 INFO CodeGenerator: Code generated in 77.903508 ms
25/04/30 13:53:48 INFO CodeGenerator: Code generated in 16.387039 ms
25/04/30 13:53:48 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/04/30 13:53:48 INFO AppInfoParser: Kafka version: 3.4.1
25/04/30 13:53:48 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
25/04/30 13:53:48 INFO AppInfoParser: Kafka startTimeMs: 1746021228232
25/04/30 13:53:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor-1, groupId=spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor] Assigned to partition(s): transaction_log-0
25/04/30 13:53:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor-1, groupId=spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor] Seeking to offset 0 for partition transaction_log-0
25/04/30 13:53:48 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor-1, groupId=spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor] Resetting the last seen epoch of partition transaction_log-0 to 0 since the associated topicId changed from null to UOhkL5CjQu6Ms2vidjhcNg
25/04/30 13:53:48 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor-1, groupId=spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor] Cluster ID: M7rQ1QLCSVqd8VDtHXyJXQ
25/04/30 13:53:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor-1, groupId=spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor] Seeking to earliest offset of partition transaction_log-0
25/04/30 13:53:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor-1, groupId=spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/04/30 13:53:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor-1, groupId=spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor] Seeking to latest offset of partition transaction_log-0
25/04/30 13:53:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor-1, groupId=spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=148, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/04/30 13:53:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/04/30 13:53:49 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/04/30 13:53:49 INFO KafkaDataConsumer: From Kafka topicPartition=transaction_log-0 groupId=spark-kafka-source-2ae200c3-bdcf-4ad3-95b1-4e9c91ea4c4d-505742181-executor read 148 records through 1 polls (polled  out 148 records), taking 615066952 nanos, during time span of 784537740 nanos.
25/04/30 13:53:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 125857 bytes result sent to driver
25/04/30 13:53:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1605 ms on 65de2eec9cb2 (executor driver) (1/1)
25/04/30 13:53:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/30 13:53:49 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 2.080 s
25/04/30 13:53:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/30 13:53:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/30 13:53:49 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 2.173759 s
25/04/30 13:53:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
25/04/30 13:53:49 INFO CodeGenerator: Code generated in 8.55281 ms
25/04/30 13:53:51 INFO CodeGenerator: Code generated in 74.900055 ms
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+---------------+---------+------+-----------------------+-----------------------+--------------+
|json_value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |key |topic          |partition|offset|timestamp              |ingestion_time         |ingestion_date|
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+---------------+---------+------+-----------------------+-----------------------+--------------+
|{"transaction_id": "TXN-10b02b5d", "timestamp": "2025-04-28T23:05:40.297908Z", "user_id": "USER-6866", "user_name": "Joshua", "product_id": "PROD-713", "amount": 225.81, "currency": "USD", "transaction_type": "purchase", "status": "pending", "location": {"city": "Hyderabad", "country": "India"}, "payment_method": "google_pay", "product_category": "books", "quantity": 1, "shipping_address": {"street": "991 Main St", "zip": "47995", "city": "Hyderabad", "country": "India"}, "device_info": {"os": "Linux", "browser": "Chrome", "ip_address": "96.206.224.102"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 3, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                        |NULL|transaction_log|0        |0     |2025-04-30 08:58:51.557|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-58647304", "timestamp": "2025-04-21T13:25:26.294660Z", "user_id": "USER-6846", "user_name": "Logan", "product_id": "PROD-790", "amount": 638.24, "currency": "EUR", "transaction_type": "payment", "status": "processing", "location": {"city": "Osaka", "country": "Japan"}, "payment_method": "google_pay", "product_category": "clothing", "quantity": 6, "shipping_address": {"street": "202 Main St", "zip": "74710", "city": "Osaka", "country": "Japan"}, "device_info": {"os": "MacOS", "browser": "Chrome", "ip_address": "71.209.206.223"}, "customer_rating": 4, "discount_code": null, "tax_amount": 0.0, "thread": 0, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                               |NULL|transaction_log|0        |1     |2025-04-30 08:58:51.557|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-5c38aa71", "timestamp": "2025-04-08T09:56:36.297908Z", "user_id": "USER-4909", "user_name": "Ramirez", "product_id": "PROD-177", "amount": 638.98, "currency": "JPY", "transaction_type": "refund", "status": "processing", "location": {"city": "Mumbai", "country": "India"}, "payment_method": "apple_pay", "product_category": "electronics", "quantity": 6, "shipping_address": {"street": "329 Main St", "zip": "69376", "city": "Mumbai", "country": "India"}, "device_info": {"os": "iOS", "browser": "Edge", "ip_address": "223.185.222.217"}, "customer_rating": 1, "discount_code": "DISCOUNT-721", "tax_amount": 0.0, "thread": 2, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                   |NULL|transaction_log|0        |2     |2025-04-30 08:58:51.557|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-d3a5f732", "timestamp": "2025-04-23T20:33:40.297908Z", "user_id": "USER-6425", "user_name": "Robinson", "product_id": "PROD-337", "amount": 27.04, "currency": "AUD", "transaction_type": "purchase", "status": "failed", "location": {"city": "Yokohama", "country": "Japan"}, "payment_method": "bank_transfer", "product_category": "books", "quantity": 4, "shipping_address": {"street": "256 Main St", "zip": "53508", "city": "Yokohama", "country": "Japan"}, "device_info": {"os": "Android", "browser": "Edge", "ip_address": "140.212.179.208"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 4, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                      |NULL|transaction_log|0        |3     |2025-04-30 08:58:51.557|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-ffa63d28", "timestamp": "2025-04-22T15:36:33.297908Z", "user_id": "USER-8020", "user_name": "Robert", "product_id": "PROD-725", "amount": 167.83, "currency": "GBP", "transaction_type": "withdrawal", "status": "pending", "location": {"city": "Brisbane", "country": "Australia"}, "payment_method": "credit_card", "product_category": "home_goods", "quantity": 4, "shipping_address": {"street": "259 Main St", "zip": "66904", "city": "Brisbane", "country": "Australia"}, "device_info": {"os": "iOS", "browser": "Firefox", "ip_address": "250.203.172.232"}, "customer_rating": null, "discount_code": "DISCOUNT-912", "tax_amount": 0.0, "thread": 1, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}|NULL|transaction_log|0        |4     |2025-04-30 08:58:51.557|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-e5ca1ffb", "timestamp": "2025-04-02T12:59:10.294660Z", "user_id": "USER-9780", "user_name": "Grace", "product_id": "PROD-805", "amount": 588.87, "currency": "JPY", "transaction_type": "payment", "status": "processing", "location": {"city": "Salvador", "country": "Brazil"}, "payment_method": "credit_card", "product_category": "home_goods", "quantity": 9, "shipping_address": {"street": "420 Main St", "zip": "94880", "city": "Salvador", "country": "Brazil"}, "device_info": {"os": "Linux", "browser": "Chrome", "ip_address": "9.176.106.84"}, "customer_rating": null, "discount_code": "DISCOUNT-312", "tax_amount": 0.0, "thread": 0, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}         |NULL|transaction_log|0        |5     |2025-04-30 08:58:51.558|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-0e019b3b", "timestamp": "2025-04-13T16:52:07.297908Z", "user_id": "USER-8591", "user_name": "Liam", "product_id": "PROD-731", "amount": 937.17, "currency": "EUR", "transaction_type": "withdrawal", "status": "completed", "location": {"city": "Nice", "country": "France"}, "payment_method": "paypal", "product_category": "books", "quantity": 8, "shipping_address": {"street": "816 Main St", "zip": "39925", "city": "Nice", "country": "France"}, "device_info": {"os": "Android", "browser": "Edge", "ip_address": "43.156.220.246"}, "customer_rating": 2, "discount_code": "DISCOUNT-432", "tax_amount": 156.33, "thread": 2, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                        |NULL|transaction_log|0        |6     |2025-04-30 08:58:51.559|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-a4c2ff9d", "timestamp": "2025-04-02T11:37:19.297908Z", "user_id": "USER-9891", "user_name": "Martin", "product_id": "PROD-297", "amount": 449.67, "currency": "EUR", "transaction_type": "payment", "status": "failed", "location": {"city": "Montreal", "country": "Canada"}, "payment_method": "credit_card", "product_category": "books", "quantity": 4, "shipping_address": {"street": "796 Main St", "zip": "86130", "city": "Montreal", "country": "Canada"}, "device_info": {"os": "MacOS", "browser": "Firefox", "ip_address": "8.98.93.141"}, "customer_rating": null, "discount_code": "DISCOUNT-232", "tax_amount": 0.0, "thread": 1, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                 |NULL|transaction_log|0        |7     |2025-04-30 08:58:51.56 |2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-8ea73fa4", "timestamp": "2025-04-29T23:05:36.297908Z", "user_id": "USER-9590", "user_name": "Abigail", "product_id": "PROD-671", "amount": 353.13, "currency": "USD", "transaction_type": "refund", "status": "pending", "location": {"city": "Adelaide", "country": "Australia"}, "payment_method": "apple_pay", "product_category": "food", "quantity": 5, "shipping_address": {"street": "408 Main St", "zip": "86159", "city": "Adelaide", "country": "Australia"}, "device_info": {"os": "Android", "browser": "Edge", "ip_address": "171.163.172.198"}, "customer_rating": null, "discount_code": "DISCOUNT-200", "tax_amount": 0.0, "thread": 3, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}          |NULL|transaction_log|0        |8     |2025-04-30 08:58:51.56 |2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-70fa8324", "timestamp": "2025-04-01T15:07:30.294660Z", "user_id": "USER-1774", "user_name": "Victoria", "product_id": "PROD-734", "amount": 535.71, "currency": "CAD", "transaction_type": "purchase", "status": "cancelled", "location": {"city": "S\u00e3o Paulo", "country": "Brazil"}, "payment_method": "bank_transfer", "product_category": "food", "quantity": 4, "shipping_address": {"street": "926 Main St", "zip": "38690", "city": "S\u00e3o Paulo", "country": "Brazil"}, "device_info": {"os": "iOS", "browser": "Chrome", "ip_address": "188.5.93.103"}, "customer_rating": 4, "discount_code": null, "tax_amount": 0.0, "thread": 0, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}             |NULL|transaction_log|0        |9     |2025-04-30 08:58:51.56 |2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-9416d784", "timestamp": "2025-04-18T12:27:26.297908Z", "user_id": "USER-3463", "user_name": "Madison", "product_id": "PROD-252", "amount": 659.44, "currency": "JPY", "transaction_type": "purchase", "status": "failed", "location": {"city": "Kyoto", "country": "Japan"}, "payment_method": "paypal", "product_category": "food", "quantity": 10, "shipping_address": {"street": "560 Main St", "zip": "86154", "city": "Kyoto", "country": "Japan"}, "device_info": {"os": "Windows", "browser": "Safari", "ip_address": "149.34.174.109"}, "customer_rating": null, "discount_code": "DISCOUNT-394", "tax_amount": 0.0, "thread": 4, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                        |NULL|transaction_log|0        |10    |2025-04-30 08:58:51.561|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-d8b5082c", "timestamp": "2025-04-17T19:26:20.297908Z", "user_id": "USER-4409", "user_name": "Scarlett", "product_id": "PROD-270", "amount": 447.16, "currency": "JPY", "transaction_type": "payment", "status": "cancelled", "location": {"city": "S\u00e3o Paulo", "country": "Brazil"}, "payment_method": "bank_transfer", "product_category": "home_goods", "quantity": 8, "shipping_address": {"street": "496 Main St", "zip": "95256", "city": "S\u00e3o Paulo", "country": "Brazil"}, "device_info": {"os": "Android", "browser": "Chrome", "ip_address": "254.98.132.217"}, "customer_rating": 4, "discount_code": null, "tax_amount": 0.0, "thread": 2, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}  |NULL|transaction_log|0        |11    |2025-04-30 08:58:51.561|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-b42e62e7", "timestamp": "2025-04-10T09:21:36.297908Z", "user_id": "USER-6016", "user_name": "John", "product_id": "PROD-786", "amount": 144.22, "currency": "AUD", "transaction_type": "payment", "status": "completed", "location": {"city": "Ottawa", "country": "Canada"}, "payment_method": "cryptocurrency", "product_category": "home_goods", "quantity": 9, "shipping_address": {"street": "630 Main St", "zip": "15386", "city": "Ottawa", "country": "Canada"}, "device_info": {"os": "Linux", "browser": "Safari", "ip_address": "205.28.78.188"}, "customer_rating": null, "discount_code": "DISCOUNT-385", "tax_amount": 25.73, "thread": 1, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}         |NULL|transaction_log|0        |12    |2025-04-30 08:58:51.562|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-ec37823d", "timestamp": "2025-04-18T14:16:14.297908Z", "user_id": "USER-8152", "user_name": "John", "product_id": "PROD-899", "amount": 697.18, "currency": "USD", "transaction_type": "refund", "status": "cancelled", "location": {"city": "Kyoto", "country": "Japan"}, "payment_method": "credit_card", "product_category": "home_goods", "quantity": 10, "shipping_address": {"street": "104 Main St", "zip": "54697", "city": "Kyoto", "country": "Japan"}, "device_info": {"os": "Linux", "browser": "Firefox", "ip_address": "38.62.132.58"}, "customer_rating": 1, "discount_code": "DISCOUNT-850", "tax_amount": 0.0, "thread": 3, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                     |NULL|transaction_log|0        |13    |2025-04-30 08:58:51.562|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-2efbba1e", "timestamp": "2025-04-20T10:13:39.297908Z", "user_id": "USER-4367", "user_name": "Evelyn", "product_id": "PROD-752", "amount": 695.09, "currency": "EUR", "transaction_type": "withdrawal", "status": "completed", "location": {"city": "Hyderabad", "country": "India"}, "payment_method": "google_pay", "product_category": "food", "quantity": 8, "shipping_address": {"street": "135 Main St", "zip": "75369", "city": "Hyderabad", "country": "India"}, "device_info": {"os": "Windows", "browser": "Edge", "ip_address": "110.200.219.23"}, "customer_rating": 1, "discount_code": null, "tax_amount": 47.22, "thread": 2, "message_number": 3, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                      |NULL|transaction_log|0        |14    |2025-04-30 08:58:51.562|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-6aa923ea", "timestamp": "2025-04-18T16:30:23.297908Z", "user_id": "USER-8169", "user_name": "Young", "product_id": "PROD-544", "amount": 619.56, "currency": "USD", "transaction_type": "payment", "status": "failed", "location": {"city": "Montreal", "country": "Canada"}, "payment_method": "apple_pay", "product_category": "food", "quantity": 1, "shipping_address": {"street": "703 Main St", "zip": "31326", "city": "Montreal", "country": "Canada"}, "device_info": {"os": "iOS", "browser": "Edge", "ip_address": "14.208.6.58"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 4, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                                    |NULL|transaction_log|0        |15    |2025-04-30 08:58:51.562|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-823589d4", "timestamp": "2025-04-23T17:35:57.297908Z", "user_id": "USER-8710", "user_name": "John", "product_id": "PROD-691", "amount": 761.21, "currency": "USD", "transaction_type": "purchase", "status": "processing", "location": {"city": "Hangzhou", "country": "China"}, "payment_method": "paypal", "product_category": "electronics", "quantity": 4, "shipping_address": {"street": "225 Main St", "zip": "71697", "city": "Hangzhou", "country": "China"}, "device_info": {"os": "MacOS", "browser": "Edge", "ip_address": "232.199.237.186"}, "customer_rating": 4, "discount_code": null, "tax_amount": 0.0, "thread": 3, "message_number": 3, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                           |NULL|transaction_log|0        |16    |2025-04-30 08:58:51.563|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-28ee1cc6", "timestamp": "2025-04-24T17:29:54.297908Z", "user_id": "USER-3897", "user_name": "Emily", "product_id": "PROD-967", "amount": 256.5, "currency": "CAD", "transaction_type": "payment", "status": "completed", "location": {"city": "Frankfurt", "country": "Germany"}, "payment_method": "paypal", "product_category": "clothing", "quantity": 5, "shipping_address": {"street": "908 Main St", "zip": "77411", "city": "Frankfurt", "country": "Germany"}, "device_info": {"os": "Windows", "browser": "Safari", "ip_address": "194.117.212.233"}, "customer_rating": 1, "discount_code": null, "tax_amount": 46.77, "thread": 2, "message_number": 4, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                    |NULL|transaction_log|0        |17    |2025-04-30 08:58:51.563|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-b6bf779d", "timestamp": "2025-04-22T10:33:44.297908Z", "user_id": "USER-2650", "user_name": "Logan", "product_id": "PROD-412", "amount": 804.94, "currency": "GBP", "transaction_type": "withdrawal", "status": "completed", "location": {"city": "Berlin", "country": "Germany"}, "payment_method": "bank_transfer", "product_category": "clothing", "quantity": 2, "shipping_address": {"street": "563 Main St", "zip": "41212", "city": "Berlin", "country": "Germany"}, "device_info": {"os": "Linux", "browser": "Chrome", "ip_address": "254.242.152.200"}, "customer_rating": null, "discount_code": "DISCOUNT-802", "tax_amount": 104.49, "thread": 4, "message_number": 3, "timestamp_of_reception_log": "30/04/2025 10:58:51"}   |NULL|transaction_log|0        |18    |2025-04-30 08:58:51.564|2025-04-30 13:53:44.738|2025-04-30    |
|{"transaction_id": "TXN-f3f6c57d", "timestamp": "2025-04-27T14:48:48.297908Z", "user_id": "USER-8014", "user_name": "Sophia", "product_id": "PROD-731", "amount": 88.97, "currency": "EUR", "transaction_type": "purchase", "status": "processing", "location": {"city": "Birmingham", "country": "UK"}, "payment_method": "bank_transfer", "product_category": "electronics", "quantity": 10, "shipping_address": {"street": "199 Main St", "zip": "49001", "city": "Birmingham", "country": "UK"}, "device_info": {"os": "Linux", "browser": "Chrome", "ip_address": "155.36.236.229"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 3, "message_number": 4, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                |NULL|transaction_log|0        |19    |2025-04-30 08:58:51.564|2025-04-30 13:53:44.738|2025-04-30    |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+---------------+---------+------+-----------------------+-----------------------+--------------+
only showing top 20 rows

25/04/30 13:53:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
25/04/30 13:53:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/commits/0 using temp file file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/commits/.0.4cc7adba-7d1f-4356-ad0b-d3f09c9a6b65.tmp
25/04/30 13:53:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/commits/.0.4cc7adba-7d1f-4356-ad0b-d3f09c9a6b65.tmp to file:/tmp/temporary-a5407594-c612-4861-a237-c3672d99b468/commits/0
25/04/30 13:53:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "bd65091d-fb34-4721-8152-1f039da29f29",
  "runId" : "1b00316b-5f64-425c-b998-60b16293e236",
  "name" : null,
  "timestamp" : "2025-04-30T13:53:43.016Z",
  "batchId" : 0,
  "numInputRows" : 148,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 18.26033312769895,
  "durationMs" : {
    "addBatch" : 5213,
    "commitOffsets" : 92,
    "getBatch" : 28,
    "latestOffset" : 1708,
    "queryPlanning" : 868,
    "triggerExecution" : 8105,
    "walCommit" : 102
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[transaction_log]]",
    "startOffset" : null,
    "endOffset" : {
      "transaction_log" : {
        "0" : 148
      }
    },
    "latestOffset" : {
      "transaction_log" : {
        "0" : 148
      }
    },
    "numInputRows" : 148,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 18.26033312769895,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@728bae8d",
    "numOutputRows" : 148
  }
}
25/04/30 13:53:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:54:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:55:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:55:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:55:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:55:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:55:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 13:55:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
