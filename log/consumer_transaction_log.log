25/04/30 14:20:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-30 14:20:47,575 - transaction_log - INFO - Lancement de l'application Spark Streaming...
25/04/30 14:20:47 INFO SparkContext: Running Spark version 3.5.0
25/04/30 14:20:47 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/04/30 14:20:47 INFO SparkContext: Java version 11.0.24
25/04/30 14:20:47 INFO ResourceUtils: ==============================================================
25/04/30 14:20:47 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/30 14:20:47 INFO ResourceUtils: ==============================================================
25/04/30 14:20:47 INFO SparkContext: Submitted application: KafkaConsumer
25/04/30 14:20:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/30 14:20:48 INFO ResourceProfile: Limiting resource is cpu
25/04/30 14:20:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/30 14:20:48 INFO SecurityManager: Changing view acls to: root
25/04/30 14:20:48 INFO SecurityManager: Changing modify acls to: root
25/04/30 14:20:48 INFO SecurityManager: Changing view acls groups to: 
25/04/30 14:20:48 INFO SecurityManager: Changing modify acls groups to: 
25/04/30 14:20:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/04/30 14:20:49 INFO Utils: Successfully started service 'sparkDriver' on port 37993.
25/04/30 14:20:49 INFO SparkEnv: Registering MapOutputTracker
25/04/30 14:20:49 INFO SparkEnv: Registering BlockManagerMaster
25/04/30 14:20:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/30 14:20:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/30 14:20:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/30 14:20:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ed8a0a6d-beb3-4822-9ffb-9944ee837acb
25/04/30 14:20:49 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/04/30 14:20:49 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/30 14:20:50 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/04/30 14:20:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/04/30 14:20:50 INFO Utils: Successfully started service 'SparkUI' on port 4041.
25/04/30 14:20:50 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://54e48cdf644c:37993/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746022847846
25/04/30 14:20:50 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://54e48cdf644c:37993/jars/kafka-clients-3.3.1.jar with timestamp 1746022847846
25/04/30 14:20:50 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://54e48cdf644c:37993/jars/commons-pool2-2.11.1.jar with timestamp 1746022847846
25/04/30 14:20:50 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://54e48cdf644c:37993/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746022847846
25/04/30 14:20:50 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://54e48cdf644c:37993/jars/postgresql-42.2.23.jar with timestamp 1746022847846
25/04/30 14:20:50 INFO Executor: Starting executor ID driver on host 54e48cdf644c
25/04/30 14:20:50 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/04/30 14:20:50 INFO Executor: Java version 11.0.24
25/04/30 14:20:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/04/30 14:20:50 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2397e4e6 for default.
25/04/30 14:20:50 INFO Executor: Fetching spark://54e48cdf644c:37993/jars/commons-pool2-2.11.1.jar with timestamp 1746022847846
25/04/30 14:20:50 INFO TransportClientFactory: Successfully created connection to 54e48cdf644c/172.21.0.11:37993 after 96 ms (0 ms spent in bootstraps)
25/04/30 14:20:50 INFO Utils: Fetching spark://54e48cdf644c:37993/jars/commons-pool2-2.11.1.jar to /tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/fetchFileTemp2376419631612185691.tmp
25/04/30 14:20:50 INFO Executor: Adding file:/tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/commons-pool2-2.11.1.jar to class loader default
25/04/30 14:20:50 INFO Executor: Fetching spark://54e48cdf644c:37993/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746022847846
25/04/30 14:20:50 INFO Utils: Fetching spark://54e48cdf644c:37993/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/fetchFileTemp8789416310384860602.tmp
25/04/30 14:20:50 INFO Executor: Adding file:/tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/04/30 14:20:50 INFO Executor: Fetching spark://54e48cdf644c:37993/jars/kafka-clients-3.3.1.jar with timestamp 1746022847846
25/04/30 14:20:50 INFO Utils: Fetching spark://54e48cdf644c:37993/jars/kafka-clients-3.3.1.jar to /tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/fetchFileTemp6317720597055309014.tmp
25/04/30 14:20:51 INFO Executor: Adding file:/tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/kafka-clients-3.3.1.jar to class loader default
25/04/30 14:20:51 INFO Executor: Fetching spark://54e48cdf644c:37993/jars/postgresql-42.2.23.jar with timestamp 1746022847846
25/04/30 14:20:51 INFO Utils: Fetching spark://54e48cdf644c:37993/jars/postgresql-42.2.23.jar to /tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/fetchFileTemp7896354487503208741.tmp
25/04/30 14:20:51 INFO Executor: Adding file:/tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/postgresql-42.2.23.jar to class loader default
25/04/30 14:20:51 INFO Executor: Fetching spark://54e48cdf644c:37993/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746022847846
25/04/30 14:20:51 INFO Utils: Fetching spark://54e48cdf644c:37993/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/fetchFileTemp11640716943046199207.tmp
25/04/30 14:20:51 INFO Executor: Adding file:/tmp/spark-1d8a87c3-b250-4be5-9dd6-4082622df54e/userFiles-fa685dd5-0ef3-4c75-84cc-228cb24a6cb1/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/04/30 14:20:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36865.
25/04/30 14:20:51 INFO NettyBlockTransferService: Server created on 54e48cdf644c:36865
25/04/30 14:20:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/30 14:20:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 54e48cdf644c, 36865, None)
25/04/30 14:20:51 INFO BlockManagerMasterEndpoint: Registering block manager 54e48cdf644c:36865 with 1007.8 MiB RAM, BlockManagerId(driver, 54e48cdf644c, 36865, None)
25/04/30 14:20:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 54e48cdf644c, 36865, None)
25/04/30 14:20:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 54e48cdf644c, 36865, None)
2025-04-30 14:20:52,354 - transaction_log - INFO - Session Spark créée.
2025-04-30 14:20:52,354 - transaction_log - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic 'transaction_log'.
25/04/30 14:20:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/04/30 14:20:52 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-04-30 14:20:57,041 - transaction_log - INFO - Connexion à Kafka réussie. Lecture des messages en streaming.
2025-04-30 14:20:57,840 - transaction_log - INFO - Transformation JSON des messages terminée. Schéma résultant :
root
 |-- json_value: string (nullable = true)
 |-- key: string (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

25/04/30 14:20:57 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/04/30 14:20:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/04/30 14:20:58 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf resolved to file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf.
25/04/30 14:20:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/04/30 14:20:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/metadata using temp file file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/.metadata.b70e985b-cbb1-41e9-a439-33253e0e0d84.tmp
25/04/30 14:20:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/.metadata.b70e985b-cbb1-41e9-a439-33253e0e0d84.tmp to file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/metadata
25/04/30 14:20:59 INFO MicroBatchExecution: Starting [id = 35b22499-d557-49c2-a6e1-82975dec8afb, runId = e44e329c-fee0-4b77-8e2e-0dc086f50220]. Use file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf to store the query checkpoint.
2025-04-30 14:20:59,250 - transaction_log - INFO - Démarrage de l'écriture en console pour le debug.
2025-04-30 14:20:59,250 - transaction_log - INFO - Initialisation de l'écriture en Parquet...
25/04/30 14:20:59 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5ff089c6] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5a3867c0]
25/04/30 14:20:59 INFO ResolveWriteToStream: Checkpoint root checkpoints/transaction_log resolved to file:/app/checkpoints/transaction_log.
25/04/30 14:20:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/04/30 14:20:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/04/30 14:20:59 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/metadata using temp file file:/app/checkpoints/transaction_log/.metadata.41cb68fa-64be-4757-b2fc-4ce11bc2b53f.tmp
25/04/30 14:20:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/04/30 14:20:59 INFO MicroBatchExecution: Starting new streaming query.
25/04/30 14:20:59 INFO MicroBatchExecution: Stream started from {}
25/04/30 14:20:59 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/.metadata.41cb68fa-64be-4757-b2fc-4ce11bc2b53f.tmp to file:/app/checkpoints/transaction_log/metadata
25/04/30 14:20:59 INFO MicroBatchExecution: Starting [id = 5811d604-41cd-4619-83df-68787edada4c, runId = 06aedd90-9b56-4013-9e4e-3dbe9877ee61]. Use file:/app/checkpoints/transaction_log to store the query checkpoint.
2025-04-30 14:20:59,847 - transaction_log - INFO - L'écriture en Parquet a démarré. En attente des messages...
25/04/30 14:20:59 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5ff089c6] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5a3867c0]
25/04/30 14:20:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/04/30 14:20:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/04/30 14:20:59 INFO MicroBatchExecution: Starting new streaming query.
25/04/30 14:20:59 INFO MicroBatchExecution: Stream started from {}
25/04/30 14:21:01 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/04/30 14:21:01 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/04/30 14:21:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/04/30 14:21:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/04/30 14:21:01 INFO AppInfoParser: Kafka version: 3.3.1
25/04/30 14:21:01 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/04/30 14:21:01 INFO AppInfoParser: Kafka startTimeMs: 1746022861550
25/04/30 14:21:01 INFO AppInfoParser: Kafka version: 3.3.1
25/04/30 14:21:01 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/04/30 14:21:01 INFO AppInfoParser: Kafka startTimeMs: 1746022861550
25/04/30 14:21:02 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/sources/0/0 using temp file file:/app/checkpoints/transaction_log/sources/0/.0.50ac5393-e4c1-48c8-8fda-8e0c68b699ce.tmp
25/04/30 14:21:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/sources/0/0 using temp file file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/sources/0/.0.5416711f-9470-4859-b92d-812cd5fed98b.tmp
25/04/30 14:21:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/sources/0/.0.5416711f-9470-4859-b92d-812cd5fed98b.tmp to file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/sources/0/0
25/04/30 14:21:02 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/sources/0/.0.50ac5393-e4c1-48c8-8fda-8e0c68b699ce.tmp to file:/app/checkpoints/transaction_log/sources/0/0
25/04/30 14:21:02 INFO KafkaMicroBatchStream: Initial offsets: {"transaction_log":{"0":0}}
25/04/30 14:21:02 INFO KafkaMicroBatchStream: Initial offsets: {"transaction_log":{"0":0}}
25/04/30 14:21:02 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/offsets/0 using temp file file:/app/checkpoints/transaction_log/offsets/.0.0ce7b820-b317-47d2-afba-f6de895edb12.tmp
25/04/30 14:21:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/offsets/0 using temp file file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/offsets/.0.a2e091cd-8994-4806-bf6f-2309264f587c.tmp
25/04/30 14:21:03 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/offsets/.0.0ce7b820-b317-47d2-afba-f6de895edb12.tmp to file:/app/checkpoints/transaction_log/offsets/0
25/04/30 14:21:03 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746022862952,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/04/30 14:21:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/offsets/.0.a2e091cd-8994-4806-bf6f-2309264f587c.tmp to file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/offsets/0
25/04/30 14:21:03 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746022862952,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/04/30 14:21:05 INFO IncrementalExecution: Current batch timestamp = 1746022862952
25/04/30 14:21:05 INFO IncrementalExecution: Current batch timestamp = 1746022862952
25/04/30 14:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:06 INFO IncrementalExecution: Current batch timestamp = 1746022862952
25/04/30 14:21:06 INFO IncrementalExecution: Current batch timestamp = 1746022862952
25/04/30 14:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:06 INFO IncrementalExecution: Current batch timestamp = 1746022862952
25/04/30 14:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/04/30 14:21:06 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2
25/04/30 14:21:06 INFO FileStreamSink: Skipping already committed batch 0
25/04/30 14:21:06 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/commits/0 using temp file file:/app/checkpoints/transaction_log/commits/.0.604aa676-4e17-4252-a2dc-4d721f321107.tmp
25/04/30 14:21:07 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/commits/.0.604aa676-4e17-4252-a2dc-4d721f321107.tmp to file:/app/checkpoints/transaction_log/commits/0
25/04/30 14:21:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "5811d604-41cd-4619-83df-68787edada4c",
  "runId" : "06aedd90-9b56-4013-9e4e-3dbe9877ee61",
  "name" : null,
  "timestamp" : "2025-04-30T14:20:59.850Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 793,
    "commitOffsets" : 203,
    "getBatch" : 198,
    "latestOffset" : 3097,
    "queryPlanning" : 2689,
    "triggerExecution" : 7197,
    "walCommit" : 193
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[transaction_log]]",
    "startOffset" : null,
    "endOffset" : {
      "transaction_log" : {
        "0" : 186
      }
    },
    "latestOffset" : {
      "transaction_log" : {
        "0" : 186
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "FileSink[/app/data_lake/transaction_log]",
    "numOutputRows" : -1
  }
}
25/04/30 14:21:08 INFO CodeGenerator: Code generated in 1299.932018 ms
25/04/30 14:21:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
25/04/30 14:21:09 INFO SparkContext: Starting job: start at <unknown>:0
25/04/30 14:21:09 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/04/30 14:21:09 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/04/30 14:21:09 INFO DAGScheduler: Parents of final stage: List()
25/04/30 14:21:09 INFO DAGScheduler: Missing parents: List()
25/04/30 14:21:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at start at <unknown>:0), which has no missing parents
25/04/30 14:21:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 11.2 KiB, free 1007.8 MiB)
25/04/30 14:21:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1007.8 MiB)
25/04/30 14:21:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 54e48cdf644c:36865 (size: 5.0 KiB, free: 1007.8 MiB)
25/04/30 14:21:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/04/30 14:21:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/04/30 14:21:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/30 14:21:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (54e48cdf644c, executor driver, partition 0, PROCESS_LOCAL, 9064 bytes) 
25/04/30 14:21:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/04/30 14:21:11 INFO CodeGenerator: Code generated in 185.726128 ms
25/04/30 14:21:11 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=transaction_log-0 fromOffset=0 untilOffset=186, for query queryId=35b22499-d557-49c2-a6e1-82975dec8afb batchId=0 taskId=0 partitionId=0
25/04/30 14:21:11 INFO CodeGenerator: Code generated in 87.503886 ms
25/04/30 14:21:12 INFO CodeGenerator: Code generated in 97.760645 ms
25/04/30 14:21:12 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/04/30 14:21:12 INFO AppInfoParser: Kafka version: 3.3.1
25/04/30 14:21:12 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/04/30 14:21:12 INFO AppInfoParser: Kafka startTimeMs: 1746022872438
25/04/30 14:21:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor-1, groupId=spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor] Assigned to partition(s): transaction_log-0
25/04/30 14:21:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor-1, groupId=spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor] Seeking to offset 0 for partition transaction_log-0
25/04/30 14:21:12 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor-1, groupId=spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor] Resetting the last seen epoch of partition transaction_log-0 to 0 since the associated topicId changed from null to UOhkL5CjQu6Ms2vidjhcNg
25/04/30 14:21:12 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor-1, groupId=spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor] Cluster ID: M7rQ1QLCSVqd8VDtHXyJXQ
25/04/30 14:21:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor-1, groupId=spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor] Seeking to earliest offset of partition transaction_log-0
25/04/30 14:21:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor-1, groupId=spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/04/30 14:21:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor-1, groupId=spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor] Seeking to latest offset of partition transaction_log-0
25/04/30 14:21:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor-1, groupId=spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=186, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/04/30 14:21:13 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/04/30 14:21:13 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/04/30 14:21:13 INFO KafkaDataConsumer: From Kafka topicPartition=transaction_log-0 groupId=spark-kafka-source-1e32594c-589a-48aa-9604-25a283cecad0--452486348-executor read 186 records through 1 polls (polled  out 186 records), taking 705802215 nanos, during time span of 997414688 nanos.
25/04/30 14:21:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 157644 bytes result sent to driver
25/04/30 14:21:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2912 ms on 54e48cdf644c (executor driver) (1/1)
25/04/30 14:21:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/30 14:21:13 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 4.194 s
25/04/30 14:21:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/30 14:21:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/30 14:21:13 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 4.406730 s
25/04/30 14:21:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
25/04/30 14:21:14 INFO CodeGenerator: Code generated in 89.921066 ms
25/04/30 14:21:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 14:21:19 INFO CodeGenerator: Code generated in 93.943759 ms
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+---------------+---------+------+-----------------------+-----------------------+--------------+
|json_value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |key |topic          |partition|offset|timestamp              |ingestion_time         |ingestion_date|
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+---------------+---------+------+-----------------------+-----------------------+--------------+
|{"transaction_id": "TXN-10b02b5d", "timestamp": "2025-04-28T23:05:40.297908Z", "user_id": "USER-6866", "user_name": "Joshua", "product_id": "PROD-713", "amount": 225.81, "currency": "USD", "transaction_type": "purchase", "status": "pending", "location": {"city": "Hyderabad", "country": "India"}, "payment_method": "google_pay", "product_category": "books", "quantity": 1, "shipping_address": {"street": "991 Main St", "zip": "47995", "city": "Hyderabad", "country": "India"}, "device_info": {"os": "Linux", "browser": "Chrome", "ip_address": "96.206.224.102"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 3, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                        |NULL|transaction_log|0        |0     |2025-04-30 08:58:51.557|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-58647304", "timestamp": "2025-04-21T13:25:26.294660Z", "user_id": "USER-6846", "user_name": "Logan", "product_id": "PROD-790", "amount": 638.24, "currency": "EUR", "transaction_type": "payment", "status": "processing", "location": {"city": "Osaka", "country": "Japan"}, "payment_method": "google_pay", "product_category": "clothing", "quantity": 6, "shipping_address": {"street": "202 Main St", "zip": "74710", "city": "Osaka", "country": "Japan"}, "device_info": {"os": "MacOS", "browser": "Chrome", "ip_address": "71.209.206.223"}, "customer_rating": 4, "discount_code": null, "tax_amount": 0.0, "thread": 0, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                               |NULL|transaction_log|0        |1     |2025-04-30 08:58:51.557|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-5c38aa71", "timestamp": "2025-04-08T09:56:36.297908Z", "user_id": "USER-4909", "user_name": "Ramirez", "product_id": "PROD-177", "amount": 638.98, "currency": "JPY", "transaction_type": "refund", "status": "processing", "location": {"city": "Mumbai", "country": "India"}, "payment_method": "apple_pay", "product_category": "electronics", "quantity": 6, "shipping_address": {"street": "329 Main St", "zip": "69376", "city": "Mumbai", "country": "India"}, "device_info": {"os": "iOS", "browser": "Edge", "ip_address": "223.185.222.217"}, "customer_rating": 1, "discount_code": "DISCOUNT-721", "tax_amount": 0.0, "thread": 2, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                   |NULL|transaction_log|0        |2     |2025-04-30 08:58:51.557|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-d3a5f732", "timestamp": "2025-04-23T20:33:40.297908Z", "user_id": "USER-6425", "user_name": "Robinson", "product_id": "PROD-337", "amount": 27.04, "currency": "AUD", "transaction_type": "purchase", "status": "failed", "location": {"city": "Yokohama", "country": "Japan"}, "payment_method": "bank_transfer", "product_category": "books", "quantity": 4, "shipping_address": {"street": "256 Main St", "zip": "53508", "city": "Yokohama", "country": "Japan"}, "device_info": {"os": "Android", "browser": "Edge", "ip_address": "140.212.179.208"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 4, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                      |NULL|transaction_log|0        |3     |2025-04-30 08:58:51.557|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-ffa63d28", "timestamp": "2025-04-22T15:36:33.297908Z", "user_id": "USER-8020", "user_name": "Robert", "product_id": "PROD-725", "amount": 167.83, "currency": "GBP", "transaction_type": "withdrawal", "status": "pending", "location": {"city": "Brisbane", "country": "Australia"}, "payment_method": "credit_card", "product_category": "home_goods", "quantity": 4, "shipping_address": {"street": "259 Main St", "zip": "66904", "city": "Brisbane", "country": "Australia"}, "device_info": {"os": "iOS", "browser": "Firefox", "ip_address": "250.203.172.232"}, "customer_rating": null, "discount_code": "DISCOUNT-912", "tax_amount": 0.0, "thread": 1, "message_number": 0, "timestamp_of_reception_log": "30/04/2025 10:58:51"}|NULL|transaction_log|0        |4     |2025-04-30 08:58:51.557|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-e5ca1ffb", "timestamp": "2025-04-02T12:59:10.294660Z", "user_id": "USER-9780", "user_name": "Grace", "product_id": "PROD-805", "amount": 588.87, "currency": "JPY", "transaction_type": "payment", "status": "processing", "location": {"city": "Salvador", "country": "Brazil"}, "payment_method": "credit_card", "product_category": "home_goods", "quantity": 9, "shipping_address": {"street": "420 Main St", "zip": "94880", "city": "Salvador", "country": "Brazil"}, "device_info": {"os": "Linux", "browser": "Chrome", "ip_address": "9.176.106.84"}, "customer_rating": null, "discount_code": "DISCOUNT-312", "tax_amount": 0.0, "thread": 0, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}         |NULL|transaction_log|0        |5     |2025-04-30 08:58:51.558|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-0e019b3b", "timestamp": "2025-04-13T16:52:07.297908Z", "user_id": "USER-8591", "user_name": "Liam", "product_id": "PROD-731", "amount": 937.17, "currency": "EUR", "transaction_type": "withdrawal", "status": "completed", "location": {"city": "Nice", "country": "France"}, "payment_method": "paypal", "product_category": "books", "quantity": 8, "shipping_address": {"street": "816 Main St", "zip": "39925", "city": "Nice", "country": "France"}, "device_info": {"os": "Android", "browser": "Edge", "ip_address": "43.156.220.246"}, "customer_rating": 2, "discount_code": "DISCOUNT-432", "tax_amount": 156.33, "thread": 2, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                        |NULL|transaction_log|0        |6     |2025-04-30 08:58:51.559|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-a4c2ff9d", "timestamp": "2025-04-02T11:37:19.297908Z", "user_id": "USER-9891", "user_name": "Martin", "product_id": "PROD-297", "amount": 449.67, "currency": "EUR", "transaction_type": "payment", "status": "failed", "location": {"city": "Montreal", "country": "Canada"}, "payment_method": "credit_card", "product_category": "books", "quantity": 4, "shipping_address": {"street": "796 Main St", "zip": "86130", "city": "Montreal", "country": "Canada"}, "device_info": {"os": "MacOS", "browser": "Firefox", "ip_address": "8.98.93.141"}, "customer_rating": null, "discount_code": "DISCOUNT-232", "tax_amount": 0.0, "thread": 1, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                 |NULL|transaction_log|0        |7     |2025-04-30 08:58:51.56 |2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-8ea73fa4", "timestamp": "2025-04-29T23:05:36.297908Z", "user_id": "USER-9590", "user_name": "Abigail", "product_id": "PROD-671", "amount": 353.13, "currency": "USD", "transaction_type": "refund", "status": "pending", "location": {"city": "Adelaide", "country": "Australia"}, "payment_method": "apple_pay", "product_category": "food", "quantity": 5, "shipping_address": {"street": "408 Main St", "zip": "86159", "city": "Adelaide", "country": "Australia"}, "device_info": {"os": "Android", "browser": "Edge", "ip_address": "171.163.172.198"}, "customer_rating": null, "discount_code": "DISCOUNT-200", "tax_amount": 0.0, "thread": 3, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}          |NULL|transaction_log|0        |8     |2025-04-30 08:58:51.56 |2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-70fa8324", "timestamp": "2025-04-01T15:07:30.294660Z", "user_id": "USER-1774", "user_name": "Victoria", "product_id": "PROD-734", "amount": 535.71, "currency": "CAD", "transaction_type": "purchase", "status": "cancelled", "location": {"city": "S\u00e3o Paulo", "country": "Brazil"}, "payment_method": "bank_transfer", "product_category": "food", "quantity": 4, "shipping_address": {"street": "926 Main St", "zip": "38690", "city": "S\u00e3o Paulo", "country": "Brazil"}, "device_info": {"os": "iOS", "browser": "Chrome", "ip_address": "188.5.93.103"}, "customer_rating": 4, "discount_code": null, "tax_amount": 0.0, "thread": 0, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}             |NULL|transaction_log|0        |9     |2025-04-30 08:58:51.56 |2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-9416d784", "timestamp": "2025-04-18T12:27:26.297908Z", "user_id": "USER-3463", "user_name": "Madison", "product_id": "PROD-252", "amount": 659.44, "currency": "JPY", "transaction_type": "purchase", "status": "failed", "location": {"city": "Kyoto", "country": "Japan"}, "payment_method": "paypal", "product_category": "food", "quantity": 10, "shipping_address": {"street": "560 Main St", "zip": "86154", "city": "Kyoto", "country": "Japan"}, "device_info": {"os": "Windows", "browser": "Safari", "ip_address": "149.34.174.109"}, "customer_rating": null, "discount_code": "DISCOUNT-394", "tax_amount": 0.0, "thread": 4, "message_number": 1, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                        |NULL|transaction_log|0        |10    |2025-04-30 08:58:51.561|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-d8b5082c", "timestamp": "2025-04-17T19:26:20.297908Z", "user_id": "USER-4409", "user_name": "Scarlett", "product_id": "PROD-270", "amount": 447.16, "currency": "JPY", "transaction_type": "payment", "status": "cancelled", "location": {"city": "S\u00e3o Paulo", "country": "Brazil"}, "payment_method": "bank_transfer", "product_category": "home_goods", "quantity": 8, "shipping_address": {"street": "496 Main St", "zip": "95256", "city": "S\u00e3o Paulo", "country": "Brazil"}, "device_info": {"os": "Android", "browser": "Chrome", "ip_address": "254.98.132.217"}, "customer_rating": 4, "discount_code": null, "tax_amount": 0.0, "thread": 2, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}  |NULL|transaction_log|0        |11    |2025-04-30 08:58:51.561|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-b42e62e7", "timestamp": "2025-04-10T09:21:36.297908Z", "user_id": "USER-6016", "user_name": "John", "product_id": "PROD-786", "amount": 144.22, "currency": "AUD", "transaction_type": "payment", "status": "completed", "location": {"city": "Ottawa", "country": "Canada"}, "payment_method": "cryptocurrency", "product_category": "home_goods", "quantity": 9, "shipping_address": {"street": "630 Main St", "zip": "15386", "city": "Ottawa", "country": "Canada"}, "device_info": {"os": "Linux", "browser": "Safari", "ip_address": "205.28.78.188"}, "customer_rating": null, "discount_code": "DISCOUNT-385", "tax_amount": 25.73, "thread": 1, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}         |NULL|transaction_log|0        |12    |2025-04-30 08:58:51.562|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-ec37823d", "timestamp": "2025-04-18T14:16:14.297908Z", "user_id": "USER-8152", "user_name": "John", "product_id": "PROD-899", "amount": 697.18, "currency": "USD", "transaction_type": "refund", "status": "cancelled", "location": {"city": "Kyoto", "country": "Japan"}, "payment_method": "credit_card", "product_category": "home_goods", "quantity": 10, "shipping_address": {"street": "104 Main St", "zip": "54697", "city": "Kyoto", "country": "Japan"}, "device_info": {"os": "Linux", "browser": "Firefox", "ip_address": "38.62.132.58"}, "customer_rating": 1, "discount_code": "DISCOUNT-850", "tax_amount": 0.0, "thread": 3, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                     |NULL|transaction_log|0        |13    |2025-04-30 08:58:51.562|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-2efbba1e", "timestamp": "2025-04-20T10:13:39.297908Z", "user_id": "USER-4367", "user_name": "Evelyn", "product_id": "PROD-752", "amount": 695.09, "currency": "EUR", "transaction_type": "withdrawal", "status": "completed", "location": {"city": "Hyderabad", "country": "India"}, "payment_method": "google_pay", "product_category": "food", "quantity": 8, "shipping_address": {"street": "135 Main St", "zip": "75369", "city": "Hyderabad", "country": "India"}, "device_info": {"os": "Windows", "browser": "Edge", "ip_address": "110.200.219.23"}, "customer_rating": 1, "discount_code": null, "tax_amount": 47.22, "thread": 2, "message_number": 3, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                      |NULL|transaction_log|0        |14    |2025-04-30 08:58:51.562|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-6aa923ea", "timestamp": "2025-04-18T16:30:23.297908Z", "user_id": "USER-8169", "user_name": "Young", "product_id": "PROD-544", "amount": 619.56, "currency": "USD", "transaction_type": "payment", "status": "failed", "location": {"city": "Montreal", "country": "Canada"}, "payment_method": "apple_pay", "product_category": "food", "quantity": 1, "shipping_address": {"street": "703 Main St", "zip": "31326", "city": "Montreal", "country": "Canada"}, "device_info": {"os": "iOS", "browser": "Edge", "ip_address": "14.208.6.58"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 4, "message_number": 2, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                                    |NULL|transaction_log|0        |15    |2025-04-30 08:58:51.562|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-823589d4", "timestamp": "2025-04-23T17:35:57.297908Z", "user_id": "USER-8710", "user_name": "John", "product_id": "PROD-691", "amount": 761.21, "currency": "USD", "transaction_type": "purchase", "status": "processing", "location": {"city": "Hangzhou", "country": "China"}, "payment_method": "paypal", "product_category": "electronics", "quantity": 4, "shipping_address": {"street": "225 Main St", "zip": "71697", "city": "Hangzhou", "country": "China"}, "device_info": {"os": "MacOS", "browser": "Edge", "ip_address": "232.199.237.186"}, "customer_rating": 4, "discount_code": null, "tax_amount": 0.0, "thread": 3, "message_number": 3, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                           |NULL|transaction_log|0        |16    |2025-04-30 08:58:51.563|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-28ee1cc6", "timestamp": "2025-04-24T17:29:54.297908Z", "user_id": "USER-3897", "user_name": "Emily", "product_id": "PROD-967", "amount": 256.5, "currency": "CAD", "transaction_type": "payment", "status": "completed", "location": {"city": "Frankfurt", "country": "Germany"}, "payment_method": "paypal", "product_category": "clothing", "quantity": 5, "shipping_address": {"street": "908 Main St", "zip": "77411", "city": "Frankfurt", "country": "Germany"}, "device_info": {"os": "Windows", "browser": "Safari", "ip_address": "194.117.212.233"}, "customer_rating": 1, "discount_code": null, "tax_amount": 46.77, "thread": 2, "message_number": 4, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                    |NULL|transaction_log|0        |17    |2025-04-30 08:58:51.563|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-b6bf779d", "timestamp": "2025-04-22T10:33:44.297908Z", "user_id": "USER-2650", "user_name": "Logan", "product_id": "PROD-412", "amount": 804.94, "currency": "GBP", "transaction_type": "withdrawal", "status": "completed", "location": {"city": "Berlin", "country": "Germany"}, "payment_method": "bank_transfer", "product_category": "clothing", "quantity": 2, "shipping_address": {"street": "563 Main St", "zip": "41212", "city": "Berlin", "country": "Germany"}, "device_info": {"os": "Linux", "browser": "Chrome", "ip_address": "254.242.152.200"}, "customer_rating": null, "discount_code": "DISCOUNT-802", "tax_amount": 104.49, "thread": 4, "message_number": 3, "timestamp_of_reception_log": "30/04/2025 10:58:51"}   |NULL|transaction_log|0        |18    |2025-04-30 08:58:51.564|2025-04-30 14:21:02.952|2025-04-30    |
|{"transaction_id": "TXN-f3f6c57d", "timestamp": "2025-04-27T14:48:48.297908Z", "user_id": "USER-8014", "user_name": "Sophia", "product_id": "PROD-731", "amount": 88.97, "currency": "EUR", "transaction_type": "purchase", "status": "processing", "location": {"city": "Birmingham", "country": "UK"}, "payment_method": "bank_transfer", "product_category": "electronics", "quantity": 10, "shipping_address": {"street": "199 Main St", "zip": "49001", "city": "Birmingham", "country": "UK"}, "device_info": {"os": "Linux", "browser": "Chrome", "ip_address": "155.36.236.229"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 3, "message_number": 4, "timestamp_of_reception_log": "30/04/2025 10:58:51"}                |NULL|transaction_log|0        |19    |2025-04-30 08:58:51.564|2025-04-30 14:21:02.952|2025-04-30    |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+---------------+---------+------+-----------------------+-----------------------+--------------+
only showing top 20 rows

25/04/30 14:21:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
25/04/30 14:21:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/commits/0 using temp file file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/commits/.0.aba2b354-1883-4c34-bf26-a7ef92a5d8d0.tmp
25/04/30 14:21:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/commits/.0.aba2b354-1883-4c34-bf26-a7ef92a5d8d0.tmp to file:/tmp/temporary-d88d9326-4f62-45cb-b4f3-e623a3214bcf/commits/0
25/04/30 14:21:19 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "35b22499-d557-49c2-a6e1-82975dec8afb",
  "runId" : "e44e329c-fee0-4b77-8e2e-0dc086f50220",
  "name" : null,
  "timestamp" : "2025-04-30T14:20:59.444Z",
  "batchId" : 0,
  "numInputRows" : 186,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 9.256955158512914,
  "durationMs" : {
    "addBatch" : 13187,
    "commitOffsets" : 301,
    "getBatch" : 198,
    "latestOffset" : 3399,
    "queryPlanning" : 2696,
    "triggerExecution" : 20092,
    "walCommit" : 195
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[transaction_log]]",
    "startOffset" : null,
    "endOffset" : {
      "transaction_log" : {
        "0" : 186
      }
    },
    "latestOffset" : {
      "transaction_log" : {
        "0" : 186
      }
    },
    "numInputRows" : 186,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 9.256955158512914,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@4c826c69",
    "numOutputRows" : 186
  }
}
25/04/30 14:21:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/04/30 14:21:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
