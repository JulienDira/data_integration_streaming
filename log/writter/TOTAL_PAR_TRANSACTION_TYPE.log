25/05/02 14:32:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:32:50,861 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:32:51 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:32:51 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:32:51 INFO SparkContext: Java version 11.0.24
25/05/02 14:32:51 INFO ResourceUtils: ==============================================================
25/05/02 14:32:51 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:32:51 INFO ResourceUtils: ==============================================================
25/05/02 14:32:51 INFO SparkContext: Submitted application: KafkaConsumer_TOTAL_PAR_TRANSACTION_TYPE
25/05/02 14:32:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:32:52 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:32:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:32:53 INFO SecurityManager: Changing view acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:32:56 INFO Utils: Successfully started service 'sparkDriver' on port 45495.
25/05/02 14:32:56 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:32:57 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:32:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:32:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:32:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:32:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e0390a11-5f99-4340-82f8-3f37ec653821
25/05/02 14:32:58 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:32:58 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:33:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/05/02 14:33:01 INFO Utils: Successfully started service 'SparkUI' on port 4045.
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:45495/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371463
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://3109b9eaa053:45495/jars/kafka-clients-3.3.1.jar with timestamp 1746196371463
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://3109b9eaa053:45495/jars/commons-pool2-2.11.1.jar with timestamp 1746196371463
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:45495/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371463
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://3109b9eaa053:45495/jars/postgresql-42.2.23.jar with timestamp 1746196371463
25/05/02 14:33:03 INFO Executor: Starting executor ID driver on host 3109b9eaa053
25/05/02 14:33:03 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:33:03 INFO Executor: Java version 11.0.24
25/05/02 14:33:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:33:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@58cca802 for default.
25/05/02 14:33:03 INFO Executor: Fetching spark://3109b9eaa053:45495/jars/postgresql-42.2.23.jar with timestamp 1746196371463
25/05/02 14:33:04 INFO TransportClientFactory: Successfully created connection to 3109b9eaa053/172.21.0.7:45495 after 694 ms (0 ms spent in bootstraps)
25/05/02 14:33:04 INFO Utils: Fetching spark://3109b9eaa053:45495/jars/postgresql-42.2.23.jar to /tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/fetchFileTemp18121518077127972982.tmp
25/05/02 14:33:05 INFO Executor: Adding file:/tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/postgresql-42.2.23.jar to class loader default
25/05/02 14:33:05 INFO Executor: Fetching spark://3109b9eaa053:45495/jars/commons-pool2-2.11.1.jar with timestamp 1746196371463
25/05/02 14:33:05 INFO Utils: Fetching spark://3109b9eaa053:45495/jars/commons-pool2-2.11.1.jar to /tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/fetchFileTemp12218859837785349714.tmp
25/05/02 14:33:05 INFO Executor: Adding file:/tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:33:05 INFO Executor: Fetching spark://3109b9eaa053:45495/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371463
25/05/02 14:33:05 INFO Utils: Fetching spark://3109b9eaa053:45495/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/fetchFileTemp7695454737757103266.tmp
25/05/02 14:33:05 INFO Executor: Adding file:/tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:05 INFO Executor: Fetching spark://3109b9eaa053:45495/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371463
25/05/02 14:33:05 INFO Utils: Fetching spark://3109b9eaa053:45495/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/fetchFileTemp18056351493896292968.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:45495/jars/kafka-clients-3.3.1.jar with timestamp 1746196371463
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:45495/jars/kafka-clients-3.3.1.jar to /tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/fetchFileTemp16301888018821865493.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-756a501a-559a-451d-9ec2-66646103e8b0/userFiles-e65a3100-9874-45ed-8b54-624527928d05/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:33:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36899.
25/05/02 14:33:06 INFO NettyBlockTransferService: Server created on 3109b9eaa053:36899
25/05/02 14:33:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:33:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3109b9eaa053, 36899, None)
25/05/02 14:33:06 INFO BlockManagerMasterEndpoint: Registering block manager 3109b9eaa053:36899 with 1007.8 MiB RAM, BlockManagerId(driver, 3109b9eaa053, 36899, None)
25/05/02 14:33:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3109b9eaa053, 36899, None)
25/05/02 14:33:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3109b9eaa053, 36899, None)
2025-05-02 14:33:11,269 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Session Spark créée.
2025-05-02 14:33:11,269 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic TOTAL_PAR_TRANSACTION_TYPE.
2025-05-02 14:33:11,269 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:33:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:33:11 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:33:30,659 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Données chargées avec succès.
2025-05-02 14:33:30,659 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Vérification du format initial.
25/05/02 14:33:32 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:33:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:33 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a resolved to file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a.
25/05/02 14:33:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/metadata using temp file file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/.metadata.24d61f13-1b94-4576-8ba2-ccb9f249faae.tmp
25/05/02 14:33:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/.metadata.24d61f13-1b94-4576-8ba2-ccb9f249faae.tmp to file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/metadata
25/05/02 14:33:36 INFO MicroBatchExecution: Starting [id = 2e5b4955-e83c-4156-ab51-1a7ae2298347, runId = 7594b7d8-019f-4075-a3bb-5b2849d95d83]. Use file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a to store the query checkpoint.
2025-05-02 14:33:36,967 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Données en cours de transformation...
25/05/02 14:33:37 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2a83a1fa] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@51295833]
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:37 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:37 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:33:43,066 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Données parsée avec succès !!!
root
 |-- TRANSACTION_TYPE: string (nullable = true)
 |-- TOTAL_AMOUNT: double (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:33:43,557 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Vérification des données chargées :
25/05/02 14:33:44 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:44 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85 resolved to file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85.
25/05/02 14:33:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/metadata using temp file file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/.metadata.31ef3b30-b5c7-4f5b-a307-a67b5864906a.tmp
25/05/02 14:33:46 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/.metadata.31ef3b30-b5c7-4f5b-a307-a67b5864906a.tmp to file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/metadata
25/05/02 14:33:46 INFO MicroBatchExecution: Starting [id = 0847a732-db6f-439d-868d-2dd903963fc5, runId = 3b52b6ea-4b64-4d17-8d23-5aed7186efa5]. Use file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85 to store the query checkpoint.
2025-05-02 14:33:47,060 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:33:47,060 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:33:47 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2a83a1fa] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@51295833]
25/05/02 14:33:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:47 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:47 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:47 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:47 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:47 INFO AppInfoParser: Kafka startTimeMs: 1746196427357
25/05/02 14:33:47 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b resolved to file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b.
25/05/02 14:33:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:48 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:48 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:48 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:48 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/metadata using temp file file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/.metadata.6620cd43-7407-4558-aa98-2698060c5925.tmp
25/05/02 14:33:48 INFO AppInfoParser: Kafka startTimeMs: 1746196428557
25/05/02 14:33:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/.metadata.6620cd43-7407-4558-aa98-2698060c5925.tmp to file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/metadata
25/05/02 14:33:50 INFO MicroBatchExecution: Starting [id = 51ff7e0f-e756-4b01-8efd-2e9ef94a318a, runId = cc0a6223-79ef-420b-bd54-6477e2dca280]. Use file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b to store the query checkpoint.
25/05/02 14:33:50 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2a83a1fa] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@51295833]
25/05/02 14:33:50 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:50 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:50 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:50 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:50 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:50 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:50 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:50 INFO AppInfoParser: Kafka startTimeMs: 1746196430757
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/sources/0/0 using temp file file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/sources/0/.0.f9869cae-0916-45ab-ab79-f30e699f458e.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/sources/0/0 using temp file file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/sources/0/.0.7120f6b1-8854-4559-8133-08eaf54c3169.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/sources/0/0 using temp file file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/sources/0/.0.d5d0d5c4-cc4d-45e8-9b5f-a6598997d47a.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/sources/0/.0.f9869cae-0916-45ab-ab79-f30e699f458e.tmp to file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/sources/0/0
25/05/02 14:33:53 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_PAR_TRANSACTION_TYPE":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/sources/0/.0.d5d0d5c4-cc4d-45e8-9b5f-a6598997d47a.tmp to file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/sources/0/0
25/05/02 14:33:53 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_PAR_TRANSACTION_TYPE":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/sources/0/.0.7120f6b1-8854-4559-8133-08eaf54c3169.tmp to file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/sources/0/0
25/05/02 14:33:53 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_PAR_TRANSACTION_TYPE":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/offsets/0 using temp file file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/offsets/.0.ebb7cb51-b4fe-4c45-bea3-4cd9ce4a0004.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/offsets/0 using temp file file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/offsets/.0.981aa590-bc2a-4a88-b9e0-cab7171c1961.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/offsets/0 using temp file file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/offsets/.0.d076e71b-3831-4b21-8bdc-1a2154686865.tmp
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/offsets/.0.981aa590-bc2a-4a88-b9e0-cab7171c1961.tmp to file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433358,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/offsets/.0.ebb7cb51-b4fe-4c45-bea3-4cd9ce4a0004.tmp to file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433360,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/offsets/.0.d076e71b-3831-4b21-8bdc-1a2154686865.tmp to file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433266,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:59 INFO IncrementalExecution: Current batch timestamp = 1746196433358
25/05/02 14:33:59 INFO IncrementalExecution: Current batch timestamp = 1746196433360
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO IncrementalExecution: Current batch timestamp = 1746196433360
25/05/02 14:34:02 INFO IncrementalExecution: Current batch timestamp = 1746196433358
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO IncrementalExecution: Current batch timestamp = 1746196433360
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]]. The input RDD has 1 partitions.
25/05/02 14:34:07 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:08 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:08 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/05/02 14:34:08 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:08 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:10 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1746196448357,WrappedArray(org.apache.spark.scheduler.StageInfo@5a5802ac),{spark.driver.port=45495, spark.submit.pyFiles=, spark.app.startTime=1746196371463, spark.executor.extraClassPath=/spark/jars/*, spark.rdd.compress=True, spark.driver.extraClassPath=/spark/jars/*, callSite.short=start at <unknown>:0, __is_continuous_processing=false, spark.jobGroup.id=3b52b6ea-4b64-4d17-8d23-5aed7186efa5, spark.sql.requireAllClusterKeysForDistribution=false, spark.app.submitTime=1746196367164, spark.sql.adaptive.enabled=false, spark.app.initial.jar.urls=spark://3109b9eaa053:45495/jars/postgresql-42.2.23.jar,spark://3109b9eaa053:45495/jars/commons-pool2-2.11.1.jar,spark://3109b9eaa053:45495/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,spark://3109b9eaa053:45495/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,spark://3109b9eaa053:45495/jars/kafka-clients-3.3.1.jar, spark.sql.execution.id=3, sql.streaming.queryId=0847a732-db6f-439d-868d-2dd903963fc5, spark.sql.warehouse.dir=file:/app/spark-warehouse, streaming.sql.batchId=0, callSite.long=org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:251)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
java.base/java.lang.reflect.Method.invoke(Unknown Source)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
py4j.Gateway.invoke(Gateway.java:282)
py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
py4j.commands.CallCommand.execute(CallCommand.java:79)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.base/java.lang.Thread.run(Unknown Source), spark.executor.memory=2g, spark.driver.memory=2g, spark.master=local[*], spark.job.interruptOnCancel=true, spark.repl.local.jars=file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/kafka-clients-3.3.1.jar,file:///spark/jars/commons-pool2-2.11.1.jar,file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/postgresql-42.2.23.jar, spark.executor.id=driver, spark.app.name=KafkaConsumer_TOTAL_PAR_TRANSACTION_TYPE, spark.submit.deployMode=client, spark.driver.host=3109b9eaa053, spark.sql.caseSensitive=true, spark.app.id=local-1746196382269, spark.job.description=
id = 0847a732-db6f-439d-868d-2dd903963fc5
runId = 3b52b6ea-4b64-4d17-8d23-5aed7186efa5
batch = 0, spark.sql.cbo.enabled=false, spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false, spark.sql.execution.root.id=0, spark.jars=file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/kafka-clients-3.3.1.jar,file:///spark/jars/commons-pool2-2.11.1.jar,file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/postgresql-42.2.23.jar, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false}) by listener AppStatusListener took 1.60123572s.
25/05/02 14:34:13 INFO CodeGenerator: Code generated in 5697.971212 ms
25/05/02 14:34:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
25/05/02 14:34:13 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 19.4 KiB, free 1007.8 MiB)
25/05/02 14:34:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 1007.8 MiB)
25/05/02 14:34:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3109b9eaa053:36899 (size: 9.2 KiB, free: 1007.8 MiB)
25/05/02 14:34:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:16 INFO CodeGenerator: Code generated in 501.088746 ms
25/05/02 14:34:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/05/02 14:34:16 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:17 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
25/05/02 14:34:17 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:17 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.6 KiB, free 1007.8 MiB)
25/05/02 14:34:17 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 1007.8 MiB)
25/05/02 14:34:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3109b9eaa053:36899 (size: 4.5 KiB, free: 1007.8 MiB)
25/05/02 14:34:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9075 bytes) 
25/05/02 14:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/05/02 14:34:18 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:18 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
25/05/02 14:34:18 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:18 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/05/02 14:34:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 33.1 KiB, free 1007.8 MiB)
25/05/02 14:34:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 1007.8 MiB)
25/05/02 14:34:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3109b9eaa053:36899 (size: 15.2 KiB, free: 1007.8 MiB)
25/05/02 14:34:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
25/05/02 14:34:21 INFO CodeGenerator: Code generated in 485.395282 ms
25/05/02 14:34:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_PAR_TRANSACTION_TYPE-0 fromOffset=0 untilOffset=46, for query queryId=0847a732-db6f-439d-868d-2dd903963fc5 batchId=0 taskId=0 partitionId=0
25/05/02 14:34:23 INFO CodeGenerator: Code generated in 305.135145 ms
25/05/02 14:34:23 INFO CodeGenerator: Code generated in 204.434733 ms
25/05/02 14:34:24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:24 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:24 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:24 INFO AppInfoParser: Kafka startTimeMs: 1746196464666
25/05/02 14:34:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor-1, groupId=spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor] Assigned to partition(s): TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor-1, groupId=spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor] Seeking to offset 0 for partition TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:25 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor-1, groupId=spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor] Resetting the last seen epoch of partition TOTAL_PAR_TRANSACTION_TYPE-0 to 0 since the associated topicId changed from null to x8RAvzq3RpS_cIzBl4VBJg
25/05/02 14:34:25 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor-1, groupId=spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor-1, groupId=spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor] Seeking to earliest offset of partition TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor-1, groupId=spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor] Resetting offset for partition TOTAL_PAR_TRANSACTION_TYPE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor-1, groupId=spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor] Seeking to latest offset of partition TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor-1, groupId=spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor] Resetting offset for partition TOTAL_PAR_TRANSACTION_TYPE-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:27 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:28 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/05/02 14:34:28 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_PAR_TRANSACTION_TYPE-0 groupId=spark-kafka-source-6fe9849c-7651-4834-a0b9-e30a6d3d6c6e--475914959-executor read 46 records through 1 polls (polled  out 46 records), taking 1205522511 nanos, during time span of 3294254372 nanos.
25/05/02 14:34:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5086 bytes result sent to driver
25/05/02 14:34:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9076 bytes) 
25/05/02 14:34:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11203 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
25/05/02 14:34:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/05/02 14:34:28 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 19.504 s
25/05/02 14:34:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/05/02 14:34:29 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 22.001254 s
25/05/02 14:34:29 INFO CodeGenerator: Code generated in 103.4551 ms
25/05/02 14:34:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] is committing.
25/05/02 14:34:29 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_PAR_TRANSACTION_TYPE-0 fromOffset=0 untilOffset=46, for query queryId=2e5b4955-e83c-4156-ab51-1a7ae2298347 batchId=0 taskId=1 partitionId=0
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/02 14:34:29 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:29 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:29 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:29 INFO AppInfoParser: Kafka startTimeMs: 1746196469859
25/05/02 14:34:29 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor-2, groupId=spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor] Assigned to partition(s): TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:29 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor-2, groupId=spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor] Seeking to offset 0 for partition TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor-2, groupId=spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor] Resetting the last seen epoch of partition TOTAL_PAR_TRANSACTION_TYPE-0 to 0 since the associated topicId changed from null to x8RAvzq3RpS_cIzBl4VBJg
25/05/02 14:34:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor-2, groupId=spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor-2, groupId=spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor] Seeking to earliest offset of partition TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor-2, groupId=spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor] Resetting offset for partition TOTAL_PAR_TRANSACTION_TYPE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor-2, groupId=spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor] Seeking to latest offset of partition TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor-2, groupId=spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor] Resetting offset for partition TOTAL_PAR_TRANSACTION_TYPE-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:30 INFO CodeGenerator: Code generated in 195.25687 ms
25/05/02 14:34:30 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:31 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
25/05/02 14:34:31 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_PAR_TRANSACTION_TYPE-0 groupId=spark-kafka-source-a9d7930d-f6d2-4e25-861f-ea64856b922a--1712701600-executor read 46 records through 1 polls (polled  out 46 records), taking 899592004 nanos, during time span of 1299529675 nanos.
25/05/02 14:34:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 5746 bytes result sent to driver
25/05/02 14:34:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9074 bytes) 
25/05/02 14:34:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
25/05/02 14:34:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2797 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/05/02 14:34:31 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 14.198 s
25/05/02 14:34:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/05/02 14:34:31 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 17.702011 s
25/05/02 14:34:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/02 14:34:33 INFO CodeGenerator: Code generated in 106.490274 ms
25/05/02 14:34:33 INFO CodeGenerator: Code generated in 96.224582 ms
25/05/02 14:34:33 INFO CodeGenerator: Code generated in 6.183188 ms
25/05/02 14:34:45 INFO CodeGenerator: Code generated in 105.4897 ms
25/05/02 14:34:45 INFO CodeGenerator: Code generated in 100.119998 ms
25/05/02 14:34:45 INFO CodeGenerator: Code generated in 107.310934 ms
25/05/02 14:34:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_PAR_TRANSACTION_TYPE-0 fromOffset=0 untilOffset=46, for query queryId=51ff7e0f-e756-4b01-8efd-2e9ef94a318a batchId=0 taskId=2 partitionId=0
+----------------+------------------+----------------------+--------------+
|TRANSACTION_TYPE|TOTAL_AMOUNT      |ingestion_time        |ingestion_date|
+----------------+------------------+----------------------+--------------+
|withdrawal      |653.3             |2025-05-02 14:33:53.36|2025-05-02    |
|purchase        |798.79            |2025-05-02 14:33:53.36|2025-05-02    |
|payment         |158.15            |2025-05-02 14:33:53.36|2025-05-02    |
|purchase        |925.0999999999999 |2025-05-02 14:33:53.36|2025-05-02    |
|purchase        |1285.0            |2025-05-02 14:33:53.36|2025-05-02    |
|payment         |205.77            |2025-05-02 14:33:53.36|2025-05-02    |
|withdrawal      |1368.31           |2025-05-02 14:33:53.36|2025-05-02    |
|refund          |563.05            |2025-05-02 14:33:53.36|2025-05-02    |
|refund          |937.6899999999999 |2025-05-02 14:33:53.36|2025-05-02    |
|refund          |1115.1599999999999|2025-05-02 14:33:53.36|2025-05-02    |
+----------------+------------------+----------------------+--------------+
only showing top 10 rows
25/05/02 14:34:46 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer


25/05/02 14:34:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] committed.
+----------+--------------------+
|       key|               value|
+----------+--------------------+
|withdrawal|{"TOTAL_AMOUNT":6...|
|  purchase|{"TOTAL_AMOUNT":7...|
|   payment|{"TOTAL_AMOUNT":1...|
|  purchase|{"TOTAL_AMOUNT":9...|
|  purchase|{"TOTAL_AMOUNT":1...|
|   payment|{"TOTAL_AMOUNT":2...|
|withdrawal|{"TOTAL_AMOUNT":1...|
|    refund|{"TOTAL_AMOUNT":5...|
|    refund|{"TOTAL_AMOUNT":9...|
|    refund|{"TOTAL_AMOUNT":1...|
|    refund|{"TOTAL_AMOUNT":1...|
|    refund|{"TOTAL_AMOUNT":1...|
|  purchase|{"TOTAL_AMOUNT":1...|
|    refund|{"TOTAL_AMOUNT":2...|
|  purchase|{"TOTAL_AMOUNT":2...|
|    refund|{"TOTAL_AMOUNT":2...|
|   payment|{"TOTAL_AMOUNT":8...|
|   payment|{"TOTAL_AMOUNT":1...|
|  purchase|{"TOTAL_AMOUNT":3...|
|withdrawal|{"TOTAL_AMOUNT":1...|
+----------+--------------------+
only showing top 20 rows
25/05/02 14:34:46 INFO AppInfoParser: Kafka version: 3.3.1

25/05/02 14:34:46 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:46 INFO AppInfoParser: Kafka startTimeMs: 1746196486259
25/05/02 14:34:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
25/05/02 14:34:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor-3, groupId=spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor] Assigned to partition(s): TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor-3, groupId=spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor] Seeking to offset 0 for partition TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/commits/0 using temp file file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/commits/.0.16a30df1-ea39-4a4d-b107-ad2feddbb8c1.tmp
25/05/02 14:34:46 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor-3, groupId=spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor] Resetting the last seen epoch of partition TOTAL_PAR_TRANSACTION_TYPE-0 to 0 since the associated topicId changed from null to x8RAvzq3RpS_cIzBl4VBJg
25/05/02 14:34:46 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor-3, groupId=spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/commits/0 using temp file file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/commits/.0.ad4108c9-580f-4940-a71d-32d42ce19c34.tmp
25/05/02 14:34:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor-3, groupId=spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor] Seeking to earliest offset of partition TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor-3, groupId=spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor] Resetting offset for partition TOTAL_PAR_TRANSACTION_TYPE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor-3, groupId=spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor] Seeking to latest offset of partition TOTAL_PAR_TRANSACTION_TYPE-0
25/05/02 14:34:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor-3, groupId=spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor] Resetting offset for partition TOTAL_PAR_TRANSACTION_TYPE-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/commits/.0.16a30df1-ea39-4a4d-b107-ad2feddbb8c1.tmp to file:/tmp/temporary-f8db923e-9fe4-43ea-b73a-0b8570a74d85/commits/0
25/05/02 14:34:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/commits/.0.ad4108c9-580f-4940-a71d-32d42ce19c34.tmp to file:/tmp/temporary-ed231168-84e4-43c9-8ca3-068164c0c06a/commits/0
25/05/02 14:34:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "0847a732-db6f-439d-868d-2dd903963fc5",
  "runId" : "3b52b6ea-4b64-4d17-8d23-5aed7186efa5",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:47.358Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.757825370675453,
  "durationMs" : {
    "addBatch" : 43995,
    "commitOffsets" : 1597,
    "getBatch" : 195,
    "latestOffset" : 5499,
    "queryPlanning" : 7502,
    "triggerExecution" : 60700,
    "walCommit" : 1302
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_PAR_TRANSACTION_TYPE]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_PAR_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TOTAL_PAR_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.757825370675453,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@c15b28b",
    "numOutputRows" : 46
  }
}
25/05/02 14:34:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "2e5b4955-e83c-4156-ab51-1a7ae2298347",
  "runId" : "7594b7d8-019f-4075-a3bb-5b2849d95d83",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:37.465Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.6516226821356225,
  "durationMs" : {
    "addBatch" : 44393,
    "commitOffsets" : 1299,
    "getBatch" : 3,
    "latestOffset" : 15504,
    "queryPlanning" : 7502,
    "triggerExecution" : 70593,
    "walCommit" : 1497
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_PAR_TRANSACTION_TYPE]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_PAR_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TOTAL_PAR_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.6516226821356225,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@c15b28b",
    "numOutputRows" : 46
  }
}
25/05/02 14:34:49 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_PAR_TRANSACTION_TYPE-0 groupId=spark-kafka-source-b56278e2-3752-481c-8574-d064f9e47b25-857713940-executor read 46 records through 1 polls (polled  out 46 records), taking 1001561713 nanos, during time span of 2604039159 nanos.
25/05/02 14:34:49 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1718 bytes result sent to driver
25/05/02 14:34:49 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 18302 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:49 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/05/02 14:34:49 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 31.199 s
25/05/02 14:34:49 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
25/05/02 14:34:49 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 31.906857 s
25/05/02 14:34:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/commits/0 using temp file file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/commits/.0.93f49361-c345-480f-9996-875f6b53d30f.tmp
25/05/02 14:34:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/commits/.0.93f49361-c345-480f-9996-875f6b53d30f.tmp to file:/tmp/temporary-d024a42f-6916-4531-9e75-8b2d05ffd84b/commits/0
25/05/02 14:34:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "51ff7e0f-e756-4b01-8efd-2e9ef94a318a",
  "runId" : "cc0a6223-79ef-420b-bd54-6477e2dca280",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:50.165Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.7444449838973314,
  "durationMs" : {
    "addBatch" : 47901,
    "commitOffsets" : 1494,
    "getBatch" : 300,
    "latestOffset" : 2903,
    "queryPlanning" : 7596,
    "triggerExecution" : 61791,
    "walCommit" : 1101
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_PAR_TRANSACTION_TYPE]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_PAR_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TOTAL_PAR_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.7444449838973314,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
25/05/02 14:34:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:34:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:51 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
25/05/02 14:38:51 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
25/05/02 14:38:51 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
25/05/02 14:38:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:39:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:46:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:46:53,425 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:46:54 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:46:54 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:46:54 INFO SparkContext: Java version 11.0.24
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO SparkContext: Submitted application: KafkaConsumer_TOTAL_PAR_TRANSACTION_TYPE
25/05/02 14:46:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:46:55 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:46:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:46:56 INFO SecurityManager: Changing view acls to: root
25/05/02 14:46:56 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:46:56 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:46:56 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:46:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:47:01 INFO Utils: Successfully started service 'sparkDriver' on port 39133.
25/05/02 14:47:01 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:47:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-54933ce2-76ed-432f-bbc8-ba14b25009c4
25/05/02 14:47:03 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:47:03 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:47:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:47:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:39133/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214138
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://5e919d8ee3aa:39133/jars/kafka-clients-3.3.1.jar with timestamp 1746197214138
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://5e919d8ee3aa:39133/jars/commons-pool2-2.11.1.jar with timestamp 1746197214138
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:39133/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214138
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://5e919d8ee3aa:39133/jars/postgresql-42.2.23.jar with timestamp 1746197214138
25/05/02 14:47:09 INFO Executor: Starting executor ID driver on host 5e919d8ee3aa
25/05/02 14:47:09 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:47:09 INFO Executor: Java version 11.0.24
25/05/02 14:47:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:47:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7a590221 for default.
25/05/02 14:47:09 INFO Executor: Fetching spark://5e919d8ee3aa:39133/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214138
25/05/02 14:47:11 INFO TransportClientFactory: Successfully created connection to 5e919d8ee3aa/172.21.0.2:39133 after 692 ms (0 ms spent in bootstraps)
25/05/02 14:47:11 INFO Utils: Fetching spark://5e919d8ee3aa:39133/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/fetchFileTemp16479007459033028679.tmp
25/05/02 14:47:11 INFO Executor: Adding file:/tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:11 INFO Executor: Fetching spark://5e919d8ee3aa:39133/jars/kafka-clients-3.3.1.jar with timestamp 1746197214138
25/05/02 14:47:11 INFO Utils: Fetching spark://5e919d8ee3aa:39133/jars/kafka-clients-3.3.1.jar to /tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/fetchFileTemp10446966001536615305.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:39133/jars/postgresql-42.2.23.jar with timestamp 1746197214138
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:39133/jars/postgresql-42.2.23.jar to /tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/fetchFileTemp7486253561394713437.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/postgresql-42.2.23.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:39133/jars/commons-pool2-2.11.1.jar with timestamp 1746197214138
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:39133/jars/commons-pool2-2.11.1.jar to /tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/fetchFileTemp10095982752639660075.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:47:13 INFO Executor: Fetching spark://5e919d8ee3aa:39133/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214138
25/05/02 14:47:13 INFO Utils: Fetching spark://5e919d8ee3aa:39133/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/fetchFileTemp9989985472234273662.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/userFiles-49248d5b-d69d-42fc-823e-8bc82025a49b/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37805.
25/05/02 14:47:13 INFO NettyBlockTransferService: Server created on 5e919d8ee3aa:37805
25/05/02 14:47:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:47:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5e919d8ee3aa, 37805, None)
25/05/02 14:47:13 INFO BlockManagerMasterEndpoint: Registering block manager 5e919d8ee3aa:37805 with 1007.8 MiB RAM, BlockManagerId(driver, 5e919d8ee3aa, 37805, None)
25/05/02 14:47:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5e919d8ee3aa, 37805, None)
25/05/02 14:47:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5e919d8ee3aa, 37805, None)
2025-05-02 14:47:18,327 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Session Spark créée.
2025-05-02 14:47:18,327 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic TOTAL_PAR_TRANSACTION_TYPE.
2025-05-02 14:47:18,327 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:47:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:47:18 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:47:39,431 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Données chargées avec succès.
2025-05-02 14:47:39,432 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Vérification du format initial.
25/05/02 14:47:41 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:47:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-46e7f467-e282-4224-884a-0c7d08131ab7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:42 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-46e7f467-e282-4224-884a-0c7d08131ab7 resolved to file:/tmp/temporary-46e7f467-e282-4224-884a-0c7d08131ab7.
25/05/02 14:47:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-46e7f467-e282-4224-884a-0c7d08131ab7/metadata using temp file file:/tmp/temporary-46e7f467-e282-4224-884a-0c7d08131ab7/.metadata.91557574-2554-483c-b40a-ac44ca83ac2a.tmp
25/05/02 14:47:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-46e7f467-e282-4224-884a-0c7d08131ab7/.metadata.91557574-2554-483c-b40a-ac44ca83ac2a.tmp to file:/tmp/temporary-46e7f467-e282-4224-884a-0c7d08131ab7/metadata
25/05/02 14:47:46 INFO MicroBatchExecution: Starting [id = fca9cce3-e131-4f4d-8b2f-7e5e2740c019, runId = 1fcef1cd-191f-4afa-ab20-9471db39f323]. Use file:/tmp/temporary-46e7f467-e282-4224-884a-0c7d08131ab7 to store the query checkpoint.
2025-05-02 14:47:47,034 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Données en cours de transformation...
25/05/02 14:47:47 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@42852846] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@478e59e7]
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:47 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:47:51,624 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Données parsée avec succès !!!
root
 |-- TRANSACTION_TYPE: string (nullable = true)
 |-- TOTAL_AMOUNT: double (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:47:51,726 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Vérification des données chargées :
25/05/02 14:47:52 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5f87e3d1-94f8-4351-935d-44ca521ef867. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:52 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-5f87e3d1-94f8-4351-935d-44ca521ef867 resolved to file:/tmp/temporary-5f87e3d1-94f8-4351-935d-44ca521ef867.
25/05/02 14:47:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-5f87e3d1-94f8-4351-935d-44ca521ef867/metadata using temp file file:/tmp/temporary-5f87e3d1-94f8-4351-935d-44ca521ef867/.metadata.7caa4c44-fcbb-43fa-815a-de2434866e25.tmp
25/05/02 14:47:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-5f87e3d1-94f8-4351-935d-44ca521ef867/.metadata.7caa4c44-fcbb-43fa-815a-de2434866e25.tmp to file:/tmp/temporary-5f87e3d1-94f8-4351-935d-44ca521ef867/metadata
25/05/02 14:47:55 INFO MicroBatchExecution: Starting [id = 4d0492a7-12bc-458a-964c-8460d28b59d6, runId = 8ea5ad24-1e73-4cea-b8c5-7b418b218c66]. Use file:/tmp/temporary-5f87e3d1-94f8-4351-935d-44ca521ef867 to store the query checkpoint.
2025-05-02 14:47:55,326 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:47:55,326 - TOTAL_PAR_TRANSACTION_TYPE - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:47:55 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@42852846] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@478e59e7]
25/05/02 14:47:55 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:55 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:55 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:55 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:55 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:56 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3c8d93aa-afa1-4718-bb01-e85b42f2c90b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:56 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-3c8d93aa-afa1-4718-bb01-e85b42f2c90b resolved to file:/tmp/temporary-3c8d93aa-afa1-4718-bb01-e85b42f2c90b.
25/05/02 14:47:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3c8d93aa-afa1-4718-bb01-e85b42f2c90b/metadata using temp file file:/tmp/temporary-3c8d93aa-afa1-4718-bb01-e85b42f2c90b/.metadata.ef812517-7cff-4a90-ba0e-882fdfac474f.tmp
25/05/02 14:47:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:57 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:57 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:57 INFO AppInfoParser: Kafka startTimeMs: 1746197277329
25/05/02 14:47:57 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:57 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:57 INFO AppInfoParser: Kafka startTimeMs: 1746197277331
25/05/02 14:47:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3c8d93aa-afa1-4718-bb01-e85b42f2c90b/.metadata.ef812517-7cff-4a90-ba0e-882fdfac474f.tmp to file:/tmp/temporary-3c8d93aa-afa1-4718-bb01-e85b42f2c90b/metadata
25/05/02 14:47:58 INFO MicroBatchExecution: Starting [id = 51b9b48a-0756-42a1-a230-fae6d2960aa5, runId = 5c26480a-5f00-4294-9c55-e44060dff7ca]. Use file:/tmp/temporary-3c8d93aa-afa1-4718-bb01-e85b42f2c90b to store the query checkpoint.
25/05/02 14:47:58 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@42852846] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@478e59e7]
25/05/02 14:47:58 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:58 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:58 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:58 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:59 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:59 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:59 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:59 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:59 INFO AppInfoParser: Kafka startTimeMs: 1746197279329
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282732
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282732
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282732
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-4 unregistered
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-5 unregistered
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-6 unregistered
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283927
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283927
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283927
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:05 INFO AppInfoParser: App info kafka.admin.client for adminclient-9 unregistered
25/05/02 14:48:05 INFO AppInfoParser: App info kafka.admin.client for adminclient-7 unregistered
25/05/02 14:48:05 INFO AppInfoParser: App info kafka.admin.client for adminclient-8 unregistered
25/05/02 14:48:05 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:05 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:05 INFO Metrics: Metrics reporters closed
25/05/02 14:48:05 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:05 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:05 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:05 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:05 INFO Metrics: Metrics reporters closed
25/05/02 14:48:05 INFO Metrics: Metrics reporters closed
25/05/02 14:48:05 ERROR MicroBatchExecution: Query [id = 4d0492a7-12bc-458a-964c-8460d28b59d6, runId = 8ea5ad24-1e73-4cea-b8c5-7b418b218c66] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:05 ERROR MicroBatchExecution: Query [id = fca9cce3-e131-4f4d-8b2f-7e5e2740c019, runId = 1fcef1cd-191f-4afa-ab20-9471db39f323] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:05 INFO MicroBatchExecution: Async log purge executor pool for query [id = 4d0492a7-12bc-458a-964c-8460d28b59d6, runId = 8ea5ad24-1e73-4cea-b8c5-7b418b218c66] has been shutdown
25/05/02 14:48:05 ERROR MicroBatchExecution: Query [id = 51b9b48a-0756-42a1-a230-fae6d2960aa5, runId = 5c26480a-5f00-4294-9c55-e44060dff7ca] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:05 INFO MicroBatchExecution: Async log purge executor pool for query [id = fca9cce3-e131-4f4d-8b2f-7e5e2740c019, runId = 1fcef1cd-191f-4afa-ab20-9471db39f323] has been shutdown
25/05/02 14:48:05 INFO MicroBatchExecution: Async log purge executor pool for query [id = 51b9b48a-0756-42a1-a230-fae6d2960aa5, runId = 5c26480a-5f00-4294-9c55-e44060dff7ca] has been shutdown
Traceback (most recent call last):
  File "/app/consumer-table/writter_TOTAL_PAR_TRANSACTION_TYPE.py", line 104, in <module>
    df_parsed
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 51b9b48a-0756-42a1-a230-fae6d2960aa5, runId = 5c26480a-5f00-4294-9c55-e44060dff7ca] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:06 INFO SparkContext: Invoking stop() from shutdown hook
25/05/02 14:48:06 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/05/02 14:48:06 INFO SparkUI: Stopped Spark web UI at http://5e919d8ee3aa:4040
25/05/02 14:48:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/05/02 14:48:06 INFO MemoryStore: MemoryStore cleared
25/05/02 14:48:06 INFO BlockManager: BlockManager stopped
25/05/02 14:48:07 INFO BlockManagerMaster: BlockManagerMaster stopped
25/05/02 14:48:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/05/02 14:48:07 INFO SparkContext: Successfully stopped SparkContext
25/05/02 14:48:07 INFO ShutdownHookManager: Shutdown hook called
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ae157aa-9bdd-49bf-bb51-d8075a74ba1f
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-5f87e3d1-94f8-4351-935d-44ca521ef867
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-3c8d93aa-afa1-4718-bb01-e85b42f2c90b
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-c7a630a0-52c6-47fb-9e80-55f02a7898c0/pyspark-cc1b30ec-d58f-405c-98d5-915c67febf70
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-46e7f467-e282-4224-884a-0c7d08131ab7
