25/05/02 14:32:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:32:50,869 - TRANSACTION_STATUS_EVOLUTION - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:32:51 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:32:51 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:32:51 INFO SparkContext: Java version 11.0.24
25/05/02 14:32:52 INFO ResourceUtils: ==============================================================
25/05/02 14:32:52 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:32:52 INFO ResourceUtils: ==============================================================
25/05/02 14:32:52 INFO SparkContext: Submitted application: KafkaConsumer_TRANSACTION_STATUS_EVOLUTION
25/05/02 14:32:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:32:52 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:32:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:32:53 INFO SecurityManager: Changing view acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:32:56 INFO Utils: Successfully started service 'sparkDriver' on port 38559.
25/05/02 14:32:56 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:32:57 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:32:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:32:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:32:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:32:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1b8cd36d-eac6-4eb5-8d31-159f7a785647
25/05/02 14:32:58 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:32:58 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:33:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/05/02 14:33:00 INFO Utils: Successfully started service 'SparkUI' on port 4042.
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:38559/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371469
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://3109b9eaa053:38559/jars/kafka-clients-3.3.1.jar with timestamp 1746196371469
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://3109b9eaa053:38559/jars/commons-pool2-2.11.1.jar with timestamp 1746196371469
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:38559/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371469
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://3109b9eaa053:38559/jars/postgresql-42.2.23.jar with timestamp 1746196371469
25/05/02 14:33:03 INFO Executor: Starting executor ID driver on host 3109b9eaa053
25/05/02 14:33:03 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:33:03 INFO Executor: Java version 11.0.24
25/05/02 14:33:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:33:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@33eac4aa for default.
25/05/02 14:33:03 INFO Executor: Fetching spark://3109b9eaa053:38559/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371469
25/05/02 14:33:04 INFO TransportClientFactory: Successfully created connection to 3109b9eaa053/172.21.0.7:38559 after 596 ms (0 ms spent in bootstraps)
25/05/02 14:33:04 INFO Utils: Fetching spark://3109b9eaa053:38559/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/fetchFileTemp9837224895123966210.tmp
25/05/02 14:33:05 INFO Executor: Adding file:/tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:05 INFO Executor: Fetching spark://3109b9eaa053:38559/jars/postgresql-42.2.23.jar with timestamp 1746196371469
25/05/02 14:33:05 INFO Utils: Fetching spark://3109b9eaa053:38559/jars/postgresql-42.2.23.jar to /tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/fetchFileTemp15053334585070431045.tmp
25/05/02 14:33:05 INFO Executor: Adding file:/tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/postgresql-42.2.23.jar to class loader default
25/05/02 14:33:05 INFO Executor: Fetching spark://3109b9eaa053:38559/jars/kafka-clients-3.3.1.jar with timestamp 1746196371469
25/05/02 14:33:05 INFO Utils: Fetching spark://3109b9eaa053:38559/jars/kafka-clients-3.3.1.jar to /tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/fetchFileTemp10382083161032423411.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:38559/jars/commons-pool2-2.11.1.jar with timestamp 1746196371469
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:38559/jars/commons-pool2-2.11.1.jar to /tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/fetchFileTemp1702727744434264892.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:38559/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371469
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:38559/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/fetchFileTemp5390077198005063021.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-e9858655-f6f9-48e6-9c09-cf334e63e360/userFiles-3407ce4c-ec73-421c-b89f-54b39881b5f2/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36961.
25/05/02 14:33:06 INFO NettyBlockTransferService: Server created on 3109b9eaa053:36961
25/05/02 14:33:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:33:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3109b9eaa053, 36961, None)
25/05/02 14:33:06 INFO BlockManagerMasterEndpoint: Registering block manager 3109b9eaa053:36961 with 1007.8 MiB RAM, BlockManagerId(driver, 3109b9eaa053, 36961, None)
25/05/02 14:33:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3109b9eaa053, 36961, None)
25/05/02 14:33:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3109b9eaa053, 36961, None)
2025-05-02 14:33:10,964 - TRANSACTION_STATUS_EVOLUTION - INFO - Session Spark créée.
2025-05-02 14:33:10,965 - TRANSACTION_STATUS_EVOLUTION - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic TRANSACTION_STATUS_EVOLUTION.
2025-05-02 14:33:10,965 - TRANSACTION_STATUS_EVOLUTION - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:33:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:33:11 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:33:30,465 - TRANSACTION_STATUS_EVOLUTION - INFO - Données chargées avec succès.
2025-05-02 14:33:30,465 - TRANSACTION_STATUS_EVOLUTION - INFO - Vérification du format initial.
25/05/02 14:33:32 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:33:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:34 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13 resolved to file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13.
25/05/02 14:33:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/metadata using temp file file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/.metadata.a05f6a06-c021-41ea-829a-1c4e7de73fc0.tmp
25/05/02 14:33:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/.metadata.a05f6a06-c021-41ea-829a-1c4e7de73fc0.tmp to file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/metadata
25/05/02 14:33:36 INFO MicroBatchExecution: Starting [id = 0812f9ed-ec51-4061-9816-097fad065582, runId = 8e1f7d4b-6fd5-4586-b96d-c15d237e5913]. Use file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13 to store the query checkpoint.
2025-05-02 14:33:37,064 - TRANSACTION_STATUS_EVOLUTION - INFO - Données en cours de transformation...
25/05/02 14:33:37 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@595e12c1] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@40d1a9ed]
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:37 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:37 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:33:42,758 - TRANSACTION_STATUS_EVOLUTION - INFO - Données parsée avec succès !!!
root
 |-- TRANSACTION_ID: string (nullable = true)
 |-- LATEST_STATUS: string (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:33:42,857 - TRANSACTION_STATUS_EVOLUTION - INFO - Vérification des données chargées :
25/05/02 14:33:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:43 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7 resolved to file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7.
25/05/02 14:33:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/metadata using temp file file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/.metadata.dd8760e9-aa51-4400-b2d3-6daafc17605c.tmp
25/05/02 14:33:45 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/.metadata.dd8760e9-aa51-4400-b2d3-6daafc17605c.tmp to file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/metadata
25/05/02 14:33:46 INFO MicroBatchExecution: Starting [id = 0d335e24-6b74-4d15-a9f5-b3b1e655e49f, runId = 6ecc0647-2b5a-4ca8-be6d-dd4eaa05757d]. Use file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7 to store the query checkpoint.
2025-05-02 14:33:46,561 - TRANSACTION_STATUS_EVOLUTION - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:33:46,561 - TRANSACTION_STATUS_EVOLUTION - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:33:46 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@595e12c1] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@40d1a9ed]
25/05/02 14:33:46 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:46 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:46 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:46 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:46 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:46 INFO AppInfoParser: Kafka startTimeMs: 1746196426957
25/05/02 14:33:46 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:47 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:47 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a resolved to file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a.
25/05/02 14:33:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:47 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/metadata using temp file file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/.metadata.c18cf783-6b57-4de3-99d7-2309279ec1a7.tmp
25/05/02 14:33:47 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:48 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:48 INFO AppInfoParser: Kafka startTimeMs: 1746196427957
25/05/02 14:33:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/.metadata.c18cf783-6b57-4de3-99d7-2309279ec1a7.tmp to file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/metadata
25/05/02 14:33:49 INFO MicroBatchExecution: Starting [id = b72490a0-d243-44e1-b0ee-afa0e4743cd1, runId = c69967ef-87b3-4f48-9642-eb82e1b8542a]. Use file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a to store the query checkpoint.
25/05/02 14:33:49 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@595e12c1] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@40d1a9ed]
25/05/02 14:33:49 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:49 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:49 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:49 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:49 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:50 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:50 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:50 INFO AppInfoParser: Kafka startTimeMs: 1746196430061
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/sources/0/0 using temp file file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/sources/0/.0.8ccecf86-954d-4358-bb3a-7f7c6232807d.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/sources/0/0 using temp file file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/sources/0/.0.4af102d3-3ff3-442c-8799-efcd0ec4c7cb.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/sources/0/0 using temp file file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/sources/0/.0.1af44011-ece5-4030-8549-b3257f673f5f.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/sources/0/.0.8ccecf86-954d-4358-bb3a-7f7c6232807d.tmp to file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/sources/0/0
25/05/02 14:33:52 INFO KafkaMicroBatchStream: Initial offsets: {"TRANSACTION_STATUS_EVOLUTION":{"0":0}}
25/05/02 14:33:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/sources/0/.0.1af44011-ece5-4030-8549-b3257f673f5f.tmp to file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/sources/0/0
25/05/02 14:33:52 INFO KafkaMicroBatchStream: Initial offsets: {"TRANSACTION_STATUS_EVOLUTION":{"0":0}}
25/05/02 14:33:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/sources/0/.0.4af102d3-3ff3-442c-8799-efcd0ec4c7cb.tmp to file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/sources/0/0
25/05/02 14:33:53 INFO KafkaMicroBatchStream: Initial offsets: {"TRANSACTION_STATUS_EVOLUTION":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/offsets/0 using temp file file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/offsets/.0.382f88c4-40bf-4843-973f-2af17d2f6e4e.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/offsets/0 using temp file file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/offsets/.0.1a71027e-f69a-45e4-abd3-58372dc1c583.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/offsets/0 using temp file file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/offsets/.0.df27745b-ac11-4df1-b69d-f290843901eb.tmp
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/offsets/.0.1a71027e-f69a-45e4-abd3-58372dc1c583.tmp to file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196432966,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/offsets/.0.382f88c4-40bf-4843-973f-2af17d2f6e4e.tmp to file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433062,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/offsets/.0.df27745b-ac11-4df1-b69d-f290843901eb.tmp to file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433460,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:59 INFO IncrementalExecution: Current batch timestamp = 1746196432966
25/05/02 14:33:59 INFO IncrementalExecution: Current batch timestamp = 1746196433062
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO IncrementalExecution: Current batch timestamp = 1746196432966
25/05/02 14:34:03 INFO IncrementalExecution: Current batch timestamp = 1746196433062
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:05 INFO IncrementalExecution: Current batch timestamp = 1746196432966
25/05/02 14:34:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]]. The input RDD has 1 partitions.
25/05/02 14:34:06 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:07 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:07 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/05/02 14:34:07 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:07 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:08 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1746196447257,WrappedArray(org.apache.spark.scheduler.StageInfo@7464b43c),{spark.driver.port=38559, spark.submit.pyFiles=, spark.app.startTime=1746196371469, spark.executor.extraClassPath=/spark/jars/*, spark.rdd.compress=True, spark.driver.extraClassPath=/spark/jars/*, callSite.short=start at <unknown>:0, __is_continuous_processing=false, spark.jobGroup.id=6ecc0647-2b5a-4ca8-be6d-dd4eaa05757d, spark.sql.requireAllClusterKeysForDistribution=false, spark.app.submitTime=1746196367262, spark.sql.adaptive.enabled=false, spark.app.initial.jar.urls=spark://3109b9eaa053:38559/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,spark://3109b9eaa053:38559/jars/postgresql-42.2.23.jar,spark://3109b9eaa053:38559/jars/kafka-clients-3.3.1.jar,spark://3109b9eaa053:38559/jars/commons-pool2-2.11.1.jar,spark://3109b9eaa053:38559/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar, spark.sql.execution.id=4, sql.streaming.queryId=0d335e24-6b74-4d15-a9f5-b3b1e655e49f, spark.sql.warehouse.dir=file:/app/spark-warehouse, streaming.sql.batchId=0, callSite.long=org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:251)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
java.base/java.lang.reflect.Method.invoke(Unknown Source)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
py4j.Gateway.invoke(Gateway.java:282)
py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
py4j.commands.CallCommand.execute(CallCommand.java:79)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.base/java.lang.Thread.run(Unknown Source), spark.executor.memory=2g, spark.driver.memory=2g, spark.master=local[*], spark.job.interruptOnCancel=true, spark.repl.local.jars=file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/kafka-clients-3.3.1.jar,file:///spark/jars/commons-pool2-2.11.1.jar,file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/postgresql-42.2.23.jar, spark.executor.id=driver, spark.app.name=KafkaConsumer_TRANSACTION_STATUS_EVOLUTION, spark.submit.deployMode=client, spark.driver.host=3109b9eaa053, spark.sql.caseSensitive=true, spark.app.id=local-1746196382360, spark.job.description=
id = 0d335e24-6b74-4d15-a9f5-b3b1e655e49f
runId = 6ecc0647-2b5a-4ca8-be6d-dd4eaa05757d
batch = 0, spark.sql.cbo.enabled=false, spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false, spark.sql.execution.root.id=0, spark.jars=file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/kafka-clients-3.3.1.jar,file:///spark/jars/commons-pool2-2.11.1.jar,file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/postgresql-42.2.23.jar, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false}) by listener AppStatusListener took 1.304306825s.
25/05/02 14:34:12 INFO CodeGenerator: Code generated in 5297.174235 ms
25/05/02 14:34:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
25/05/02 14:34:13 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 19.3 KiB, free 1007.8 MiB)
25/05/02 14:34:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 1007.8 MiB)
25/05/02 14:34:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3109b9eaa053:36961 (size: 9.2 KiB, free: 1007.8 MiB)
25/05/02 14:34:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/05/02 14:34:17 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:17 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
25/05/02 14:34:17 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:17 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:17 INFO CodeGenerator: Code generated in 791.78325 ms
25/05/02 14:34:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.6 KiB, free 1007.8 MiB)
25/05/02 14:34:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 1007.8 MiB)
25/05/02 14:34:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3109b9eaa053:36961 (size: 4.5 KiB, free: 1007.8 MiB)
25/05/02 14:34:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/05/02 14:34:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9076 bytes) 
25/05/02 14:34:18 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:19 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:19 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
25/05/02 14:34:19 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/05/02 14:34:19 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 33.0 KiB, free 1007.8 MiB)
25/05/02 14:34:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.1 KiB, free 1007.8 MiB)
25/05/02 14:34:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3109b9eaa053:36961 (size: 15.1 KiB, free: 1007.8 MiB)
25/05/02 14:34:21 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
25/05/02 14:34:22 INFO CodeGenerator: Code generated in 301.677269 ms
25/05/02 14:34:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TRANSACTION_STATUS_EVOLUTION-0 fromOffset=0 untilOffset=46, for query queryId=0d335e24-6b74-4d15-a9f5-b3b1e655e49f batchId=0 taskId=0 partitionId=0
25/05/02 14:34:23 INFO CodeGenerator: Code generated in 197.837908 ms
25/05/02 14:34:24 INFO CodeGenerator: Code generated in 303.487301 ms
25/05/02 14:34:24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:24 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:24 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:24 INFO AppInfoParser: Kafka startTimeMs: 1746196464857
25/05/02 14:34:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor-1, groupId=spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor] Assigned to partition(s): TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor-1, groupId=spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor] Seeking to offset 0 for partition TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:25 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor-1, groupId=spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor] Resetting the last seen epoch of partition TRANSACTION_STATUS_EVOLUTION-0 to 0 since the associated topicId changed from null to ILkLyYuGTEib8Iz2WcOApg
25/05/02 14:34:25 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor-1, groupId=spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor-1, groupId=spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor] Seeking to earliest offset of partition TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor-1, groupId=spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor] Resetting offset for partition TRANSACTION_STATUS_EVOLUTION-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor-1, groupId=spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor] Seeking to latest offset of partition TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor-1, groupId=spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor] Resetting offset for partition TRANSACTION_STATUS_EVOLUTION-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:28 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/05/02 14:34:28 INFO KafkaDataConsumer: From Kafka topicPartition=TRANSACTION_STATUS_EVOLUTION-0 groupId=spark-kafka-source-8102fd93-3a13-4e22-9869-d65716e6ccce-372669564-executor read 46 records through 1 polls (polled  out 46 records), taking 1107766632 nanos, during time span of 3104293925 nanos.
25/05/02 14:34:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5398 bytes result sent to driver
25/05/02 14:34:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9077 bytes) 
25/05/02 14:34:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
25/05/02 14:34:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 10897 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/05/02 14:34:29 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 21.198 s
25/05/02 14:34:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/05/02 14:34:29 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 22.600856 s
25/05/02 14:34:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] is committing.
-------------------------------------------
Batch: 025/05/02 14:34:29 INFO CodeGenerator: Code generated in 202.915993 ms

-------------------------------------------
25/05/02 14:34:29 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TRANSACTION_STATUS_EVOLUTION-0 fromOffset=0 untilOffset=46, for query queryId=0812f9ed-ec51-4061-9816-097fad065582 batchId=0 taskId=1 partitionId=0
25/05/02 14:34:29 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:30 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:30 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:30 INFO AppInfoParser: Kafka startTimeMs: 1746196470158
25/05/02 14:34:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor-2, groupId=spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor] Assigned to partition(s): TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor-2, groupId=spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor] Seeking to offset 0 for partition TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor-2, groupId=spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor] Resetting the last seen epoch of partition TRANSACTION_STATUS_EVOLUTION-0 to 0 since the associated topicId changed from null to ILkLyYuGTEib8Iz2WcOApg
25/05/02 14:34:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor-2, groupId=spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor-2, groupId=spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor] Seeking to earliest offset of partition TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:30 INFO CodeGenerator: Code generated in 97.574406 ms
25/05/02 14:34:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor-2, groupId=spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor] Resetting offset for partition TRANSACTION_STATUS_EVOLUTION-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor-2, groupId=spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor] Seeking to latest offset of partition TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor-2, groupId=spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor] Resetting offset for partition TRANSACTION_STATUS_EVOLUTION-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:31 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
25/05/02 14:34:31 INFO KafkaDataConsumer: From Kafka topicPartition=TRANSACTION_STATUS_EVOLUTION-0 groupId=spark-kafka-source-9c08daae-bd23-4bfa-adbf-ca7d0bfe8150-1042198202-executor read 46 records through 1 polls (polled  out 46 records), taking 1101507767 nanos, during time span of 1300981719 nanos.
25/05/02 14:34:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 5754 bytes result sent to driver
25/05/02 14:34:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9077 bytes) 
25/05/02 14:34:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3599 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
25/05/02 14:34:32 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 14.500 s
25/05/02 14:34:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/05/02 14:34:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/05/02 14:34:33 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 19.494162 s
25/05/02 14:34:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/02 14:34:34 INFO CodeGenerator: Code generated in 196.043726 ms
25/05/02 14:34:34 INFO CodeGenerator: Code generated in 98.423547 ms
25/05/02 14:34:34 INFO CodeGenerator: Code generated in 194.668933 ms
25/05/02 14:34:45 INFO CodeGenerator: Code generated in 6.322257 ms
25/05/02 14:34:45 INFO CodeGenerator: Code generated in 202.762409 ms
+------------+--------------------+
|         key|               value|
+------------+--------------------+
|TXN-9afaba9e|{"LATEST_STATUS":...|
|TXN-98af35dd|{"LATEST_STATUS":...|
|TXN-df38c1a2|{"LATEST_STATUS":...|
|TXN-42c92b77|{"LATEST_STATUS":...|
|TXN-9dec8c9d|{"LATEST_STATUS":...|
|TXN-4dc92f3f|{"LATEST_STATUS":...|
|TXN-724f1cd9|{"LATEST_STATUS":...|
|TXN-8691185f|{"LATEST_STATUS":...|
|TXN-b0f482f1|{"LATEST_STATUS":...|
|TXN-f20e400c|{"LATEST_STATUS":...|
|TXN-fed1b1b0|{"LATEST_STATUS":...|
|TXN-58f98f9e|{"LATEST_STATUS":...|
|TXN-91a33ffb|{"LATEST_STATUS":...|
|TXN-e0a89ea4|{"LATEST_STATUS":...|
|TXN-7ca78e2d|{"LATEST_STATUS":...|
|TXN-736a5a90|{"LATEST_STATUS":...|
|TXN-712caa15|{"LATEST_STATUS":...|
|TXN-59f9cee3|{"LATEST_STATUS":...|
|TXN-58213410|{"LATEST_STATUS":...|
|TXN-be652fea|{"LATEST_STATUS":...|
+------------+--------------------+
only showing top 20 rows

+--------------+-------------+-----------------------+--------------+
|TRANSACTION_ID|LATEST_STATUS|ingestion_time         |ingestion_date|
+--------------+-------------+-----------------------+--------------+
|TXN-9afaba9e  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
|TXN-98af35dd  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
|TXN-df38c1a2  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
|TXN-42c92b77  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
|TXN-9dec8c9d  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
|TXN-4dc92f3f  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
|TXN-724f1cd9  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
|TXN-8691185f  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
|TXN-b0f482f1  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
|TXN-f20e400c  |NULL         |2025-05-02 14:33:52.966|2025-05-02    |
+--------------+-------------+-----------------------+--------------+
only showing top 10 rows

25/05/02 14:34:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
25/05/02 14:34:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] committed.
25/05/02 14:34:46 INFO CodeGenerator: Code generated in 399.038733 ms
25/05/02 14:34:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TRANSACTION_STATUS_EVOLUTION-0 fromOffset=0 untilOffset=46, for query queryId=b72490a0-d243-44e1-b0ee-afa0e4743cd1 batchId=0 taskId=2 partitionId=0
25/05/02 14:34:46 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:46 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:46 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:46 INFO AppInfoParser: Kafka startTimeMs: 1746196486561
25/05/02 14:34:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor-3, groupId=spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor] Assigned to partition(s): TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor-3, groupId=spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor] Seeking to offset 0 for partition TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:46 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor-3, groupId=spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor] Resetting the last seen epoch of partition TRANSACTION_STATUS_EVOLUTION-0 to 0 since the associated topicId changed from null to ILkLyYuGTEib8Iz2WcOApg
25/05/02 14:34:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/commits/0 using temp file file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/commits/.0.c1cbd2c3-178f-40b2-a665-ea4803f4aba7.tmp
25/05/02 14:34:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/commits/0 using temp file file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/commits/.0.dfd663ec-c773-4b9e-be79-3a11756d2e13.tmp
25/05/02 14:34:46 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor-3, groupId=spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor-3, groupId=spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor] Seeking to earliest offset of partition TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor-3, groupId=spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor] Resetting offset for partition TRANSACTION_STATUS_EVOLUTION-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor-3, groupId=spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor] Seeking to latest offset of partition TRANSACTION_STATUS_EVOLUTION-0
25/05/02 14:34:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor-3, groupId=spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor] Resetting offset for partition TRANSACTION_STATUS_EVOLUTION-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/commits/.0.c1cbd2c3-178f-40b2-a665-ea4803f4aba7.tmp to file:/tmp/temporary-1d7d160d-db72-46a1-8ede-2c1f541a2b13/commits/0
25/05/02 14:34:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/commits/.0.dfd663ec-c773-4b9e-be79-3a11756d2e13.tmp to file:/tmp/temporary-493fbf7b-e0a2-48dd-a0c4-c5f3c53c9ab7/commits/0
25/05/02 14:34:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "0812f9ed-ec51-4061-9816-097fad065582",
  "runId" : "8e1f7d4b-6fd5-4586-b96d-c15d237e5913",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:37.568Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.6516688388961297,
  "durationMs" : {
    "addBatch" : 43897,
    "commitOffsets" : 1803,
    "getBatch" : 3,
    "latestOffset" : 15600,
    "queryPlanning" : 7604,
    "triggerExecution" : 70497,
    "walCommit" : 1098
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TRANSACTION_STATUS_EVOLUTION]]",
    "startOffset" : null,
    "endOffset" : {
      "TRANSACTION_STATUS_EVOLUTION" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TRANSACTION_STATUS_EVOLUTION" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.6516688388961297,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@55ff5621",
    "numOutputRows" : 46
  }
}
25/05/02 14:34:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "0d335e24-6b74-4d15-a9f5-b3b1e655e49f",
  "runId" : "6ecc0647-2b5a-4ca8-be6d-dd4eaa05757d",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:46.760Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.7491734662301917,
  "durationMs" : {
    "addBatch" : 43896,
    "commitOffsets" : 1806,
    "getBatch" : 296,
    "latestOffset" : 5807,
    "queryPlanning" : 7600,
    "triggerExecution" : 61401,
    "walCommit" : 1203
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TRANSACTION_STATUS_EVOLUTION]]",
    "startOffset" : null,
    "endOffset" : {
      "TRANSACTION_STATUS_EVOLUTION" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TRANSACTION_STATUS_EVOLUTION" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.7491734662301917,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@55ff5621",
    "numOutputRows" : 46
  }
}
25/05/02 14:34:49 INFO KafkaDataConsumer: From Kafka topicPartition=TRANSACTION_STATUS_EVOLUTION-0 groupId=spark-kafka-source-88316897-aa52-4d68-bb56-f9e96c064ee2--365797564-executor read 46 records through 1 polls (polled  out 46 records), taking 1296323522 nanos, during time span of 2997294932 nanos.
25/05/02 14:34:49 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1718 bytes result sent to driver
25/05/02 14:34:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 18202 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:50 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/05/02 14:34:50 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 30.596 s
25/05/02 14:34:50 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
25/05/02 14:34:50 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 31.202668 s
25/05/02 14:34:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/commits/0 using temp file file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/commits/.0.f6694770-6838-43e9-bf54-0dae845384f8.tmp
25/05/02 14:34:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/commits/.0.f6694770-6838-43e9-bf54-0dae845384f8.tmp to file:/tmp/temporary-834edf4d-3a7a-4920-9ddf-57963dee833a/commits/0
25/05/02 14:34:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "b72490a0-d243-44e1-b0ee-afa0e4743cd1",
  "runId" : "c69967ef-87b3-4f48-9642-eb82e1b8542a",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:49.664Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.7290593549409621,
  "durationMs" : {
    "addBatch" : 48999,
    "commitOffsets" : 1402,
    "getBatch" : 104,
    "latestOffset" : 3303,
    "queryPlanning" : 7502,
    "triggerExecution" : 63095,
    "walCommit" : 1395
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TRANSACTION_STATUS_EVOLUTION]]",
    "startOffset" : null,
    "endOffset" : {
      "TRANSACTION_STATUS_EVOLUTION" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TRANSACTION_STATUS_EVOLUTION" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.7290593549409621,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
25/05/02 14:34:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:34:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:51 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
25/05/02 14:38:51 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
25/05/02 14:38:51 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
25/05/02 14:38:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:46:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:46:53,725 - TRANSACTION_STATUS_EVOLUTION - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:46:54 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:46:54 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:46:54 INFO SparkContext: Java version 11.0.24
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO SparkContext: Submitted application: KafkaConsumer_TRANSACTION_STATUS_EVOLUTION
25/05/02 14:46:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:46:55 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:46:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:46:56 INFO SecurityManager: Changing view acls to: root
25/05/02 14:46:56 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:46:56 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:46:56 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:46:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:47:01 INFO Utils: Successfully started service 'sparkDriver' on port 36025.
25/05/02 14:47:01 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:47:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fe760c56-e93d-4159-b83e-5a71f9083a85
25/05/02 14:47:03 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:47:04 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:47:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/05/02 14:47:07 INFO Utils: Successfully started service 'SparkUI' on port 4045.
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:36025/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214528
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://5e919d8ee3aa:36025/jars/kafka-clients-3.3.1.jar with timestamp 1746197214528
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://5e919d8ee3aa:36025/jars/commons-pool2-2.11.1.jar with timestamp 1746197214528
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:36025/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214528
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://5e919d8ee3aa:36025/jars/postgresql-42.2.23.jar with timestamp 1746197214528
25/05/02 14:47:09 INFO Executor: Starting executor ID driver on host 5e919d8ee3aa
25/05/02 14:47:09 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:47:09 INFO Executor: Java version 11.0.24
25/05/02 14:47:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:47:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3100a98e for default.
25/05/02 14:47:10 INFO Executor: Fetching spark://5e919d8ee3aa:36025/jars/postgresql-42.2.23.jar with timestamp 1746197214528
25/05/02 14:47:10 INFO TransportClientFactory: Successfully created connection to 5e919d8ee3aa/172.21.0.2:36025 after 495 ms (0 ms spent in bootstraps)
25/05/02 14:47:11 INFO Utils: Fetching spark://5e919d8ee3aa:36025/jars/postgresql-42.2.23.jar to /tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/fetchFileTemp5422140776353149885.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/postgresql-42.2.23.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:36025/jars/kafka-clients-3.3.1.jar with timestamp 1746197214528
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:36025/jars/kafka-clients-3.3.1.jar to /tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/fetchFileTemp12236648529585916497.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:36025/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214528
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:36025/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/fetchFileTemp15218253959134673329.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:13 INFO Executor: Fetching spark://5e919d8ee3aa:36025/jars/commons-pool2-2.11.1.jar with timestamp 1746197214528
25/05/02 14:47:13 INFO Utils: Fetching spark://5e919d8ee3aa:36025/jars/commons-pool2-2.11.1.jar to /tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/fetchFileTemp6527738906894253964.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:47:13 INFO Executor: Fetching spark://5e919d8ee3aa:36025/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214528
25/05/02 14:47:13 INFO Utils: Fetching spark://5e919d8ee3aa:36025/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/fetchFileTemp16496712652392025962.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/userFiles-d293ca6a-da38-48ee-8b90-7929d81ec529/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43091.
25/05/02 14:47:13 INFO NettyBlockTransferService: Server created on 5e919d8ee3aa:43091
25/05/02 14:47:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:47:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5e919d8ee3aa, 43091, None)
25/05/02 14:47:13 INFO BlockManagerMasterEndpoint: Registering block manager 5e919d8ee3aa:43091 with 1007.8 MiB RAM, BlockManagerId(driver, 5e919d8ee3aa, 43091, None)
25/05/02 14:47:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5e919d8ee3aa, 43091, None)
25/05/02 14:47:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5e919d8ee3aa, 43091, None)
2025-05-02 14:47:18,129 - TRANSACTION_STATUS_EVOLUTION - INFO - Session Spark créée.
2025-05-02 14:47:18,129 - TRANSACTION_STATUS_EVOLUTION - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic TRANSACTION_STATUS_EVOLUTION.
2025-05-02 14:47:18,130 - TRANSACTION_STATUS_EVOLUTION - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:47:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:47:18 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:47:39,131 - TRANSACTION_STATUS_EVOLUTION - INFO - Données chargées avec succès.
2025-05-02 14:47:39,132 - TRANSACTION_STATUS_EVOLUTION - INFO - Vérification du format initial.
25/05/02 14:47:41 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:47:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0d99f5cf-bb2d-4a46-a23b-3274087745f1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:42 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-0d99f5cf-bb2d-4a46-a23b-3274087745f1 resolved to file:/tmp/temporary-0d99f5cf-bb2d-4a46-a23b-3274087745f1.
25/05/02 14:47:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0d99f5cf-bb2d-4a46-a23b-3274087745f1/metadata using temp file file:/tmp/temporary-0d99f5cf-bb2d-4a46-a23b-3274087745f1/.metadata.e079c0d5-e399-47ac-b510-7b580fbb2fc3.tmp
25/05/02 14:47:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0d99f5cf-bb2d-4a46-a23b-3274087745f1/.metadata.e079c0d5-e399-47ac-b510-7b580fbb2fc3.tmp to file:/tmp/temporary-0d99f5cf-bb2d-4a46-a23b-3274087745f1/metadata
25/05/02 14:47:46 INFO MicroBatchExecution: Starting [id = 01f0fcf4-4d6a-44f2-b5c4-73b0ef6cd0d4, runId = dd05d332-2dcb-4033-9b46-5c9f13869ee9]. Use file:/tmp/temporary-0d99f5cf-bb2d-4a46-a23b-3274087745f1 to store the query checkpoint.
2025-05-02 14:47:46,927 - TRANSACTION_STATUS_EVOLUTION - INFO - Données en cours de transformation...
25/05/02 14:47:47 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5efb064a] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@63ef335b]
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:47 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:47:53,629 - TRANSACTION_STATUS_EVOLUTION - INFO - Données parsée avec succès !!!
root
 |-- TRANSACTION_ID: string (nullable = true)
 |-- LATEST_STATUS: string (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:47:53,724 - TRANSACTION_STATUS_EVOLUTION - INFO - Vérification des données chargées :
25/05/02 14:47:54 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1c3acf92-f6a1-423a-b854-f8650295d271. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:54 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-1c3acf92-f6a1-423a-b854-f8650295d271 resolved to file:/tmp/temporary-1c3acf92-f6a1-423a-b854-f8650295d271.
25/05/02 14:47:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1c3acf92-f6a1-423a-b854-f8650295d271/metadata using temp file file:/tmp/temporary-1c3acf92-f6a1-423a-b854-f8650295d271/.metadata.17f016d6-20e8-4f80-9fc2-143eed29aa75.tmp
25/05/02 14:47:55 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1c3acf92-f6a1-423a-b854-f8650295d271/.metadata.17f016d6-20e8-4f80-9fc2-143eed29aa75.tmp to file:/tmp/temporary-1c3acf92-f6a1-423a-b854-f8650295d271/metadata
25/05/02 14:47:56 INFO MicroBatchExecution: Starting [id = 42e640b5-73af-4413-983a-002ef4326390, runId = 09428a4b-2224-4cc8-b706-8c25bd439a18]. Use file:/tmp/temporary-1c3acf92-f6a1-423a-b854-f8650295d271 to store the query checkpoint.
2025-05-02 14:47:56,526 - TRANSACTION_STATUS_EVOLUTION - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:47:56,526 - TRANSACTION_STATUS_EVOLUTION - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:47:56 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5efb064a] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@63ef335b]
25/05/02 14:47:56 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:56 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:56 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:56 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:56 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:57 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:57 INFO AppInfoParser: Kafka startTimeMs: 1746197276925
25/05/02 14:47:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c021d676-e43e-49c0-b7f9-696dd76bc514. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-c021d676-e43e-49c0-b7f9-696dd76bc514 resolved to file:/tmp/temporary-c021d676-e43e-49c0-b7f9-696dd76bc514.
25/05/02 14:47:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:57 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:57 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:57 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:57 INFO AppInfoParser: Kafka startTimeMs: 1746197277424
25/05/02 14:47:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c021d676-e43e-49c0-b7f9-696dd76bc514/metadata using temp file file:/tmp/temporary-c021d676-e43e-49c0-b7f9-696dd76bc514/.metadata.1f6216c9-fa01-44ed-98ce-83f5f807ae8d.tmp
25/05/02 14:47:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c021d676-e43e-49c0-b7f9-696dd76bc514/.metadata.1f6216c9-fa01-44ed-98ce-83f5f807ae8d.tmp to file:/tmp/temporary-c021d676-e43e-49c0-b7f9-696dd76bc514/metadata
25/05/02 14:47:59 INFO MicroBatchExecution: Starting [id = 208d6e3a-e921-4bae-9951-dfcac5f60276, runId = e4092886-f6d1-4df5-bd73-1b14bd1bb4d6]. Use file:/tmp/temporary-c021d676-e43e-49c0-b7f9-696dd76bc514 to store the query checkpoint.
25/05/02 14:47:59 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5efb064a] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@63ef335b]
25/05/02 14:47:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:59 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:59 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:59 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:59 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:59 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:59 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:59 INFO AppInfoParser: Kafka startTimeMs: 1746197279731
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282428
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282525
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282526
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-4 unregistered
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-5 unregistered
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-6 unregistered
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283631
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283635
25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283639
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-7 unregistered
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-8 unregistered
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-9 unregistered
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 ERROR MicroBatchExecution: Query [id = 208d6e3a-e921-4bae-9951-dfcac5f60276, runId = e4092886-f6d1-4df5-bd73-1b14bd1bb4d6] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 ERROR MicroBatchExecution: Query [id = 01f0fcf4-4d6a-44f2-b5c4-73b0ef6cd0d4, runId = dd05d332-2dcb-4033-9b46-5c9f13869ee9] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = 208d6e3a-e921-4bae-9951-dfcac5f60276, runId = e4092886-f6d1-4df5-bd73-1b14bd1bb4d6] has been shutdown
25/05/02 14:48:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = 01f0fcf4-4d6a-44f2-b5c4-73b0ef6cd0d4, runId = dd05d332-2dcb-4033-9b46-5c9f13869ee9] has been shutdown
25/05/02 14:48:04 ERROR MicroBatchExecution: Query [id = 42e640b5-73af-4413-983a-002ef4326390, runId = 09428a4b-2224-4cc8-b706-8c25bd439a18] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = 42e640b5-73af-4413-983a-002ef4326390, runId = 09428a4b-2224-4cc8-b706-8c25bd439a18] has been shutdown
Traceback (most recent call last):
  File "/app/consumer-table/writter_TRANSACTION_STATUS_EVOLUTION.py", line 104, in <module>
    df_parsed
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 208d6e3a-e921-4bae-9951-dfcac5f60276, runId = e4092886-f6d1-4df5-bd73-1b14bd1bb4d6] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:06 INFO SparkContext: Invoking stop() from shutdown hook
25/05/02 14:48:06 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/05/02 14:48:06 INFO SparkUI: Stopped Spark web UI at http://5e919d8ee3aa:4045
25/05/02 14:48:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/05/02 14:48:06 INFO MemoryStore: MemoryStore cleared
25/05/02 14:48:06 INFO BlockManager: BlockManager stopped
25/05/02 14:48:06 INFO BlockManagerMaster: BlockManagerMaster stopped
25/05/02 14:48:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/05/02 14:48:07 INFO SparkContext: Successfully stopped SparkContext
25/05/02 14:48:07 INFO ShutdownHookManager: Shutdown hook called
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-50157d88-05c5-44de-ae71-d245f871773f/pyspark-b077373d-4b49-4853-8404-2bfc9c449805
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-1c3acf92-f6a1-423a-b854-f8650295d271
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-b30f6d78-3fc0-4a3a-9f4e-7e23941fc292
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-0d99f5cf-bb2d-4a46-a23b-3274087745f1
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-50157d88-05c5-44de-ae71-d245f871773f
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-c021d676-e43e-49c0-b7f9-696dd76bc514
