25/05/02 14:32:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:32:50,774 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:32:51 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:32:51 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:32:51 INFO SparkContext: Java version 11.0.24
25/05/02 14:32:52 INFO ResourceUtils: ==============================================================
25/05/02 14:32:52 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:32:52 INFO ResourceUtils: ==============================================================
25/05/02 14:32:52 INFO SparkContext: Submitted application: KafkaConsumer_TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD
25/05/02 14:32:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:32:52 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:32:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:32:53 INFO SecurityManager: Changing view acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:32:56 INFO Utils: Successfully started service 'sparkDriver' on port 38231.
25/05/02 14:32:56 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:32:57 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:32:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:32:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:32:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:32:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-08b78014-fac9-4ed9-8639-29a93fdb79b0
25/05/02 14:32:58 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:32:58 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:33:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:33:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:38231/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371462
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://3109b9eaa053:38231/jars/kafka-clients-3.3.1.jar with timestamp 1746196371462
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://3109b9eaa053:38231/jars/commons-pool2-2.11.1.jar with timestamp 1746196371462
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:38231/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371462
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://3109b9eaa053:38231/jars/postgresql-42.2.23.jar with timestamp 1746196371462
25/05/02 14:33:03 INFO Executor: Starting executor ID driver on host 3109b9eaa053
25/05/02 14:33:03 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:33:03 INFO Executor: Java version 11.0.24
25/05/02 14:33:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:33:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@34338ec4 for default.
25/05/02 14:33:03 INFO Executor: Fetching spark://3109b9eaa053:38231/jars/postgresql-42.2.23.jar with timestamp 1746196371462
25/05/02 14:33:04 INFO TransportClientFactory: Successfully created connection to 3109b9eaa053/172.21.0.7:38231 after 695 ms (0 ms spent in bootstraps)
25/05/02 14:33:04 INFO Utils: Fetching spark://3109b9eaa053:38231/jars/postgresql-42.2.23.jar to /tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/fetchFileTemp12200407722810206337.tmp
25/05/02 14:33:05 INFO Executor: Adding file:/tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/postgresql-42.2.23.jar to class loader default
25/05/02 14:33:05 INFO Executor: Fetching spark://3109b9eaa053:38231/jars/kafka-clients-3.3.1.jar with timestamp 1746196371462
25/05/02 14:33:05 INFO Utils: Fetching spark://3109b9eaa053:38231/jars/kafka-clients-3.3.1.jar to /tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/fetchFileTemp4948109497834072352.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:38231/jars/commons-pool2-2.11.1.jar with timestamp 1746196371462
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:38231/jars/commons-pool2-2.11.1.jar to /tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/fetchFileTemp9934738154755497675.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:38231/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371462
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:38231/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/fetchFileTemp3887346275048672251.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:38231/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371462
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:38231/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/fetchFileTemp3958747091874964955.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-a55597df-a8bc-492a-b1e3-bff3b9903eb7/userFiles-c670fcc4-1da1-46fe-ab0c-52ed52647874/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44117.
25/05/02 14:33:06 INFO NettyBlockTransferService: Server created on 3109b9eaa053:44117
25/05/02 14:33:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:33:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3109b9eaa053, 44117, None)
25/05/02 14:33:06 INFO BlockManagerMasterEndpoint: Registering block manager 3109b9eaa053:44117 with 1007.8 MiB RAM, BlockManagerId(driver, 3109b9eaa053, 44117, None)
25/05/02 14:33:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3109b9eaa053, 44117, None)
25/05/02 14:33:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3109b9eaa053, 44117, None)
2025-05-02 14:33:11,060 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Session Spark créée.
2025-05-02 14:33:11,060 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD.
2025-05-02 14:33:11,060 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:33:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:33:11 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:33:30,659 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Données chargées avec succès.
2025-05-02 14:33:30,659 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Vérification du format initial.
25/05/02 14:33:32 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:33:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:33 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393 resolved to file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393.
25/05/02 14:33:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/metadata using temp file file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/.metadata.5a55e153-587a-43a4-a81c-b6313ad7b125.tmp
25/05/02 14:33:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/.metadata.5a55e153-587a-43a4-a81c-b6313ad7b125.tmp to file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/metadata
25/05/02 14:33:36 INFO MicroBatchExecution: Starting [id = dbaa7376-d22c-47c4-8a7c-958fcc90b4e4, runId = 0a6f1c6b-7ee4-4634-ba08-065bc3604ebb]. Use file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393 to store the query checkpoint.
2025-05-02 14:33:37,061 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Données en cours de transformation...
25/05/02 14:33:37 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1e819ba4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3a2c6c0d]
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:37 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:37 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:33:43,757 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Données parsée avec succès !!!
root
 |-- PAYMENT_METHOD: string (nullable = true)
 |-- TOTAL_AMOUNT: double (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:33:43,857 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Vérification des données chargées :
25/05/02 14:33:44 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:44 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573 resolved to file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573.
25/05/02 14:33:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/metadata using temp file file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/.metadata.dfce70b1-7cdc-4a50-9add-3767e3abcf67.tmp
25/05/02 14:33:45 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/.metadata.dfce70b1-7cdc-4a50-9add-3767e3abcf67.tmp to file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/metadata
25/05/02 14:33:46 INFO MicroBatchExecution: Starting [id = 35e82855-ad10-440a-accf-e6afe3ab7d8c, runId = ceb3a5e2-fb0c-4dbf-8343-b4a1abfc1ae9]. Use file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573 to store the query checkpoint.
25/05/02 14:33:46 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:46 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:46 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:46 INFO AppInfoParser: Kafka startTimeMs: 1746196426757
2025-05-02 14:33:46,759 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:33:46,759 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:33:46 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1e819ba4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3a2c6c0d]
25/05/02 14:33:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:47 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:47 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:47 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53 resolved to file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53.
25/05/02 14:33:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:47 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:48 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:48 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:48 INFO AppInfoParser: Kafka startTimeMs: 1746196428058
25/05/02 14:33:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/metadata using temp file file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/.metadata.094829c3-76da-4922-9f84-e85fbf6d8caa.tmp
25/05/02 14:33:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/.metadata.094829c3-76da-4922-9f84-e85fbf6d8caa.tmp to file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/metadata
25/05/02 14:33:49 INFO MicroBatchExecution: Starting [id = fff8ca60-eabe-44c3-9031-a5ea4bad0a62, runId = 18382e4a-9bfc-4811-81f5-57904b198130]. Use file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53 to store the query checkpoint.
25/05/02 14:33:49 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1e819ba4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3a2c6c0d]
25/05/02 14:33:49 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:49 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:49 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:49 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:50 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:50 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:50 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:50 INFO AppInfoParser: Kafka startTimeMs: 1746196430257
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/sources/0/0 using temp file file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/sources/0/.0.f2774622-cf22-4961-a499-467019618943.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/sources/0/0 using temp file file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/sources/0/.0.2d210206-b278-4d15-bea9-d4841c15e638.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/sources/0/0 using temp file file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/sources/0/.0.2079ecf2-90b3-497b-ac56-bda0852c9720.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/sources/0/.0.2d210206-b278-4d15-bea9-d4841c15e638.tmp to file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/sources/0/0
25/05/02 14:33:53 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/sources/0/.0.f2774622-cf22-4961-a499-467019618943.tmp to file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/sources/0/0
25/05/02 14:33:53 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/sources/0/.0.2079ecf2-90b3-497b-ac56-bda0852c9720.tmp to file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/sources/0/0
25/05/02 14:33:53 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/offsets/0 using temp file file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/offsets/.0.3f8196fa-b5c8-46c8-b141-924bc2db317b.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/offsets/0 using temp file file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/offsets/.0.66bc2a9c-a99c-40d4-b822-9d5b0ce1f705.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/offsets/0 using temp file file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/offsets/.0.92b065f4-5599-47f8-b280-058daed8b43a.tmp
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/offsets/.0.3f8196fa-b5c8-46c8-b141-924bc2db317b.tmp to file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/offsets/0
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/offsets/.0.66bc2a9c-a99c-40d4-b822-9d5b0ce1f705.tmp to file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/offsets/0
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/offsets/.0.92b065f4-5599-47f8-b280-058daed8b43a.tmp to file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433459,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433459,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433560,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:34:00 INFO IncrementalExecution: Current batch timestamp = 1746196433459
25/05/02 14:34:00 INFO IncrementalExecution: Current batch timestamp = 1746196433560
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO IncrementalExecution: Current batch timestamp = 1746196433560
25/05/02 14:34:03 INFO IncrementalExecution: Current batch timestamp = 1746196433459
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO IncrementalExecution: Current batch timestamp = 1746196433459
25/05/02 14:34:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]]. The input RDD has 1 partitions.
25/05/02 14:34:06 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:07 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:07 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/05/02 14:34:07 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:07 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:09 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1746196447657,WrappedArray(org.apache.spark.scheduler.StageInfo@6835b42e),{spark.driver.port=38231, spark.submit.pyFiles=, spark.app.startTime=1746196371462, spark.executor.extraClassPath=/spark/jars/*, spark.rdd.compress=True, spark.driver.extraClassPath=/spark/jars/*, callSite.short=start at <unknown>:0, __is_continuous_processing=false, spark.jobGroup.id=ceb3a5e2-fb0c-4dbf-8343-b4a1abfc1ae9, spark.sql.requireAllClusterKeysForDistribution=false, spark.app.submitTime=1746196367364, spark.sql.adaptive.enabled=false, spark.app.initial.jar.urls=spark://3109b9eaa053:38231/jars/postgresql-42.2.23.jar,spark://3109b9eaa053:38231/jars/kafka-clients-3.3.1.jar,spark://3109b9eaa053:38231/jars/commons-pool2-2.11.1.jar,spark://3109b9eaa053:38231/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,spark://3109b9eaa053:38231/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar, spark.sql.execution.id=4, sql.streaming.queryId=35e82855-ad10-440a-accf-e6afe3ab7d8c, spark.sql.warehouse.dir=file:/app/spark-warehouse, streaming.sql.batchId=0, callSite.long=org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:251)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
java.base/java.lang.reflect.Method.invoke(Unknown Source)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
py4j.Gateway.invoke(Gateway.java:282)
py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
py4j.commands.CallCommand.execute(CallCommand.java:79)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.base/java.lang.Thread.run(Unknown Source), spark.executor.memory=2g, spark.driver.memory=2g, spark.master=local[*], spark.job.interruptOnCancel=true, spark.repl.local.jars=file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/kafka-clients-3.3.1.jar,file:///spark/jars/commons-pool2-2.11.1.jar,file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/postgresql-42.2.23.jar, spark.executor.id=driver, spark.app.name=KafkaConsumer_TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD, spark.submit.deployMode=client, spark.driver.host=3109b9eaa053, spark.sql.caseSensitive=true, spark.app.id=local-1746196382269, spark.job.description=
id = 35e82855-ad10-440a-accf-e6afe3ab7d8c
runId = ceb3a5e2-fb0c-4dbf-8343-b4a1abfc1ae9
batch = 0, spark.sql.cbo.enabled=false, spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false, spark.sql.execution.root.id=1, spark.jars=file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/kafka-clients-3.3.1.jar,file:///spark/jars/commons-pool2-2.11.1.jar,file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/postgresql-42.2.23.jar, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false}) by listener AppStatusListener took 1.306474426s.
25/05/02 14:34:12 INFO CodeGenerator: Code generated in 4898.468176 ms
25/05/02 14:34:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
25/05/02 14:34:13 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 19.4 KiB, free 1007.8 MiB)
25/05/02 14:34:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 1007.8 MiB)
25/05/02 14:34:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3109b9eaa053:44117 (size: 9.2 KiB, free: 1007.8 MiB)
25/05/02 14:34:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/05/02 14:34:16 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:16 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
25/05/02 14:34:16 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:16 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:16 INFO CodeGenerator: Code generated in 298.562469 ms
25/05/02 14:34:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.6 KiB, free 1007.8 MiB)
25/05/02 14:34:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 1007.8 MiB)
25/05/02 14:34:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3109b9eaa053:44117 (size: 4.5 KiB, free: 1007.8 MiB)
25/05/02 14:34:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/05/02 14:34:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9093 bytes) 
25/05/02 14:34:17 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:17 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/05/02 14:34:17 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
25/05/02 14:34:17 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:17 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 33.2 KiB, free 1007.8 MiB)
25/05/02 14:34:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 1007.8 MiB)
25/05/02 14:34:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3109b9eaa053:44117 (size: 15.2 KiB, free: 1007.8 MiB)
25/05/02 14:34:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
25/05/02 14:34:21 INFO CodeGenerator: Code generated in 396.30056 ms
25/05/02 14:34:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 fromOffset=0 untilOffset=46, for query queryId=35e82855-ad10-440a-accf-e6afe3ab7d8c batchId=0 taskId=0 partitionId=0
25/05/02 14:34:22 INFO CodeGenerator: Code generated in 297.147687 ms
25/05/02 14:34:22 INFO CodeGenerator: Code generated in 296.401867 ms
25/05/02 14:34:23 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:23 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:23 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:24 INFO AppInfoParser: Kafka startTimeMs: 1746196463865
25/05/02 14:34:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor-1, groupId=spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor] Assigned to partition(s): TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor-1, groupId=spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor] Seeking to offset 0 for partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:24 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor-1, groupId=spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor] Resetting the last seen epoch of partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 to 0 since the associated topicId changed from null to YlftBGnKQT-DHSU883bPLg
25/05/02 14:34:24 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor-1, groupId=spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor-1, groupId=spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor] Seeking to earliest offset of partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor-1, groupId=spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor] Resetting offset for partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor-1, groupId=spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor] Seeking to latest offset of partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor-1, groupId=spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor] Resetting offset for partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:27 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:27 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/05/02 14:34:27 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 groupId=spark-kafka-source-6ad4ec53-638b-4e7f-ba3a-87ff393d8687--1838188379-executor read 46 records through 1 polls (polled  out 46 records), taking 1396083100 nanos, during time span of 3200268986 nanos.
25/05/02 14:34:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5366 bytes result sent to driver
25/05/02 14:34:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9093 bytes) 
25/05/02 14:34:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
25/05/02 14:34:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11401 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/05/02 14:34:28 INFO CodeGenerator: Code generated in 106.317176 ms
25/05/02 14:34:28 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 20.296 s
25/05/02 14:34:28 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 fromOffset=0 untilOffset=46, for query queryId=dbaa7376-d22c-47c4-8a7c-958fcc90b4e4 batchId=0 taskId=1 partitionId=0
25/05/02 14:34:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/05/02 14:34:28 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 21.699005 s
25/05/02 14:34:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] is committing.
-------------------------------------------25/05/02 14:34:28 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer


Batch: 0
-------------------------------------------
25/05/02 14:34:29 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:29 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:29 INFO AppInfoParser: Kafka startTimeMs: 1746196469160
25/05/02 14:34:29 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor-2, groupId=spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor] Assigned to partition(s): TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:29 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor-2, groupId=spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor] Seeking to offset 0 for partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:29 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor-2, groupId=spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor] Resetting the last seen epoch of partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 to 0 since the associated topicId changed from null to YlftBGnKQT-DHSU883bPLg
25/05/02 14:34:29 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor-2, groupId=spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor-2, groupId=spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor] Seeking to earliest offset of partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor-2, groupId=spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor] Resetting offset for partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor-2, groupId=spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor] Seeking to latest offset of partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor-2, groupId=spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor] Resetting offset for partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:30 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:30 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
25/05/02 14:34:30 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 groupId=spark-kafka-source-ce2bdca2-9323-4ef1-a637-31dce7fb0d77--2113258156-executor read 46 records through 1 polls (polled  out 46 records), taking 903316368 nanos, during time span of 1100284238 nanos.
25/05/02 14:34:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 5981 bytes result sent to driver
25/05/02 14:34:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9092 bytes) 
25/05/02 14:34:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
25/05/02 14:34:30 INFO CodeGenerator: Code generated in 193.055549 ms
25/05/02 14:34:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2999 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:31 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 14.896 s
25/05/02 14:34:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/05/02 14:34:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/05/02 14:34:31 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 18.100186 s
25/05/02 14:34:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/02 14:34:32 INFO CodeGenerator: Code generated in 202.027843 ms
25/05/02 14:34:32 INFO CodeGenerator: Code generated in 297.730834 ms
25/05/02 14:34:32 INFO CodeGenerator: Code generated in 109.822427 ms
25/05/02 14:34:46 INFO CodeGenerator: Code generated in 101.004415 ms
25/05/02 14:34:46 INFO CodeGenerator: Code generated in 98.826998 ms
+--------------+--------------------+
|           key|               value|
+--------------+--------------------+
|     apple_pay|{"TOTAL_AMOUNT":6...|
| bank_transfer|{"TOTAL_AMOUNT":7...|
| bank_transfer|{"TOTAL_AMOUNT":9...|
|     apple_pay|{"TOTAL_AMOUNT":7...|
| bank_transfer|{"TOTAL_AMOUNT":1...|
|    google_pay|{"TOTAL_AMOUNT":4...|
|   credit_card|{"TOTAL_AMOUNT":7...|
| bank_transfer|{"TOTAL_AMOUNT":1...|
|cryptocurrency|{"TOTAL_AMOUNT":3...|
|    google_pay|{"TOTAL_AMOUNT":2...|
|     apple_pay|{"TOTAL_AMOUNT":7...|
| bank_transfer|{"TOTAL_AMOUNT":2...|
| bank_transfer|{"TOTAL_AMOUNT":2...|
| bank_transfer|{"TOTAL_AMOUNT":3...|
|   credit_card|{"TOTAL_AMOUNT":1...|
|   credit_card|{"TOTAL_AMOUNT":2...|
|cryptocurrency|{"TOTAL_AMOUNT":1...|
| bank_transfer|{"TOTAL_AMOUNT":3...|
| bank_transfer|{"TOTAL_AMOUNT":4...|
| bank_transfer|{"TOTAL_AMOUNT":5...|
+--------------+--------------------+
only showing top 20 rows

25/05/02 14:34:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
+--------------+------------------+-----------------------+--------------+
|PAYMENT_METHOD|TOTAL_AMOUNT      |ingestion_time         |ingestion_date|
+--------------+------------------+-----------------------+--------------+
|apple_pay     |653.3             |2025-05-02 14:33:53.459|2025-05-02    |
|bank_transfer |798.79            |2025-05-02 14:33:53.459|2025-05-02    |
|bank_transfer |956.9399999999999 |2025-05-02 14:33:53.459|2025-05-02    |
|apple_pay     |779.6099999999999 |2025-05-02 14:33:53.459|2025-05-02    |
|bank_transfer |1316.84           |2025-05-02 14:33:53.459|2025-05-02    |
|google_pay    |47.62             |2025-05-02 14:33:53.459|2025-05-02    |
|credit_card   |715.01            |2025-05-02 14:33:53.459|2025-05-02    |
|bank_transfer |1879.8899999999999|2025-05-02 14:33:53.459|2025-05-02    |
|cryptocurrency|374.64            |2025-05-02 14:33:53.459|2025-05-02    |
|google_pay    |225.09            |2025-05-02 14:33:53.459|2025-05-02    |
+--------------+------------------+-----------------------+--------------+
only showing top 10 rows
25/05/02 14:34:46 INFO CodeGenerator: Code generated in 105.123444 ms

25/05/02 14:34:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] committed.
25/05/02 14:34:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 fromOffset=0 untilOffset=46, for query queryId=fff8ca60-eabe-44c3-9031-a5ea4bad0a62 batchId=0 taskId=2 partitionId=0
25/05/02 14:34:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/commits/0 using temp file file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/commits/.0.2de32783-c8f9-42c1-83b5-ca1c5dce4129.tmp
25/05/02 14:34:47 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/commits/0 using temp file file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/commits/.0.499efb98-29c3-4780-b56b-77bf2e5dcd9e.tmp
25/05/02 14:34:47 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:47 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:47 INFO AppInfoParser: Kafka startTimeMs: 1746196487462
25/05/02 14:34:47 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor-3, groupId=spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor] Assigned to partition(s): TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:47 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor-3, groupId=spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor] Seeking to offset 0 for partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:47 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor-3, groupId=spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor] Resetting the last seen epoch of partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 to 0 since the associated topicId changed from null to YlftBGnKQT-DHSU883bPLg
25/05/02 14:34:47 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor-3, groupId=spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor-3, groupId=spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor] Seeking to earliest offset of partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/commits/.0.2de32783-c8f9-42c1-83b5-ca1c5dce4129.tmp to file:/tmp/temporary-e91d0efc-9f2f-4b98-b159-772996870573/commits/0
25/05/02 14:34:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/commits/.0.499efb98-29c3-4780-b56b-77bf2e5dcd9e.tmp to file:/tmp/temporary-cbaa87c1-f834-47df-80d8-7fc3b49a3393/commits/0
25/05/02 14:34:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor-3, groupId=spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor] Resetting offset for partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor-3, groupId=spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor] Seeking to latest offset of partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0
25/05/02 14:34:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor-3, groupId=spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor] Resetting offset for partition TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dbaa7376-d22c-47c4-8a7c-958fcc90b4e4",
  "runId" : "0a6f1c6b-7ee4-4634-ba08-065bc3604ebb",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:37.664Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.651613451568122,
  "durationMs" : {
    "addBatch" : 44194,
    "commitOffsets" : 1299,
    "getBatch" : 201,
    "latestOffset" : 15303,
    "queryPlanning" : 7502,
    "triggerExecution" : 70593,
    "walCommit" : 1498
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.651613451568122,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@25c22043",
    "numOutputRows" : 46
  }
}
25/05/02 14:34:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "35e82855-ad10-440a-accf-e6afe3ab7d8c",
  "runId" : "ceb3a5e2-fb0c-4dbf-8343-b4a1abfc1ae9",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:47.059Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.7540612756749668,
  "durationMs" : {
    "addBatch" : 44202,
    "commitOffsets" : 1101,
    "getBatch" : 299,
    "latestOffset" : 6195,
    "queryPlanning" : 7399,
    "triggerExecution" : 61003,
    "walCommit" : 1303
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.7540612756749668,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@25c22043",
    "numOutputRows" : 46
  }
}
25/05/02 14:34:50 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD-0 groupId=spark-kafka-source-d858f65f-cca8-4b27-9e58-3a02edb715b0--145348779-executor read 46 records through 1 polls (polled  out 46 records), taking 997077705 nanos, during time span of 2698964122 nanos.
25/05/02 14:34:50 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1718 bytes result sent to driver
25/05/02 14:34:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 19803 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:50 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/05/02 14:34:50 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 32.497 s
25/05/02 14:34:50 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
25/05/02 14:34:50 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 33.100378 s
25/05/02 14:34:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/commits/0 using temp file file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/commits/.0.e76e2ae2-ba0c-4b07-b608-3db26f0dce1a.tmp
25/05/02 14:34:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/commits/.0.e76e2ae2-ba0c-4b07-b608-3db26f0dce1a.tmp to file:/tmp/temporary-a26307a2-2514-4095-bb6b-77b8ce2d3e53/commits/0
25/05/02 14:34:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fff8ca60-eabe-44c3-9031-a5ea4bad0a62",
  "runId" : "18382e4a-9bfc-4811-81f5-57904b198130",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:49.663Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.7256092751794305,
  "durationMs" : {
    "addBatch" : 49399,
    "commitOffsets" : 1000,
    "getBatch" : 200,
    "latestOffset" : 3895,
    "queryPlanning" : 7207,
    "triggerExecution" : 63395,
    "walCommit" : 1498
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.7256092751794305,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
25/05/02 14:34:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:34:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:51 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
25/05/02 14:38:51 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
25/05/02 14:38:51 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
25/05/02 14:38:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:46:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:46:53,425 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:46:54 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:46:54 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:46:54 INFO SparkContext: Java version 11.0.24
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO SparkContext: Submitted application: KafkaConsumer_TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD
25/05/02 14:46:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:46:56 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:46:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:46:56 INFO SecurityManager: Changing view acls to: root
25/05/02 14:46:56 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:46:56 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:46:56 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:46:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:47:01 INFO Utils: Successfully started service 'sparkDriver' on port 35437.
25/05/02 14:47:01 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:47:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-211b47a8-eede-47e7-b9f7-d4de63fef658
25/05/02 14:47:03 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:47:03 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:47:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/05/02 14:47:07 INFO Utils: Successfully started service 'SparkUI' on port 4041.
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:35437/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214229
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://5e919d8ee3aa:35437/jars/kafka-clients-3.3.1.jar with timestamp 1746197214229
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://5e919d8ee3aa:35437/jars/commons-pool2-2.11.1.jar with timestamp 1746197214229
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:35437/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214229
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://5e919d8ee3aa:35437/jars/postgresql-42.2.23.jar with timestamp 1746197214229
25/05/02 14:47:09 INFO Executor: Starting executor ID driver on host 5e919d8ee3aa
25/05/02 14:47:09 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:47:09 INFO Executor: Java version 11.0.24
25/05/02 14:47:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:47:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5089aef6 for default.
25/05/02 14:47:10 INFO Executor: Fetching spark://5e919d8ee3aa:35437/jars/postgresql-42.2.23.jar with timestamp 1746197214229
25/05/02 14:47:11 INFO TransportClientFactory: Successfully created connection to 5e919d8ee3aa/172.21.0.2:35437 after 499 ms (0 ms spent in bootstraps)
25/05/02 14:47:11 INFO Utils: Fetching spark://5e919d8ee3aa:35437/jars/postgresql-42.2.23.jar to /tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/fetchFileTemp12987693344544503692.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/postgresql-42.2.23.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:35437/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214229
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:35437/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/fetchFileTemp2212143789397611689.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:35437/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214229
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:35437/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/fetchFileTemp5421418621893187585.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:35437/jars/kafka-clients-3.3.1.jar with timestamp 1746197214229
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:35437/jars/kafka-clients-3.3.1.jar to /tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/fetchFileTemp9567743890568143309.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:47:13 INFO Executor: Fetching spark://5e919d8ee3aa:35437/jars/commons-pool2-2.11.1.jar with timestamp 1746197214229
25/05/02 14:47:13 INFO Utils: Fetching spark://5e919d8ee3aa:35437/jars/commons-pool2-2.11.1.jar to /tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/fetchFileTemp4883501600040934444.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/userFiles-4f1bfe46-1b04-4f26-bd5f-ad23b6ae03ea/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:47:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46735.
25/05/02 14:47:13 INFO NettyBlockTransferService: Server created on 5e919d8ee3aa:46735
25/05/02 14:47:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:47:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5e919d8ee3aa, 46735, None)
25/05/02 14:47:13 INFO BlockManagerMasterEndpoint: Registering block manager 5e919d8ee3aa:46735 with 1007.8 MiB RAM, BlockManagerId(driver, 5e919d8ee3aa, 46735, None)
25/05/02 14:47:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5e919d8ee3aa, 46735, None)
25/05/02 14:47:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5e919d8ee3aa, 46735, None)
2025-05-02 14:47:18,428 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Session Spark créée.
2025-05-02 14:47:18,429 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD.
2025-05-02 14:47:18,429 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:47:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:47:18 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:47:39,224 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Données chargées avec succès.
2025-05-02 14:47:39,224 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Vérification du format initial.
25/05/02 14:47:41 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:47:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-658b4ecb-348c-4bdf-84dd-37039a851506. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:42 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-658b4ecb-348c-4bdf-84dd-37039a851506 resolved to file:/tmp/temporary-658b4ecb-348c-4bdf-84dd-37039a851506.
25/05/02 14:47:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-658b4ecb-348c-4bdf-84dd-37039a851506/metadata using temp file file:/tmp/temporary-658b4ecb-348c-4bdf-84dd-37039a851506/.metadata.45b13b29-3333-4c67-8df2-b82adfdb7583.tmp
25/05/02 14:47:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-658b4ecb-348c-4bdf-84dd-37039a851506/.metadata.45b13b29-3333-4c67-8df2-b82adfdb7583.tmp to file:/tmp/temporary-658b4ecb-348c-4bdf-84dd-37039a851506/metadata
25/05/02 14:47:46 INFO MicroBatchExecution: Starting [id = e9615233-ee7a-4819-afad-fc23c043ee09, runId = 5acd737a-db60-47a5-a23a-26d3a49f7c82]. Use file:/tmp/temporary-658b4ecb-348c-4bdf-84dd-37039a851506 to store the query checkpoint.
2025-05-02 14:47:46,930 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Données en cours de transformation...
25/05/02 14:47:47 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@15df8d3] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@664b567]
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:47 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:47:51,734 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Données parsée avec succès !!!
root
 |-- PAYMENT_METHOD: string (nullable = true)
 |-- TOTAL_AMOUNT: double (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:47:51,831 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Vérification des données chargées :
25/05/02 14:47:52 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5e5dfaa0-04cf-49ed-9815-ad0b254741de. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:52 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-5e5dfaa0-04cf-49ed-9815-ad0b254741de resolved to file:/tmp/temporary-5e5dfaa0-04cf-49ed-9815-ad0b254741de.
25/05/02 14:47:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-5e5dfaa0-04cf-49ed-9815-ad0b254741de/metadata using temp file file:/tmp/temporary-5e5dfaa0-04cf-49ed-9815-ad0b254741de/.metadata.78c52435-be4b-4b0c-a9c6-ca511a041c76.tmp
25/05/02 14:47:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-5e5dfaa0-04cf-49ed-9815-ad0b254741de/.metadata.78c52435-be4b-4b0c-a9c6-ca511a041c76.tmp to file:/tmp/temporary-5e5dfaa0-04cf-49ed-9815-ad0b254741de/metadata
25/05/02 14:47:55 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:55 INFO MicroBatchExecution: Starting [id = 072f4ee1-58b6-47fa-a527-94b5c7a4f1c4, runId = 6ea795ea-567e-4ac4-8fe9-0b204353fcbe]. Use file:/tmp/temporary-5e5dfaa0-04cf-49ed-9815-ad0b254741de to store the query checkpoint.
2025-05-02 14:47:55,826 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:47:55,826 - TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:47:55 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@15df8d3] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@664b567]
25/05/02 14:47:55 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:56 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:56 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:56 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2b45fea2-ee55-40a0-a697-a1902ca8b1ba. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:56 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-2b45fea2-ee55-40a0-a697-a1902ca8b1ba resolved to file:/tmp/temporary-2b45fea2-ee55-40a0-a697-a1902ca8b1ba.
25/05/02 14:47:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:56 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:56 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:56 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:56 INFO AppInfoParser: Kafka startTimeMs: 1746197276724
25/05/02 14:47:56 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:56 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:56 INFO AppInfoParser: Kafka startTimeMs: 1746197276826
25/05/02 14:47:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2b45fea2-ee55-40a0-a697-a1902ca8b1ba/metadata using temp file file:/tmp/temporary-2b45fea2-ee55-40a0-a697-a1902ca8b1ba/.metadata.63963622-cdd0-492a-b580-968f82c89122.tmp
25/05/02 14:47:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2b45fea2-ee55-40a0-a697-a1902ca8b1ba/.metadata.63963622-cdd0-492a-b580-968f82c89122.tmp to file:/tmp/temporary-2b45fea2-ee55-40a0-a697-a1902ca8b1ba/metadata
25/05/02 14:47:58 INFO MicroBatchExecution: Starting [id = 633e8adb-b162-492c-8fe9-03216dd17266, runId = 2db3ee40-25cb-4dee-861b-53de0b834e14]. Use file:/tmp/temporary-2b45fea2-ee55-40a0-a697-a1902ca8b1ba to store the query checkpoint.
25/05/02 14:47:58 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@15df8d3] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@664b567]
25/05/02 14:47:58 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:58 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:58 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:58 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:59 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:59 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:59 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:59 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:59 INFO AppInfoParser: Kafka startTimeMs: 1746197279124
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282224
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282225
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282224
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-6 unregistered
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283335
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-5 unregistered
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-4 unregistered
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283433
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283433
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-7 unregistered
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 ERROR MicroBatchExecution: Query [id = 633e8adb-b162-492c-8fe9-03216dd17266, runId = 2db3ee40-25cb-4dee-861b-53de0b834e14] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = 633e8adb-b162-492c-8fe9-03216dd17266, runId = 2db3ee40-25cb-4dee-861b-53de0b834e14] has been shutdown
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-9 unregistered
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-8 unregistered
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 ERROR MicroBatchExecution: Query [id = e9615233-ee7a-4819-afad-fc23c043ee09, runId = 5acd737a-db60-47a5-a23a-26d3a49f7c82] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = e9615233-ee7a-4819-afad-fc23c043ee09, runId = 5acd737a-db60-47a5-a23a-26d3a49f7c82] has been shutdown
Traceback (most recent call last):
  File "/app/consumer-table/writter_TOTAL_TRANSACTION_AMOUNT_PER_PAYMENT_METHOD.py", line 104, in <module>
25/05/02 14:48:04 ERROR MicroBatchExecution: Query [id = 072f4ee1-58b6-47fa-a527-94b5c7a4f1c4, runId = 6ea795ea-567e-4ac4-8fe9-0b204353fcbe] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = 072f4ee1-58b6-47fa-a527-94b5c7a4f1c4, runId = 6ea795ea-567e-4ac4-8fe9-0b204353fcbe] has been shutdown
    df_parsed
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 633e8adb-b162-492c-8fe9-03216dd17266, runId = 2db3ee40-25cb-4dee-861b-53de0b834e14] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO SparkContext: Invoking stop() from shutdown hook
25/05/02 14:48:04 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/05/02 14:48:04 INFO SparkUI: Stopped Spark web UI at http://5e919d8ee3aa:4041
25/05/02 14:48:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/05/02 14:48:04 INFO MemoryStore: MemoryStore cleared
25/05/02 14:48:04 INFO BlockManager: BlockManager stopped
25/05/02 14:48:05 INFO BlockManagerMaster: BlockManagerMaster stopped
25/05/02 14:48:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/05/02 14:48:05 INFO SparkContext: Successfully stopped SparkContext
25/05/02 14:48:05 INFO ShutdownHookManager: Shutdown hook called
25/05/02 14:48:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-469c50ff-39e2-4a28-bcf4-c8ef8f50e8bd
25/05/02 14:48:05 INFO ShutdownHookManager: Deleting directory /tmp/temporary-658b4ecb-348c-4bdf-84dd-37039a851506
25/05/02 14:48:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a
25/05/02 14:48:05 INFO ShutdownHookManager: Deleting directory /tmp/temporary-2b45fea2-ee55-40a0-a697-a1902ca8b1ba
25/05/02 14:48:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-66738baa-3948-4378-8cc5-fd51dcce057a/pyspark-0cc12a34-f605-4731-abb3-d9e85544004b
25/05/02 14:48:05 INFO ShutdownHookManager: Deleting directory /tmp/temporary-5e5dfaa0-04cf-49ed-9815-ad0b254741de
