25/05/02 14:32:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:32:50,868 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:32:51 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:32:51 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:32:51 INFO SparkContext: Java version 11.0.24
25/05/02 14:32:52 INFO ResourceUtils: ==============================================================
25/05/02 14:32:52 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:32:52 INFO ResourceUtils: ==============================================================
25/05/02 14:32:52 INFO SparkContext: Submitted application: KafkaConsumer_TOTAL_SPENT_PER_USER_TRANSACTION_TYPE
25/05/02 14:32:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:32:52 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:32:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:32:53 INFO SecurityManager: Changing view acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:32:56 INFO Utils: Successfully started service 'sparkDriver' on port 38767.
25/05/02 14:32:56 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:32:57 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:32:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:32:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:32:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:32:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c01f73ec-3240-4770-9834-3f20803f2394
25/05/02 14:32:58 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:32:58 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:33:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/05/02 14:33:01 INFO Utils: Successfully started service 'SparkUI' on port 4044.
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:38767/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371462
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://3109b9eaa053:38767/jars/kafka-clients-3.3.1.jar with timestamp 1746196371462
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://3109b9eaa053:38767/jars/commons-pool2-2.11.1.jar with timestamp 1746196371462
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:38767/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371462
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://3109b9eaa053:38767/jars/postgresql-42.2.23.jar with timestamp 1746196371462
25/05/02 14:33:03 INFO Executor: Starting executor ID driver on host 3109b9eaa053
25/05/02 14:33:03 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:33:03 INFO Executor: Java version 11.0.24
25/05/02 14:33:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:33:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@598130d for default.
25/05/02 14:33:03 INFO Executor: Fetching spark://3109b9eaa053:38767/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371462
25/05/02 14:33:04 INFO TransportClientFactory: Successfully created connection to 3109b9eaa053/172.21.0.7:38767 after 694 ms (0 ms spent in bootstraps)
25/05/02 14:33:04 INFO Utils: Fetching spark://3109b9eaa053:38767/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/fetchFileTemp4472529810889147610.tmp
25/05/02 14:33:05 INFO Executor: Adding file:/tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:05 INFO Executor: Fetching spark://3109b9eaa053:38767/jars/postgresql-42.2.23.jar with timestamp 1746196371462
25/05/02 14:33:05 INFO Utils: Fetching spark://3109b9eaa053:38767/jars/postgresql-42.2.23.jar to /tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/fetchFileTemp8143701646864084249.tmp
25/05/02 14:33:05 INFO Executor: Adding file:/tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/postgresql-42.2.23.jar to class loader default
25/05/02 14:33:05 INFO Executor: Fetching spark://3109b9eaa053:38767/jars/commons-pool2-2.11.1.jar with timestamp 1746196371462
25/05/02 14:33:05 INFO Utils: Fetching spark://3109b9eaa053:38767/jars/commons-pool2-2.11.1.jar to /tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/fetchFileTemp7537639724150432793.tmp
25/05/02 14:33:05 INFO Executor: Adding file:/tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:33:05 INFO Executor: Fetching spark://3109b9eaa053:38767/jars/kafka-clients-3.3.1.jar with timestamp 1746196371462
25/05/02 14:33:05 INFO Utils: Fetching spark://3109b9eaa053:38767/jars/kafka-clients-3.3.1.jar to /tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/fetchFileTemp8904297870948083754.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:38767/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371462
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:38767/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/fetchFileTemp2500915548814474663.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-f762019b-39f1-4bbd-8a66-d35c7e48db17/userFiles-ba790903-cba2-4f93-a1e4-c1eaeb22689d/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42385.
25/05/02 14:33:06 INFO NettyBlockTransferService: Server created on 3109b9eaa053:42385
25/05/02 14:33:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:33:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3109b9eaa053, 42385, None)
25/05/02 14:33:06 INFO BlockManagerMasterEndpoint: Registering block manager 3109b9eaa053:42385 with 1007.8 MiB RAM, BlockManagerId(driver, 3109b9eaa053, 42385, None)
25/05/02 14:33:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3109b9eaa053, 42385, None)
25/05/02 14:33:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3109b9eaa053, 42385, None)
2025-05-02 14:33:11,063 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Session Spark créée.
2025-05-02 14:33:11,063 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic TOTAL_SPENT_PER_USER_TRANSACTION_TYPE.
2025-05-02 14:33:11,064 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:33:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:33:11 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:33:30,868 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Données chargées avec succès.
2025-05-02 14:33:30,868 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Vérification du format initial.
25/05/02 14:33:33 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:33:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:33 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea resolved to file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea.
25/05/02 14:33:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/metadata using temp file file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/.metadata.597c061b-983b-44ad-9589-9f1bc1362c32.tmp
25/05/02 14:33:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/.metadata.597c061b-983b-44ad-9589-9f1bc1362c32.tmp to file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/metadata
25/05/02 14:33:36 INFO MicroBatchExecution: Starting [id = 81313910-acba-4718-8468-6859ab9467ff, runId = f7bdfb12-2b75-4695-92d1-f10c00a2d4fc]. Use file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea to store the query checkpoint.
2025-05-02 14:33:37,059 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Données en cours de transformation...
25/05/02 14:33:37 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2956798c] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@59410af2]
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:37 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:37 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:33:42,267 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Données parsée avec succès !!!
root
 |-- KSQL_COL_0: string (nullable = true)
 |-- TOTAL_SPENT: double (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:33:42,657 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Vérification des données chargées :
25/05/02 14:33:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:43 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60 resolved to file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60.
25/05/02 14:33:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/metadata using temp file file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/.metadata.f9f1169f-d93c-47bf-a754-e54fdc352e05.tmp
25/05/02 14:33:45 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/.metadata.f9f1169f-d93c-47bf-a754-e54fdc352e05.tmp to file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/metadata
25/05/02 14:33:46 INFO MicroBatchExecution: Starting [id = d7dfd0b9-f3cb-4b02-93a8-b3b220857804, runId = 92682a5c-958e-468b-be4e-86f8e90452cb]. Use file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60 to store the query checkpoint.
2025-05-02 14:33:46,362 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:33:46,362 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:33:46 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2956798c] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@59410af2]
25/05/02 14:33:46 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:46 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:46 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:46 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:46 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:46 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:46 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:46 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:46 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226 resolved to file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226.
25/05/02 14:33:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:46 INFO AppInfoParser: Kafka startTimeMs: 1746196426861
25/05/02 14:33:47 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:47 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:47 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:47 INFO AppInfoParser: Kafka startTimeMs: 1746196427357
25/05/02 14:33:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/metadata using temp file file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/.metadata.7ac9689c-ae28-4907-b73b-b8fa0b7d3950.tmp
25/05/02 14:33:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/.metadata.7ac9689c-ae28-4907-b73b-b8fa0b7d3950.tmp to file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/metadata
25/05/02 14:33:49 INFO MicroBatchExecution: Starting [id = af6324a6-292f-43c9-9397-c2f95393f313, runId = 0d437e4b-cec0-487b-8a64-42b979089610]. Use file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226 to store the query checkpoint.
25/05/02 14:33:49 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2956798c] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@59410af2]
25/05/02 14:33:49 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:49 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:49 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:49 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:50 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:50 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:50 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:50 INFO AppInfoParser: Kafka startTimeMs: 1746196430257
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/sources/0/0 using temp file file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/sources/0/.0.7a1e3533-9a35-43cf-a456-1067c79346fb.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/sources/0/0 using temp file file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/sources/0/.0.ce920138-7b22-4def-90c6-9f98fc34a9b3.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/sources/0/0 using temp file file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/sources/0/.0.14c7986c-14f9-4f2f-a3ae-05397b563cbd.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/sources/0/.0.ce920138-7b22-4def-90c6-9f98fc34a9b3.tmp to file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/sources/0/0
25/05/02 14:33:52 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_SPENT_PER_USER_TRANSACTION_TYPE":{"0":0}}
25/05/02 14:33:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/sources/0/.0.7a1e3533-9a35-43cf-a456-1067c79346fb.tmp to file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/sources/0/0
25/05/02 14:33:52 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_SPENT_PER_USER_TRANSACTION_TYPE":{"0":0}}
25/05/02 14:33:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/sources/0/.0.14c7986c-14f9-4f2f-a3ae-05397b563cbd.tmp to file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/sources/0/0
25/05/02 14:33:52 INFO KafkaMicroBatchStream: Initial offsets: {"TOTAL_SPENT_PER_USER_TRANSACTION_TYPE":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/offsets/0 using temp file file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/offsets/.0.c2c25f6b-f5de-4ec3-9751-1abc42c7488b.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/offsets/0 using temp file file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/offsets/.0.d6defca3-f811-44de-bd1a-892087bd0c96.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/offsets/0 using temp file file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/offsets/.0.727beaff-4cde-486a-b974-1702133ced7a.tmp
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/offsets/.0.727beaff-4cde-486a-b974-1702133ced7a.tmp to file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433160,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/offsets/.0.d6defca3-f811-44de-bd1a-892087bd0c96.tmp to file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196432966,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/offsets/.0.c2c25f6b-f5de-4ec3-9751-1abc42c7488b.tmp to file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433059,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:34:00 INFO IncrementalExecution: Current batch timestamp = 1746196432966
25/05/02 14:34:00 INFO IncrementalExecution: Current batch timestamp = 1746196433059
25/05/02 14:34:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO IncrementalExecution: Current batch timestamp = 1746196433059
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO IncrementalExecution: Current batch timestamp = 1746196432966
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO IncrementalExecution: Current batch timestamp = 1746196432966
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]]. The input RDD has 1 partitions.
25/05/02 14:34:06 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:07 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:07 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/05/02 14:34:07 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:07 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:09 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1746196447558,WrappedArray(org.apache.spark.scheduler.StageInfo@550b9b6b),{spark.driver.port=38767, spark.submit.pyFiles=, spark.app.startTime=1746196371462, spark.executor.extraClassPath=/spark/jars/*, spark.rdd.compress=True, spark.driver.extraClassPath=/spark/jars/*, callSite.short=start at <unknown>:0, __is_continuous_processing=false, spark.jobGroup.id=92682a5c-958e-468b-be4e-86f8e90452cb, spark.sql.requireAllClusterKeysForDistribution=false, spark.app.submitTime=1746196367168, spark.sql.adaptive.enabled=false, spark.app.initial.jar.urls=spark://3109b9eaa053:38767/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,spark://3109b9eaa053:38767/jars/postgresql-42.2.23.jar,spark://3109b9eaa053:38767/jars/commons-pool2-2.11.1.jar,spark://3109b9eaa053:38767/jars/kafka-clients-3.3.1.jar,spark://3109b9eaa053:38767/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar, spark.sql.execution.id=3, sql.streaming.queryId=d7dfd0b9-f3cb-4b02-93a8-b3b220857804, spark.sql.warehouse.dir=file:/app/spark-warehouse, streaming.sql.batchId=0, callSite.long=org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:251)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
java.base/java.lang.reflect.Method.invoke(Unknown Source)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
py4j.Gateway.invoke(Gateway.java:282)
py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
py4j.commands.CallCommand.execute(CallCommand.java:79)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.base/java.lang.Thread.run(Unknown Source), spark.executor.memory=2g, spark.driver.memory=2g, spark.master=local[*], spark.job.interruptOnCancel=true, spark.repl.local.jars=file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/kafka-clients-3.3.1.jar,file:///spark/jars/commons-pool2-2.11.1.jar,file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/postgresql-42.2.23.jar, spark.executor.id=driver, spark.app.name=KafkaConsumer_TOTAL_SPENT_PER_USER_TRANSACTION_TYPE, spark.submit.deployMode=client, spark.driver.host=3109b9eaa053, spark.sql.caseSensitive=true, spark.app.id=local-1746196382563, spark.job.description=
id = d7dfd0b9-f3cb-4b02-93a8-b3b220857804
runId = 92682a5c-958e-468b-be4e-86f8e90452cb
batch = 0, spark.sql.cbo.enabled=false, spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false, spark.sql.execution.root.id=2, spark.jars=file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/kafka-clients-3.3.1.jar,file:///spark/jars/commons-pool2-2.11.1.jar,file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,file:///spark/jars/postgresql-42.2.23.jar, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false}) by listener AppStatusListener took 1.299719626s.
25/05/02 14:34:14 INFO CodeGenerator: Code generated in 7100.262605 ms
25/05/02 14:34:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
25/05/02 14:34:15 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 19.3 KiB, free 1007.8 MiB)
25/05/02 14:34:15 INFO CodeGenerator: Code generated in 498.965964 ms
25/05/02 14:34:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 1007.8 MiB)
25/05/02 14:34:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3109b9eaa053:42385 (size: 9.2 KiB, free: 1007.8 MiB)
25/05/02 14:34:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:17 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/05/02 14:34:17 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:18 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
25/05/02 14:34:18 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:18 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.6 KiB, free 1007.8 MiB)
25/05/02 14:34:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 1007.8 MiB)
25/05/02 14:34:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3109b9eaa053:42385 (size: 4.5 KiB, free: 1007.8 MiB)
25/05/02 14:34:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9086 bytes) 
25/05/02 14:34:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/05/02 14:34:19 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:19 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
25/05/02 14:34:19 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:19 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/05/02 14:34:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 33.1 KiB, free 1007.8 MiB)
25/05/02 14:34:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 1007.8 MiB)
25/05/02 14:34:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3109b9eaa053:42385 (size: 15.2 KiB, free: 1007.8 MiB)
25/05/02 14:34:21 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
25/05/02 14:34:22 INFO CodeGenerator: Code generated in 803.171187 ms
25/05/02 14:34:23 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 fromOffset=0 untilOffset=46, for query queryId=d7dfd0b9-f3cb-4b02-93a8-b3b220857804 batchId=0 taskId=0 partitionId=0
25/05/02 14:34:23 INFO CodeGenerator: Code generated in 196.653704 ms
25/05/02 14:34:24 INFO CodeGenerator: Code generated in 397.328944 ms
25/05/02 14:34:24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:25 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:25 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:25 INFO AppInfoParser: Kafka startTimeMs: 1746196465457
25/05/02 14:34:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor-1, groupId=spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor] Assigned to partition(s): TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor-1, groupId=spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor] Seeking to offset 0 for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:25 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor-1, groupId=spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor] Resetting the last seen epoch of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to 0 since the associated topicId changed from null to p5AvJOwsTnuw9q1OuN0JQQ
25/05/02 14:34:25 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor-1, groupId=spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor-1, groupId=spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor] Seeking to earliest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor-1, groupId=spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor-1, groupId=spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor] Seeking to latest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor-1, groupId=spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:28 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/05/02 14:34:28 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 groupId=spark-kafka-source-1b07bfbb-6548-441b-807c-ba7b86272307-1340818913-executor read 46 records through 1 polls (polled  out 46 records), taking 900879871 nanos, during time span of 3194617752 nanos.
25/05/02 14:34:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 6653 bytes result sent to driver
25/05/02 14:34:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9086 bytes) 
25/05/02 14:34:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
25/05/02 14:34:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11003 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/05/02 14:34:29 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 21.600 s
25/05/02 14:34:29 INFO CodeGenerator: Code generated in 100.565061 ms
25/05/02 14:34:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:29 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 fromOffset=0 untilOffset=46, for query queryId=81313910-acba-4718-8468-6859ab9467ff batchId=0 taskId=1 partitionId=0
25/05/02 14:34:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/05/02 14:34:30 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 23.401865 s
25/05/02 14:34:30 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] is committing.
25/05/02 14:34:30 INFO AppInfoParser: Kafka version: 3.3.1
-------------------------------------------
25/05/02 14:34:30 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:30 INFO AppInfoParser: Kafka startTimeMs: 1746196470163
Batch: 0
25/05/02 14:34:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor-2, groupId=spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor] Assigned to partition(s): TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor-2, groupId=spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor] Seeking to offset 0 for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
-------------------------------------------
25/05/02 14:34:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor-2, groupId=spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor] Resetting the last seen epoch of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to 0 since the associated topicId changed from null to p5AvJOwsTnuw9q1OuN0JQQ
25/05/02 14:34:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor-2, groupId=spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor-2, groupId=spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor] Seeking to earliest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor-2, groupId=spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor-2, groupId=spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor] Seeking to latest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor-2, groupId=spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:31 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
25/05/02 14:34:31 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 groupId=spark-kafka-source-05838e85-0c52-4ea7-9dd1-905e17651fec--158229274-executor read 46 records through 1 polls (polled  out 46 records), taking 1197918681 nanos, during time span of 1299126358 nanos.
25/05/02 14:34:31 INFO CodeGenerator: Code generated in 103.366927 ms
25/05/02 14:34:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 7234 bytes result sent to driver
25/05/02 14:34:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9086 bytes) 
25/05/02 14:34:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
25/05/02 14:34:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3096 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/05/02 14:34:32 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 14.397 s
25/05/02 14:34:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/05/02 14:34:32 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 17.302406 s
25/05/02 14:34:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/02 14:34:34 INFO CodeGenerator: Code generated in 393.243711 ms
25/05/02 14:34:35 INFO CodeGenerator: Code generated in 103.054264 ms
25/05/02 14:34:35 INFO CodeGenerator: Code generated in 8.573476 ms
25/05/02 14:34:47 INFO CodeGenerator: Code generated in 97.97658 ms
25/05/02 14:34:47 INFO CodeGenerator: Code generated in 198.264537 ms
25/05/02 14:34:47 INFO CodeGenerator: Code generated in 297.172126 ms
+--------------------+--------------------+
|                 key|               value|
+--------------------+--------------------+
|77e9cdcb374f18c6a...|{"TOTAL_SPENT":48...|
|b84666e6e9147e515...|{"TOTAL_SPENT":5....|
|4a768c0fa4009f2f1...|{"TOTAL_SPENT":17...|
|4cac338b918381d53...|{"TOTAL_SPENT":13...|
|7501d5488f7114f6f...|{"TOTAL_SPENT":38...|
|1c6254042b7ed7465...|{"TOTAL_SPENT":51...|
|e590d9663000d8e61...|{"TOTAL_SPENT":53...|
|38f050b1100158e91...|{"TOTAL_SPENT":42...|
|408ed72451c80bc4b...|{"TOTAL_SPENT":40...|
|7a0865eb4e33e8f57...|{"TOTAL_SPENT":13...|
|66d6a49b9d82e6c73...|{"TOTAL_SPENT":19...|
|15515cb01fca83455...|{"TOTAL_SPENT":10...|
|88708f34e1e6a2c93...|{"TOTAL_SPENT":14...|
|6f5e27f7c3e389bce...|{"TOTAL_SPENT":23...|
|420dbe968c87dc574...|{"TOTAL_SPENT":5....|
|4b564cbf0845b3578...|{"TOTAL_SPENT":59...|
|60fb204653b6ae155...|{"TOTAL_SPENT":41...|
|439e3169c933d710b...|{"TOTAL_SPENT":5....|
|63b3b1b2ae37e08ab...|{"TOTAL_SPENT":60...|
|07e234a06f091e83a...|{"TOTAL_SPENT":36...|
+--------------------+--------------------+
only showing top 20 rows

25/05/02 14:34:47 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 fromOffset=0 untilOffset=46, for query queryId=af6324a6-292f-43c9-9397-c2f95393f313 batchId=0 taskId=2 partitionId=0
25/05/02 14:34:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
+-------------------------------------------+------------------+-----------------------+--------------+
|KSQL_COL_0                                 |TOTAL_SPENT       |ingestion_time         |ingestion_date|
+-------------------------------------------+------------------+-----------------------+--------------+
|77e9cdcb374f18c6ae06e89e61b545dd-withdrawal|489.97499999999997|2025-05-02 14:33:52.966|2025-05-02    |
|b84666e6e9147e5155016ddfe3559a0d-purchase  |5.59153           |2025-05-02 14:33:52.966|2025-05-02    |
|4a768c0fa4009f2f1d24c2ea26e9c6a7-payment   |170.80200000000002|2025-05-02 14:33:52.966|2025-05-02    |
|4cac338b918381d5367740c8277ea61b-purchase  |136.4148          |2025-05-02 14:33:52.966|2025-05-02    |
|7501d5488f7114f6f016d1726f6f77e4-purchase  |388.692           |2025-05-02 14:33:52.966|2025-05-02    |
|1c6254042b7ed7465c6e20ab37a26d2b-payment   |51.4296           |2025-05-02 14:33:52.966|2025-05-02    |
|e590d9663000d8e616bbeffbaf633d6d-withdrawal|536.2574999999999 |2025-05-02 14:33:52.966|2025-05-02    |
|38f050b1100158e918fa4c0088002205-refund    |422.28749999999997|2025-05-02 14:33:52.966|2025-05-02    |
|408ed72451c80bc4bad8ae1c217e7192-refund    |404.6112          |2025-05-02 14:33:52.966|2025-05-02    |
|7a0865eb4e33e8f571fad4fc294a152c-refund    |133.1025          |2025-05-02 14:33:52.966|2025-05-02    |
+-------------------------------------------+------------------+-----------------------+--------------+
only showing top 10 rows

25/05/02 14:34:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] committed.
25/05/02 14:34:48 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:48 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:48 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:48 INFO AppInfoParser: Kafka startTimeMs: 1746196488461
25/05/02 14:34:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor-3, groupId=spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor] Assigned to partition(s): TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor-3, groupId=spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor] Seeking to offset 0 for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:48 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor-3, groupId=spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor] Resetting the last seen epoch of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to 0 since the associated topicId changed from null to p5AvJOwsTnuw9q1OuN0JQQ
25/05/02 14:34:48 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor-3, groupId=spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/commits/0 using temp file file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/commits/.0.159480b5-29a6-4f5a-a505-3e04e6431946.tmp
25/05/02 14:34:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/commits/0 using temp file file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/commits/.0.c7f45844-142f-4366-813b-c8997eb511ce.tmp
25/05/02 14:34:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor-3, groupId=spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor] Seeking to earliest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor-3, groupId=spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor-3, groupId=spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor] Seeking to latest offset of partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0
25/05/02 14:34:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor-3, groupId=spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor] Resetting offset for partition TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/commits/.0.c7f45844-142f-4366-813b-c8997eb511ce.tmp to file:/tmp/temporary-30d3c254-3512-4958-b06f-6fe30d0ef9ea/commits/0
25/05/02 14:34:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/commits/.0.159480b5-29a6-4f5a-a505-3e04e6431946.tmp to file:/tmp/temporary-188943f9-b890-474b-9f7d-1f677871ad60/commits/0
25/05/02 14:34:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "d7dfd0b9-f3cb-4b02-93a8-b3b220857804",
  "runId" : "92682a5c-958e-468b-be4e-86f8e90452cb",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:46.461Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.7232476966133141,
  "durationMs" : {
    "addBatch" : 45302,
    "commitOffsets" : 2003,
    "getBatch" : 3,
    "latestOffset" : 6207,
    "queryPlanning" : 7799,
    "triggerExecution" : 63602,
    "walCommit" : 1598
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_SPENT_PER_USER_TRANSACTION_TYPE]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.7232476966133141,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@6aeee665",
    "numOutputRows" : 46
  }
}
25/05/02 14:34:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "81313910-acba-4718-8468-6859ab9467ff",
  "runId" : "f7bdfb12-2b75-4695-92d1-f10c00a2d4fc",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:37.465Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.6345615317763584,
  "durationMs" : {
    "addBatch" : 45299,
    "commitOffsets" : 1896,
    "getBatch" : 398,
    "latestOffset" : 15399,
    "queryPlanning" : 7991,
    "triggerExecution" : 72490,
    "walCommit" : 1004
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_SPENT_PER_USER_TRANSACTION_TYPE]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.6345615317763584,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@6aeee665",
    "numOutputRows" : 46
  }
}
25/05/02 14:34:52 INFO KafkaDataConsumer: From Kafka topicPartition=TOTAL_SPENT_PER_USER_TRANSACTION_TYPE-0 groupId=spark-kafka-source-2ae95841-4a74-4ad3-9104-d86208a44adc-1413608602-executor read 46 records through 1 polls (polled  out 46 records), taking 1203169223 nanos, during time span of 3599474733 nanos.
25/05/02 14:34:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1675 bytes result sent to driver
25/05/02 14:34:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 20304 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/05/02 14:34:52 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 32.996 s
25/05/02 14:34:52 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
25/05/02 14:34:52 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 35.601030 s
25/05/02 14:34:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/commits/0 using temp file file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/commits/.0.8d8ea019-0e76-4fed-ad99-19fe50d4e7aa.tmp
25/05/02 14:34:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/commits/.0.8d8ea019-0e76-4fed-ad99-19fe50d4e7aa.tmp to file:/tmp/temporary-e4fd4643-d868-4f30-9a50-c8e2cc056226/commits/0
25/05/02 14:34:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "af6324a6-292f-43c9-9397-c2f95393f313",
  "runId" : "0d437e4b-cec0-487b-8a64-42b979089610",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:49.562Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.7055322934400835,
  "durationMs" : {
    "addBatch" : 51095,
    "commitOffsets" : 1004,
    "getBatch" : 3,
    "latestOffset" : 3302,
    "queryPlanning" : 7799,
    "triggerExecution" : 65199,
    "walCommit" : 1601
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TOTAL_SPENT_PER_USER_TRANSACTION_TYPE]]",
    "startOffset" : null,
    "endOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "TOTAL_SPENT_PER_USER_TRANSACTION_TYPE" : {
        "0" : 46
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.7055322934400835,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "ForeachBatchSink",
    "numOutputRows" : -1
  }
}
25/05/02 14:35:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:35:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:36:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:37:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:38:50 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
25/05/02 14:38:50 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
25/05/02 14:38:50 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
25/05/02 14:38:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:39:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:39:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/02 14:46:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:46:53,230 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:46:54 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:46:54 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:46:54 INFO SparkContext: Java version 11.0.24
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO SparkContext: Submitted application: KafkaConsumer_TOTAL_SPENT_PER_USER_TRANSACTION_TYPE
25/05/02 14:46:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:46:56 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:46:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:46:57 INFO SecurityManager: Changing view acls to: root
25/05/02 14:46:57 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:46:57 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:46:57 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:46:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:47:01 INFO Utils: Successfully started service 'sparkDriver' on port 41391.
25/05/02 14:47:01 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:47:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6b593f82-0efd-4dc4-bf8d-67f9a3a482b5
25/05/02 14:47:03 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:47:03 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:47:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/05/02 14:47:07 INFO Utils: Successfully started service 'SparkUI' on port 4044.
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:41391/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214129
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://5e919d8ee3aa:41391/jars/kafka-clients-3.3.1.jar with timestamp 1746197214129
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://5e919d8ee3aa:41391/jars/commons-pool2-2.11.1.jar with timestamp 1746197214129
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:41391/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214129
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://5e919d8ee3aa:41391/jars/postgresql-42.2.23.jar with timestamp 1746197214129
25/05/02 14:47:09 INFO Executor: Starting executor ID driver on host 5e919d8ee3aa
25/05/02 14:47:09 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:47:09 INFO Executor: Java version 11.0.24
25/05/02 14:47:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:47:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@26387cb6 for default.
25/05/02 14:47:10 INFO Executor: Fetching spark://5e919d8ee3aa:41391/jars/commons-pool2-2.11.1.jar with timestamp 1746197214129
25/05/02 14:47:11 INFO TransportClientFactory: Successfully created connection to 5e919d8ee3aa/172.21.0.2:41391 after 502 ms (0 ms spent in bootstraps)
25/05/02 14:47:11 INFO Utils: Fetching spark://5e919d8ee3aa:41391/jars/commons-pool2-2.11.1.jar to /tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/fetchFileTemp16415029414481584911.tmp
25/05/02 14:47:11 INFO Executor: Adding file:/tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:47:11 INFO Executor: Fetching spark://5e919d8ee3aa:41391/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214129
25/05/02 14:47:11 INFO Utils: Fetching spark://5e919d8ee3aa:41391/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/fetchFileTemp13754382062149278848.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:41391/jars/postgresql-42.2.23.jar with timestamp 1746197214129
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:41391/jars/postgresql-42.2.23.jar to /tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/fetchFileTemp13841759176536248055.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/postgresql-42.2.23.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:41391/jars/kafka-clients-3.3.1.jar with timestamp 1746197214129
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:41391/jars/kafka-clients-3.3.1.jar to /tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/fetchFileTemp17866799917553982424.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:47:13 INFO Executor: Fetching spark://5e919d8ee3aa:41391/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214129
25/05/02 14:47:13 INFO Utils: Fetching spark://5e919d8ee3aa:41391/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/fetchFileTemp13819000772424801598.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/userFiles-4985ae18-b26f-4b47-80dc-72ed1cba5847/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46423.
25/05/02 14:47:13 INFO NettyBlockTransferService: Server created on 5e919d8ee3aa:46423
25/05/02 14:47:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:47:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5e919d8ee3aa, 46423, None)
25/05/02 14:47:13 INFO BlockManagerMasterEndpoint: Registering block manager 5e919d8ee3aa:46423 with 1007.8 MiB RAM, BlockManagerId(driver, 5e919d8ee3aa, 46423, None)
25/05/02 14:47:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5e919d8ee3aa, 46423, None)
25/05/02 14:47:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5e919d8ee3aa, 46423, None)
2025-05-02 14:47:18,431 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Session Spark créée.
2025-05-02 14:47:18,431 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic TOTAL_SPENT_PER_USER_TRANSACTION_TYPE.
2025-05-02 14:47:18,432 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:47:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:47:18 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:47:39,224 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Données chargées avec succès.
2025-05-02 14:47:39,224 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Vérification du format initial.
25/05/02 14:47:41 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:47:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5b7593a7-4e03-439b-b897-244b8f5d1419. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:42 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-5b7593a7-4e03-439b-b897-244b8f5d1419 resolved to file:/tmp/temporary-5b7593a7-4e03-439b-b897-244b8f5d1419.
25/05/02 14:47:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-5b7593a7-4e03-439b-b897-244b8f5d1419/metadata using temp file file:/tmp/temporary-5b7593a7-4e03-439b-b897-244b8f5d1419/.metadata.79f9cb7f-f1a8-4fac-82bf-bcee6afafe9c.tmp
25/05/02 14:47:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-5b7593a7-4e03-439b-b897-244b8f5d1419/.metadata.79f9cb7f-f1a8-4fac-82bf-bcee6afafe9c.tmp to file:/tmp/temporary-5b7593a7-4e03-439b-b897-244b8f5d1419/metadata
25/05/02 14:47:46 INFO MicroBatchExecution: Starting [id = 66ed2f58-bb4f-4887-a403-1b5655430131, runId = f7b37de3-c029-4001-9ddc-1ec11b3611ad]. Use file:/tmp/temporary-5b7593a7-4e03-439b-b897-244b8f5d1419 to store the query checkpoint.
2025-05-02 14:47:46,929 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Données en cours de transformation...
25/05/02 14:47:47 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@751c8d61] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7078d9c2]
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:47 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:47:52,634 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Données parsée avec succès !!!
root
 |-- KSQL_COL_0: string (nullable = true)
 |-- TOTAL_SPENT: double (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:47:53,024 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Vérification des données chargées :
25/05/02 14:47:53 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1eb71d81-5592-4355-ad2d-7cc571321a4a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:53 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-1eb71d81-5592-4355-ad2d-7cc571321a4a resolved to file:/tmp/temporary-1eb71d81-5592-4355-ad2d-7cc571321a4a.
25/05/02 14:47:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1eb71d81-5592-4355-ad2d-7cc571321a4a/metadata using temp file file:/tmp/temporary-1eb71d81-5592-4355-ad2d-7cc571321a4a/.metadata.2719ef8f-5877-4f11-8d14-0f9b5e5a534d.tmp
25/05/02 14:47:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1eb71d81-5592-4355-ad2d-7cc571321a4a/.metadata.2719ef8f-5877-4f11-8d14-0f9b5e5a534d.tmp to file:/tmp/temporary-1eb71d81-5592-4355-ad2d-7cc571321a4a/metadata
25/05/02 14:47:55 INFO MicroBatchExecution: Starting [id = 3f3845be-d2c0-4832-9298-710dc592899f, runId = 9a9bb47c-0f9a-41c9-b057-836204c6f5ee]. Use file:/tmp/temporary-1eb71d81-5592-4355-ad2d-7cc571321a4a to store the query checkpoint.
2025-05-02 14:47:56,024 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:47:56,024 - TOTAL_SPENT_PER_USER_TRANSACTION_TYPE - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:47:56 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@751c8d61] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7078d9c2]
25/05/02 14:47:56 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:56 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:56 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:56 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:56 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ff213c85-9282-4ef8-b86e-2d78d16e1160. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:56 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-ff213c85-9282-4ef8-b86e-2d78d16e1160 resolved to file:/tmp/temporary-ff213c85-9282-4ef8-b86e-2d78d16e1160.
25/05/02 14:47:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:57 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ff213c85-9282-4ef8-b86e-2d78d16e1160/metadata using temp file file:/tmp/temporary-ff213c85-9282-4ef8-b86e-2d78d16e1160/.metadata.2d7c7438-c23e-44e8-a655-284a2646f8fc.tmp
25/05/02 14:47:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:58 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:58 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:58 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:58 INFO AppInfoParser: Kafka startTimeMs: 1746197278024
25/05/02 14:47:58 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:58 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:58 INFO AppInfoParser: Kafka startTimeMs: 1746197278027
25/05/02 14:47:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ff213c85-9282-4ef8-b86e-2d78d16e1160/.metadata.2d7c7438-c23e-44e8-a655-284a2646f8fc.tmp to file:/tmp/temporary-ff213c85-9282-4ef8-b86e-2d78d16e1160/metadata
25/05/02 14:47:59 INFO MicroBatchExecution: Starting [id = 7b73f0d4-db65-45af-9e46-71775d8711b1, runId = 58791358-df1a-46dd-a942-71f3f8d2bec4]. Use file:/tmp/temporary-ff213c85-9282-4ef8-b86e-2d78d16e1160 to store the query checkpoint.
25/05/02 14:47:59 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@751c8d61] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7078d9c2]
25/05/02 14:47:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:59 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:59 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:59 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:59 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:59 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:59 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:59 INFO AppInfoParser: Kafka startTimeMs: 1746197279627
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282724
25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282732
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282732
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-4 unregistered
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283832
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-5 unregistered
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-6 unregistered
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283842
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283842
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-7 unregistered
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-8 unregistered
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-9 unregistered
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:05 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:05 INFO Metrics: Metrics reporters closed
25/05/02 14:48:05 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:05 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:05 INFO Metrics: Metrics reporters closed
25/05/02 14:48:05 ERROR MicroBatchExecution: Query [id = 7b73f0d4-db65-45af-9e46-71775d8711b1, runId = 58791358-df1a-46dd-a942-71f3f8d2bec4] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:05 ERROR MicroBatchExecution: Query [id = 3f3845be-d2c0-4832-9298-710dc592899f, runId = 9a9bb47c-0f9a-41c9-b057-836204c6f5ee] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:05 ERROR MicroBatchExecution: Query [id = 66ed2f58-bb4f-4887-a403-1b5655430131, runId = f7b37de3-c029-4001-9ddc-1ec11b3611ad] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:05 INFO MicroBatchExecution: Async log purge executor pool for query [id = 7b73f0d4-db65-45af-9e46-71775d8711b1, runId = 58791358-df1a-46dd-a942-71f3f8d2bec4] has been shutdown
25/05/02 14:48:05 INFO MicroBatchExecution: Async log purge executor pool for query [id = 3f3845be-d2c0-4832-9298-710dc592899f, runId = 9a9bb47c-0f9a-41c9-b057-836204c6f5ee] has been shutdown
25/05/02 14:48:05 INFO MicroBatchExecution: Async log purge executor pool for query [id = 66ed2f58-bb4f-4887-a403-1b5655430131, runId = f7b37de3-c029-4001-9ddc-1ec11b3611ad] has been shutdown
Traceback (most recent call last):
  File "/app/consumer-table/writter_TOTAL_SPENT_PER_USER_TRANSACTION_TYPE.py", line 104, in <module>
    df_parsed
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 7b73f0d4-db65-45af-9e46-71775d8711b1, runId = 58791358-df1a-46dd-a942-71f3f8d2bec4] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:06 INFO SparkContext: Invoking stop() from shutdown hook
25/05/02 14:48:06 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/05/02 14:48:06 INFO SparkUI: Stopped Spark web UI at http://5e919d8ee3aa:4044
25/05/02 14:48:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/05/02 14:48:06 INFO MemoryStore: MemoryStore cleared
25/05/02 14:48:06 INFO BlockManager: BlockManager stopped
25/05/02 14:48:06 INFO BlockManagerMaster: BlockManagerMaster stopped
25/05/02 14:48:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/05/02 14:48:06 INFO SparkContext: Successfully stopped SparkContext
25/05/02 14:48:06 INFO ShutdownHookManager: Shutdown hook called
25/05/02 14:48:06 INFO ShutdownHookManager: Deleting directory /tmp/temporary-ff213c85-9282-4ef8-b86e-2d78d16e1160
25/05/02 14:48:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-edf0b7fa-2627-492a-9659-6d720c7e0d7a
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-1eb71d81-5592-4355-ad2d-7cc571321a4a
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-5b7593a7-4e03-439b-b897-244b8f5d1419
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11/pyspark-34b75d93-c384-4e28-85b4-c1bc1583e902
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-53a61d67-4af7-44e5-bde7-002ae70f3c11
