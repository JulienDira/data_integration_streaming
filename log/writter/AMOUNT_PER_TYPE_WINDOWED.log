25/05/02 14:32:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:32:50,975 - AMOUNT_PER_TYPE_WINDOWED - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:32:51 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:32:51 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:32:51 INFO SparkContext: Java version 11.0.24
25/05/02 14:32:52 INFO ResourceUtils: ==============================================================
25/05/02 14:32:52 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:32:52 INFO ResourceUtils: ==============================================================
25/05/02 14:32:52 INFO SparkContext: Submitted application: KafkaConsumer_AMOUNT_PER_TYPE_WINDOWED
25/05/02 14:32:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:32:52 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:32:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:32:53 INFO SecurityManager: Changing view acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:32:53 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:32:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:32:56 INFO Utils: Successfully started service 'sparkDriver' on port 33915.
25/05/02 14:32:57 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:32:57 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:32:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:32:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:32:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:32:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d8f1d8a8-0bc2-4923-8fcf-83cdd29a721d
25/05/02 14:32:58 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:32:58 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:33:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/05/02 14:33:01 INFO Utils: Successfully started service 'SparkUI' on port 4041.
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:33915/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371575
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://3109b9eaa053:33915/jars/kafka-clients-3.3.1.jar with timestamp 1746196371575
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://3109b9eaa053:33915/jars/commons-pool2-2.11.1.jar with timestamp 1746196371575
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://3109b9eaa053:33915/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371575
25/05/02 14:33:01 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://3109b9eaa053:33915/jars/postgresql-42.2.23.jar with timestamp 1746196371575
25/05/02 14:33:03 INFO Executor: Starting executor ID driver on host 3109b9eaa053
25/05/02 14:33:03 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:33:03 INFO Executor: Java version 11.0.24
25/05/02 14:33:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:33:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6b6b0e6a for default.
25/05/02 14:33:03 INFO Executor: Fetching spark://3109b9eaa053:33915/jars/kafka-clients-3.3.1.jar with timestamp 1746196371575
25/05/02 14:33:04 INFO TransportClientFactory: Successfully created connection to 3109b9eaa053/172.21.0.7:33915 after 600 ms (0 ms spent in bootstraps)
25/05/02 14:33:04 INFO Utils: Fetching spark://3109b9eaa053:33915/jars/kafka-clients-3.3.1.jar to /tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/fetchFileTemp4091552896739581232.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:33915/jars/commons-pool2-2.11.1.jar with timestamp 1746196371575
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:33915/jars/commons-pool2-2.11.1.jar to /tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/fetchFileTemp5910674078274070990.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:33915/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371575
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:33915/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/fetchFileTemp3462562325390232806.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:33915/jars/postgresql-42.2.23.jar with timestamp 1746196371575
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:33915/jars/postgresql-42.2.23.jar to /tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/fetchFileTemp16561157814705719697.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/postgresql-42.2.23.jar to class loader default
25/05/02 14:33:06 INFO Executor: Fetching spark://3109b9eaa053:33915/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746196371575
25/05/02 14:33:06 INFO Utils: Fetching spark://3109b9eaa053:33915/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/fetchFileTemp2239313571584213221.tmp
25/05/02 14:33:06 INFO Executor: Adding file:/tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/userFiles-ebd8fe1b-34ba-48b1-896d-27a5f63b043a/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:33:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40451.
25/05/02 14:33:06 INFO NettyBlockTransferService: Server created on 3109b9eaa053:40451
25/05/02 14:33:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:33:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3109b9eaa053, 40451, None)
25/05/02 14:33:06 INFO BlockManagerMasterEndpoint: Registering block manager 3109b9eaa053:40451 with 1007.8 MiB RAM, BlockManagerId(driver, 3109b9eaa053, 40451, None)
25/05/02 14:33:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3109b9eaa053, 40451, None)
25/05/02 14:33:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3109b9eaa053, 40451, None)
2025-05-02 14:33:10,963 - AMOUNT_PER_TYPE_WINDOWED - INFO - Session Spark créée.
2025-05-02 14:33:10,963 - AMOUNT_PER_TYPE_WINDOWED - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic AMOUNT_PER_TYPE_WINDOWED.
2025-05-02 14:33:10,963 - AMOUNT_PER_TYPE_WINDOWED - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:33:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:33:11 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:33:30,859 - AMOUNT_PER_TYPE_WINDOWED - INFO - Données chargées avec succès.
2025-05-02 14:33:30,859 - AMOUNT_PER_TYPE_WINDOWED - INFO - Vérification du format initial.
25/05/02 14:33:32 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:33:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:33 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0 resolved to file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0.
25/05/02 14:33:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/metadata using temp file file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/.metadata.c0ed2358-851e-47d4-9c62-2c4baebeaf24.tmp
25/05/02 14:33:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/.metadata.c0ed2358-851e-47d4-9c62-2c4baebeaf24.tmp to file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/metadata
25/05/02 14:33:37 INFO MicroBatchExecution: Starting [id = c16b9dba-dc46-4ee0-9a91-edb19032ec8b, runId = 8342703c-1cad-43fc-be29-c64a9fec4a61]. Use file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0 to store the query checkpoint.
2025-05-02 14:33:37,066 - AMOUNT_PER_TYPE_WINDOWED - INFO - Données en cours de transformation...
25/05/02 14:33:37 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6bbc71ad] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@fbdbd3f]
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:37 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:38 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:38 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:33:43,157 - AMOUNT_PER_TYPE_WINDOWED - INFO - Données parsée avec succès !!!
root
 |-- TRANSACTION_TYPE: string (nullable = true)
 |-- WINDOW_START: long (nullable = true)
 |-- WINDOW_END: long (nullable = true)
 |-- TOTAL_AMOUNT: double (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:33:43,459 - AMOUNT_PER_TYPE_WINDOWED - INFO - Vérification des données chargées :
25/05/02 14:33:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:43 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d resolved to file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d.
25/05/02 14:33:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/metadata using temp file file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/.metadata.227df4e9-dcbb-4e16-862e-bee82bd1eb1a.tmp
25/05/02 14:33:45 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/.metadata.227df4e9-dcbb-4e16-862e-bee82bd1eb1a.tmp to file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/metadata
25/05/02 14:33:46 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:46 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:46 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:46 INFO MicroBatchExecution: Starting [id = f44a6d12-ea9a-4d55-85c2-0a5163d2f787, runId = 499185ca-b6ec-443e-b9c7-a1fc57db855b]. Use file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d to store the query checkpoint.
25/05/02 14:33:46 INFO AppInfoParser: Kafka startTimeMs: 1746196426562
2025-05-02 14:33:47,059 - AMOUNT_PER_TYPE_WINDOWED - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:33:47,059 - AMOUNT_PER_TYPE_WINDOWED - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:33:47 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6bbc71ad] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@fbdbd3f]
25/05/02 14:33:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:47 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:47 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:33:47 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de resolved to file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de.
25/05/02 14:33:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:33:48 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:48 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:48 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:48 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:48 INFO AppInfoParser: Kafka startTimeMs: 1746196428262
25/05/02 14:33:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/metadata using temp file file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/.metadata.0710f288-4db4-4a3e-850d-9b2588ce8ac2.tmp
25/05/02 14:33:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/.metadata.0710f288-4db4-4a3e-850d-9b2588ce8ac2.tmp to file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/metadata
25/05/02 14:33:50 INFO MicroBatchExecution: Starting [id = afe05855-a91c-4cb3-b577-88ea13d00b68, runId = 6f2ddd78-9aa8-40a9-836d-b362fd00111b]. Use file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de to store the query checkpoint.
25/05/02 14:33:50 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6bbc71ad] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@fbdbd3f]
25/05/02 14:33:50 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:50 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:33:50 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:33:50 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:33:50 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:33:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:33:50 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:33:50 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:33:50 INFO AppInfoParser: Kafka startTimeMs: 1746196430857
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/sources/0/0 using temp file file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/sources/0/.0.25ea0127-895e-40c6-9b7e-afc8457e0460.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/sources/0/0 using temp file file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/sources/0/.0.73e4df22-5db3-409e-a004-45877047e983.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/sources/0/0 using temp file file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/sources/0/.0.6d1b4656-0c4e-47d9-a54b-a1cd92db9e90.tmp
25/05/02 14:33:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/sources/0/.0.25ea0127-895e-40c6-9b7e-afc8457e0460.tmp to file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/sources/0/0
25/05/02 14:33:52 INFO KafkaMicroBatchStream: Initial offsets: {"AMOUNT_PER_TYPE_WINDOWED":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/sources/0/.0.73e4df22-5db3-409e-a004-45877047e983.tmp to file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/sources/0/0
25/05/02 14:33:53 INFO KafkaMicroBatchStream: Initial offsets: {"AMOUNT_PER_TYPE_WINDOWED":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/sources/0/.0.6d1b4656-0c4e-47d9-a54b-a1cd92db9e90.tmp to file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/sources/0/0
25/05/02 14:33:53 INFO KafkaMicroBatchStream: Initial offsets: {"AMOUNT_PER_TYPE_WINDOWED":{"0":0}}
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/offsets/0 using temp file file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/offsets/.0.198a4bf0-e452-4043-a6ea-e88accefe85a.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/offsets/0 using temp file file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/offsets/.0.70dd41d3-bf0f-447c-b5ae-254473db59aa.tmp
25/05/02 14:33:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/offsets/0 using temp file file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/offsets/.0.cfc039b2-9fb5-44fd-9413-42a4715ee63d.tmp
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/offsets/.0.70dd41d3-bf0f-447c-b5ae-254473db59aa.tmp to file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/offsets/0
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/offsets/.0.198a4bf0-e452-4043-a6ea-e88accefe85a.tmp to file:/tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433259,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433066,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/offsets/.0.cfc039b2-9fb5-44fd-9413-42a4715ee63d.tmp to file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/offsets/0
25/05/02 14:33:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746196433363,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/02 14:33:59 INFO IncrementalExecution: Current batch timestamp = 1746196433363
25/05/02 14:33:59 INFO IncrementalExecution: Current batch timestamp = 1746196433066
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO IncrementalExecution: Current batch timestamp = 1746196433363
25/05/02 14:34:03 INFO IncrementalExecution: Current batch timestamp = 1746196433066
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:04 INFO IncrementalExecution: Current batch timestamp = 1746196433363
25/05/02 14:34:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/02 14:34:10 INFO CodeGenerator: Code generated in 4297.583315 ms
25/05/02 14:34:11 INFO CodeGenerator: Code generated in 4297.091826 ms
25/05/02 14:34:11 INFO CodeGenerator: Code generated in 4595.277971 ms
25/05/02 14:34:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
25/05/02 14:34:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]]. The input RDD has 1 partitions.
25/05/02 14:34:12 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:12 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:13 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:13 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/05/02 14:34:13 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:13 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.6 KiB, free 1007.8 MiB)
25/05/02 14:34:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 1007.8 MiB)
25/05/02 14:34:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3109b9eaa053:40451 (size: 4.5 KiB, free: 1007.8 MiB)
25/05/02 14:34:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/05/02 14:34:19 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:19 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
25/05/02 14:34:19 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:19 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:19 INFO CodeGenerator: Code generated in 298.848909 ms
25/05/02 14:34:20 INFO SparkContext: Starting job: start at <unknown>:0
25/05/02 14:34:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9073 bytes) 
25/05/02 14:34:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 23.2 KiB, free 1007.8 MiB)
25/05/02 14:34:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 1007.8 MiB)
25/05/02 14:34:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3109b9eaa053:40451 (size: 10.5 KiB, free: 1007.8 MiB)
25/05/02 14:34:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/05/02 14:34:21 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
25/05/02 14:34:21 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
25/05/02 14:34:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/05/02 14:34:21 INFO DAGScheduler: Parents of final stage: List()
25/05/02 14:34:21 INFO DAGScheduler: Missing parents: List()
25/05/02 14:34:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at start at <unknown>:0), which has no missing parents
25/05/02 14:34:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.2 KiB, free 1007.8 MiB)
25/05/02 14:34:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 1007.8 MiB)
25/05/02 14:34:24 INFO CodeGenerator: Code generated in 295.155383 ms
25/05/02 14:34:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3109b9eaa053:40451 (size: 16.5 KiB, free: 1007.8 MiB)
25/05/02 14:34:24 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=AMOUNT_PER_TYPE_WINDOWED-0 fromOffset=0 untilOffset=230, for query queryId=c16b9dba-dc46-4ee0-9a91-edb19032ec8b batchId=0 taskId=0 partitionId=0
25/05/02 14:34:25 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
25/05/02 14:34:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/02 14:34:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
25/05/02 14:34:25 INFO CodeGenerator: Code generated in 197.689427 ms
25/05/02 14:34:25 INFO CodeGenerator: Code generated in 98.82633 ms
25/05/02 14:34:26 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:26 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:26 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:26 INFO AppInfoParser: Kafka startTimeMs: 1746196466462
25/05/02 14:34:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Assigned to partition(s): AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Seeking to offset 0 for partition AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:27 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Resetting the last seen epoch of partition AMOUNT_PER_TYPE_WINDOWED-0 to 0 since the associated topicId changed from null to 6lgI3xfHQ0aVaZ4DQhWU3A
25/05/02 14:34:27 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Seeking to earliest offset of partition AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Resetting offset for partition AMOUNT_PER_TYPE_WINDOWED-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Seeking to latest offset of partition AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Resetting offset for partition AMOUNT_PER_TYPE_WINDOWED-0 to position FetchPosition{offset=230, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:29 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:29 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/05/02 14:34:29 INFO KafkaDataConsumer: From Kafka topicPartition=AMOUNT_PER_TYPE_WINDOWED-0 groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor read 230 records through 1 polls (polled  out 230 records), taking 1602444283 nanos, during time span of 3300597274 nanos.
25/05/02 14:34:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 36817 bytes result sent to driver
25/05/02 14:34:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9074 bytes) 
25/05/02 14:34:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11100 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
25/05/02 14:34:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/05/02 14:34:31 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 17.005 s
25/05/02 14:34:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:31 INFO CodeGenerator: Code generated in 9.197688 ms
25/05/02 14:34:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/05/02 14:34:31 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 18.803209 s
25/05/02 14:34:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/02 14:34:31 INFO CodeGenerator: Code generated in 205.31309 ms
25/05/02 14:34:31 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=AMOUNT_PER_TYPE_WINDOWED-0 fromOffset=0 untilOffset=230, for query queryId=f44a6d12-ea9a-4d55-85c2-0a5163d2f787 batchId=0 taskId=1 partitionId=0
25/05/02 14:34:32 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:32 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:32 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:32 INFO AppInfoParser: Kafka startTimeMs: 1746196472461
25/05/02 14:34:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Assigned to partition(s): AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Seeking to offset 0 for partition AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:32 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Resetting the last seen epoch of partition AMOUNT_PER_TYPE_WINDOWED-0 to 0 since the associated topicId changed from null to 6lgI3xfHQ0aVaZ4DQhWU3A
25/05/02 14:34:32 INFO CodeGenerator: Code generated in 98.733045 ms
25/05/02 14:34:32 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Seeking to earliest offset of partition AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Resetting offset for partition AMOUNT_PER_TYPE_WINDOWED-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Seeking to latest offset of partition AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Resetting offset for partition AMOUNT_PER_TYPE_WINDOWED-0 to position FetchPosition{offset=230, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/02 14:34:35 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
25/05/02 14:34:35 INFO KafkaDataConsumer: From Kafka topicPartition=AMOUNT_PER_TYPE_WINDOWED-0 groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor read 230 records through 1 polls (polled  out 230 records), taking 1098890225 nanos, during time span of 2700106221 nanos.
25/05/02 14:34:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 22904 bytes result sent to driver
25/05/02 14:34:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (3109b9eaa053, executor driver, partition 0, PROCESS_LOCAL, 9073 bytes) 
25/05/02 14:34:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
25/05/02 14:34:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4896 ms on 3109b9eaa053 (executor driver) (1/1)
25/05/02 14:34:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/05/02 14:34:35 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 15.995 s
25/05/02 14:34:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/02 14:34:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/05/02 14:34:35 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 22.703131 s
25/05/02 14:34:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/02 14:34:36 INFO CodeGenerator: Code generated in 206.182556 ms
25/05/02 14:34:37 INFO CodeGenerator: Code generated in 103.899655 ms
25/05/02 14:34:37 INFO CodeGenerator: Code generated in 199.88755 ms
25/05/02 14:34:47 INFO CodeGenerator: Code generated in 104.726974 ms
25/05/02 14:34:47 INFO CodeGenerator: Code generated in 397.274897 ms
25/05/02 14:34:47 INFO CodeGenerator: Code generated in 401.324774 ms
25/05/02 14:34:47 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=AMOUNT_PER_TYPE_WINDOWED-0 fromOffset=0 untilOffset=230, for query queryId=afe05855-a91c-4cb3-b577-88ea13d00b68 batchId=0 taskId=2 partitionId=0
+------------------+-------------+-------------+------------------+-----------------------+--------------+
|TRANSACTION_TYPE  |WINDOW_START |WINDOW_END   |TOTAL_AMOUNT      |ingestion_time         |ingestion_date|
+------------------+-------------+-------------+------------------+-----------------------+--------------+
|withdrawal  ��IC�|1746194220000|1746194520000|489.97499999999997|2025-05-02 14:33:53.363|2025-05-02    |
|withdrawal  ��J.@|1746194280000|1746194580000|489.97499999999997|2025-05-02 14:33:53.363|2025-05-02    |
|withdrawal  ��K�|1746194340000|1746194640000|489.97499999999997|2025-05-02 14:33:53.363|2025-05-02    |
|withdrawal  ��L |1746194400000|1746194700000|489.97499999999997|2025-05-02 14:33:53.363|2025-05-02    |
|withdrawal  ��L�`|1746194460000|1746194760000|489.97499999999997|2025-05-02 14:33:53.363|2025-05-02    |
|purchase  ��IC�  |1746194220000|1746194520000|5.59153           |2025-05-02 14:33:53.363|2025-05-02    |
|purchase  ��J.@  |1746194280000|1746194580000|5.59153           |2025-05-02 14:33:53.363|2025-05-02    |
|purchase  ��K�  |1746194340000|1746194640000|5.59153           |2025-05-02 14:33:53.363|2025-05-02    |
|purchase  ��L   |1746194400000|1746194700000|5.59153           |2025-05-02 14:33:53.363|2025-05-02    |
|purchase  ��L�`  |1746194460000|1746194760000|5.59153           |2025-05-02 14:33:53.363|2025-05-02    |
+------------------+-------------+-------------+------------------+-----------------------+--------------+
only showing top 10 rows

+------------------+--------------------+
|               key|               value|
+------------------+--------------------+
|withdrawal  ��IC�|{"WINDOW_START":1...|
|withdrawal  ��J.@|{"WINDOW_START":1...|
|withdrawal  ��K�|{"WINDOW_START":1...|
|withdrawal  ��L |{"WINDOW_START":1...|
|withdrawal  ��L�`|{"WINDOW_START":1...|
|  purchase  ��IC�|{"WINDOW_START":1...|
|  purchase  ��J.@|{"WINDOW_START":1...|
|  purchase  ��K�|{"WINDOW_START":1...|
|  purchase  ��L |{"WINDOW_START":1...|
|  purchase  ��L�`|{"WINDOW_START":1...|
|   payment  ��IC�|{"WINDOW_START":1...|
|   payment  ��J.@|{"WINDOW_START":1...|
|   payment  ��K�|{"WINDOW_START":1...|
|   payment  ��L |{"WINDOW_START":1...|
|   payment  ��L�`|{"WINDOW_START":1...|
|  purchase  ��IC�|{"WINDOW_START":1...|
|  purchase  ��J.@|{"WINDOW_START":1...|
|  purchase  ��K�|{"WINDOW_START":1...|
|  purchase  ��L |{"WINDOW_START":1...|
|  purchase  ��L�`|{"WINDOW_START":1...|
+------------------+--------------------+
only showing top 20 rows

25/05/02 14:34:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] committed.
25/05/02 14:34:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
25/05/02 14:34:47 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/02 14:34:48 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:34:48 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:34:48 INFO AppInfoParser: Kafka startTimeMs: 1746196488062
25/05/02 14:34:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Assigned to partition(s): AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Seeking to offset 0 for partition AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:48 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Resetting the last seen epoch of partition AMOUNT_PER_TYPE_WINDOWED-0 to 0 since the associated topicId changed from null to 6lgI3xfHQ0aVaZ4DQhWU3A
25/05/02 14:34:48 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Cluster ID: JSo_1ZeNTYOGv55PhJj_2g
25/05/02 14:34:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/commits/0 using temp file file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/commits/.0.477d6a8e-7042-418b-b79b-12b37e4e696d.tmp
25/05/02 14:34:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/commits/0 using temp file file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/commits/.0.2738f117-880a-4316-b9fc-dc2557cc6a5c.tmp
25/05/02 14:34:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Seeking to earliest offset of partition AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Resetting offset for partition AMOUNT_PER_TYPE_WINDOWED-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Seeking to latest offset of partition AMOUNT_PER_TYPE_WINDOWED-0
25/05/02 14:34:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Resetting offset for partition AMOUNT_PER_TYPE_WINDOWED-0 to position FetchPosition{offset=230, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/02 14:34:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/commits/.0.2738f117-880a-4316-b9fc-dc2557cc6a5c.tmp to file:/tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0/commits/0
25/05/02 14:34:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/commits/.0.477d6a8e-7042-418b-b79b-12b37e4e696d.tmp to file:/tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d/commits/0
25/05/02 14:34:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "f44a6d12-ea9a-4d55-85c2-0a5163d2f787",
  "runId" : "499185ca-b6ec-443e-b9c7-a1fc57db855b",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:47.160Z",
  "batchId" : 0,
  "numInputRows" : 230,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 3.6564233820326537,
  "durationMs" : {
    "addBatch" : 45197,
    "commitOffsets" : 2006,
    "getBatch" : 198,
    "latestOffset" : 5803,
    "queryPlanning" : 7905,
    "triggerExecution" : 62903,
    "walCommit" : 1000
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[AMOUNT_PER_TYPE_WINDOWED]]",
    "startOffset" : null,
    "endOffset" : {
      "AMOUNT_PER_TYPE_WINDOWED" : {
        "0" : 230
      }
    },
    "latestOffset" : {
      "AMOUNT_PER_TYPE_WINDOWED" : {
        "0" : 230
      }
    },
    "numInputRows" : 230,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 3.6564233820326537,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@3bd13244",
    "numOutputRows" : 230
  }
}
25/05/02 14:34:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c16b9dba-dc46-4ee0-9a91-edb19032ec8b",
  "runId" : "8342703c-1cad-43fc-be29-c64a9fec4a61",
  "name" : null,
  "timestamp" : "2025-05-02T14:33:37.864Z",
  "batchId" : 0,
  "numInputRows" : 230,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 3.194533181477263,
  "durationMs" : {
    "addBatch" : 45198,
    "commitOffsets" : 1806,
    "getBatch" : 204,
    "latestOffset" : 15099,
    "queryPlanning" : 8191,
    "triggerExecution" : 71997,
    "walCommit" : 1004
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[AMOUNT_PER_TYPE_WINDOWED]]",
    "startOffset" : null,
    "endOffset" : {
      "AMOUNT_PER_TYPE_WINDOWED" : {
        "0" : 230
      }
    },
    "latestOffset" : {
      "AMOUNT_PER_TYPE_WINDOWED" : {
        "0" : 230
      }
    },
    "numInputRows" : 230,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 3.194533181477263,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@3bd13244",
    "numOutputRows" : 230
  }
}
25/05/02 14:34:52 INFO KafkaDataConsumer: From Kafka topicPartition=AMOUNT_PER_TYPE_WINDOWED-0 groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor read 230 records through 1 polls (polled  out 230 records), taking 1100138164 nanos, during time span of 4203932804 nanos.
25/05/02 14:34:52 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)
java.sql.BatchUpdateException: Batch entry 0 INSERT INTO AMOUNT_PER_TYPE_WINDOWED ("TRANSACTION_TYPE","WINDOW_START","WINDOW_END","TOTAL_AMOUNT","ingestion_time","ingestion_date") VALUES ('withdrawalwithdrawal  ��IC�',1746194220000,1746194520000,489.97499999999997,'2025-05-02 14:33:53.066+00'::timestamp,'2025-05-02 +00'::date) was aborted: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:520)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2552)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2284)
	... 21 more
25/05/02 14:34:53 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (3109b9eaa053 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO AMOUNT_PER_TYPE_WINDOWED ("TRANSACTION_TYPE","WINDOW_START","WINDOW_END","TOTAL_AMOUNT","ingestion_time","ingestion_date") VALUES ('withdrawalwithdrawal  ��IC�',1746194220000,1746194520000,489.97499999999997,'2025-05-02 14:33:53.066+00'::timestamp,'2025-05-02 +00'::date) was aborted: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:520)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2552)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2284)
	... 21 more

25/05/02 14:34:53 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
25/05/02 14:34:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/05/02 14:34:53 INFO TaskSchedulerImpl: Cancelling stage 2
25/05/02 14:34:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (3109b9eaa053 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO AMOUNT_PER_TYPE_WINDOWED ("TRANSACTION_TYPE","WINDOW_START","WINDOW_END","TOTAL_AMOUNT","ingestion_time","ingestion_date") VALUES ('withdrawalwithdrawal  ��IC�',1746194220000,1746194520000,489.97499999999997,'2025-05-02 14:33:53.066+00'::timestamp,'2025-05-02 +00'::date) was aborted: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:520)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2552)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2284)
	... 21 more

Driver stacktrace:
25/05/02 14:34:53 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) failed in 31.401 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (3109b9eaa053 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO AMOUNT_PER_TYPE_WINDOWED ("TRANSACTION_TYPE","WINDOW_START","WINDOW_END","TOTAL_AMOUNT","ingestion_time","ingestion_date") VALUES ('withdrawalwithdrawal  ��IC�',1746194220000,1746194520000,489.97499999999997,'2025-05-02 14:33:53.066+00'::timestamp,'2025-05-02 +00'::date) was aborted: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:520)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2552)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2284)
	... 21 more

Driver stacktrace:
25/05/02 14:34:53 INFO DAGScheduler: Job 2 failed: start at <unknown>:0, took 32.601223 s
25/05/02 14:34:58 ERROR MicroBatchExecution: Query [id = afe05855-a91c-4cb3-b577-88ea13d00b68, runId = 6f2ddd78-9aa8-40a9-836d-b362fd00111b] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/consumer-table/writter_AMOUNT_PER_TYPE_WINDOWED.py", line 109, in <lambda>
    lambda df, epoch_id: df.write
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o99.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (3109b9eaa053 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO AMOUNT_PER_TYPE_WINDOWED ("TRANSACTION_TYPE","WINDOW_START","WINDOW_END","TOTAL_AMOUNT","ingestion_time","ingestion_date") VALUES ('withdrawalwithdrawal  ��IC�',1746194220000,1746194520000,489.97499999999997,'2025-05-02 14:33:53.066+00'::timestamp,'2025-05-02 +00'::date) was aborted: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:520)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2552)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2284)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:60)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy32.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO AMOUNT_PER_TYPE_WINDOWED ("TRANSACTION_TYPE","WINDOW_START","WINDOW_END","TOTAL_AMOUNT","ingestion_time","ingestion_date") VALUES ('withdrawalwithdrawal  ��IC�',1746194220000,1746194520000,489.97499999999997,'2025-05-02 14:33:53.066+00'::timestamp,'2025-05-02 +00'::date) was aborted: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:520)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2552)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2284)
	... 21 more


	at py4j.Protocol.getReturnValue(Protocol.java:476)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
	at com.sun.proxy.$Proxy32.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
25/05/02 14:34:58 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
25/05/02 14:34:58 INFO Metrics: Metrics scheduler closed
25/05/02 14:34:58 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:34:58 INFO Metrics: Metrics reporters closed
25/05/02 14:34:58 INFO MicroBatchExecution: Async log purge executor pool for query [id = afe05855-a91c-4cb3-b577-88ea13d00b68, runId = 6f2ddd78-9aa8-40a9-836d-b362fd00111b] has been shutdown
Traceback (most recent call last):
  File "/app/consumer-table/writter_AMOUNT_PER_TYPE_WINDOWED.py", line 106, in <module>
25/05/02 14:35:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
    df_parsed
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException25/05/02 14:35:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
: [STREAM_FAILED] Query [id = afe05855-a91c-4cb3-b577-88ea13d00b68, runId = 6f2ddd78-9aa8-40a9-836d-b362fd00111b] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/consumer-table/writter_AMOUNT_PER_TYPE_WINDOWED.py", line 109, in <lambda>
    lambda df, epoch_id: df.write
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o99.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (3109b9eaa053 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO AMOUNT_PER_TYPE_WINDOWED ("TRANSACTION_TYPE","WINDOW_START","WINDOW_END","TOTAL_AMOUNT","ingestion_time","ingestion_date") VALUES ('withdrawalwithdrawal  ��IC�',1746194220000,1746194520000,489.97499999999997,'2025-05-02 14:33:53.066+00'::timestamp,'2025-05-02 +00'::date) was aborted: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:520)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2552)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2284)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:60)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy32.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO AMOUNT_PER_TYPE_WINDOWED ("TRANSACTION_TYPE","WINDOW_START","WINDOW_END","TOTAL_AMOUNT","ingestion_time","ingestion_date") VALUES ('withdrawalwithdrawal  ��IC�',1746194220000,1746194520000,489.97499999999997,'2025-05-02 14:33:53.066+00'::timestamp,'2025-05-02 +00'::date) was aborted: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:520)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:901)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.postgresql.util.PSQLException: ERROR: invalid byte sequence for encoding "UTF8": 0x00
  Where: unnamed portal parameter $1
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2552)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2284)
	... 21 more


25/05/02 14:35:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
25/05/02 14:35:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2, groupId=spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor] Request joining group due to: consumer pro-actively leaving the group
25/05/02 14:35:01 INFO Metrics: Metrics scheduler closed
25/05/02 14:35:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:35:01 INFO Metrics: Metrics reporters closed
25/05/02 14:35:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-f20203ab-fd8e-48b1-883a-b6f29d4fec3b--1017915681-executor-2 unregistered
25/05/02 14:35:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
25/05/02 14:35:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1, groupId=spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor] Request joining group due to: consumer pro-actively leaving the group
25/05/02 14:35:01 INFO Metrics: Metrics scheduler closed
25/05/02 14:35:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:35:01 INFO Metrics: Metrics reporters closed
25/05/02 14:35:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-540e640a-3cea-4516-9748-e1678d668042-1467497652-executor-1 unregistered
25/05/02 14:35:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
25/05/02 14:35:01 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3, groupId=spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor] Request joining group due to: consumer pro-actively leaving the group
25/05/02 14:35:01 INFO Metrics: Metrics scheduler closed
25/05/02 14:35:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:35:01 INFO Metrics: Metrics reporters closed
25/05/02 14:35:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ca44df7f-1bb1-4db6-ad8b-cb36320fae49--245855602-executor-3 unregistered
25/05/02 14:35:01 INFO SparkContext: Invoking stop() from shutdown hook
25/05/02 14:35:01 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/05/02 14:35:01 INFO SparkUI: Stopped Spark web UI at http://3109b9eaa053:4041
25/05/02 14:35:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/05/02 14:35:02 INFO MemoryStore: MemoryStore cleared
25/05/02 14:35:02 INFO BlockManager: BlockManager stopped
25/05/02 14:35:02 INFO BlockManagerMaster: BlockManagerMaster stopped
25/05/02 14:35:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/05/02 14:35:02 INFO SparkContext: Successfully stopped SparkContext
25/05/02 14:35:02 INFO ShutdownHookManager: Shutdown hook called
25/05/02 14:35:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4/pyspark-5505f453-311c-4483-adb1-ec0165c6e204
25/05/02 14:35:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-13e34d96-f44f-4574-b057-698a61c8b0b4
25/05/02 14:35:02 INFO ShutdownHookManager: Deleting directory /tmp/temporary-a924db2d-58e3-4de5-a50e-313c5a6b19de
25/05/02 14:35:02 INFO ShutdownHookManager: Deleting directory /tmp/temporary-78469475-6c26-42cd-b2b3-d7caa4734ee0
25/05/02 14:35:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-79e57d1d-8dd2-4130-939a-972f45ce2985
25/05/02 14:35:02 INFO ShutdownHookManager: Deleting directory /tmp/temporary-f6fa0752-8701-4008-ad9e-aed219ca891d
25/05/02 14:46:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-02 14:46:53,339 - AMOUNT_PER_TYPE_WINDOWED - INFO - Lancement de l'application Spark Streaming...
25/05/02 14:46:54 INFO SparkContext: Running Spark version 3.5.0
25/05/02 14:46:54 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:46:54 INFO SparkContext: Java version 11.0.24
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/02 14:46:55 INFO ResourceUtils: ==============================================================
25/05/02 14:46:55 INFO SparkContext: Submitted application: KafkaConsumer_AMOUNT_PER_TYPE_WINDOWED
25/05/02 14:46:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/02 14:46:56 INFO ResourceProfile: Limiting resource is cpu
25/05/02 14:46:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/02 14:46:56 INFO SecurityManager: Changing view acls to: root
25/05/02 14:46:56 INFO SecurityManager: Changing modify acls to: root
25/05/02 14:46:56 INFO SecurityManager: Changing view acls groups to: 
25/05/02 14:46:56 INFO SecurityManager: Changing modify acls groups to: 
25/05/02 14:46:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/02 14:47:01 INFO Utils: Successfully started service 'sparkDriver' on port 42999.
25/05/02 14:47:01 INFO SparkEnv: Registering MapOutputTracker
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMaster
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/02 14:47:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/02 14:47:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/02 14:47:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c254e58d-ca7e-4d8d-91d7-9773976dd15f
25/05/02 14:47:03 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB
25/05/02 14:47:03 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/02 14:47:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/05/02 14:47:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/05/02 14:47:07 INFO Utils: Successfully started service 'SparkUI' on port 4043.
25/05/02 14:47:07 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:42999/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214231
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://5e919d8ee3aa:42999/jars/kafka-clients-3.3.1.jar with timestamp 1746197214231
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://5e919d8ee3aa:42999/jars/commons-pool2-2.11.1.jar with timestamp 1746197214231
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://5e919d8ee3aa:42999/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214231
25/05/02 14:47:08 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://5e919d8ee3aa:42999/jars/postgresql-42.2.23.jar with timestamp 1746197214231
25/05/02 14:47:09 INFO Executor: Starting executor ID driver on host 5e919d8ee3aa
25/05/02 14:47:09 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/02 14:47:09 INFO Executor: Java version 11.0.24
25/05/02 14:47:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/02 14:47:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@30ffdedd for default.
25/05/02 14:47:09 INFO Executor: Fetching spark://5e919d8ee3aa:42999/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214231
25/05/02 14:47:11 INFO TransportClientFactory: Successfully created connection to 5e919d8ee3aa/172.21.0.2:42999 after 595 ms (0 ms spent in bootstraps)
25/05/02 14:47:11 INFO Utils: Fetching spark://5e919d8ee3aa:42999/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/fetchFileTemp5010117484837442345.tmp
25/05/02 14:47:11 INFO Executor: Adding file:/tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:11 INFO Executor: Fetching spark://5e919d8ee3aa:42999/jars/postgresql-42.2.23.jar with timestamp 1746197214231
25/05/02 14:47:11 INFO Utils: Fetching spark://5e919d8ee3aa:42999/jars/postgresql-42.2.23.jar to /tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/fetchFileTemp1456179862433380587.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/postgresql-42.2.23.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:42999/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746197214231
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:42999/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/fetchFileTemp13667032109695324270.tmp
25/05/02 14:47:12 INFO Executor: Adding file:/tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/02 14:47:12 INFO Executor: Fetching spark://5e919d8ee3aa:42999/jars/kafka-clients-3.3.1.jar with timestamp 1746197214231
25/05/02 14:47:12 INFO Utils: Fetching spark://5e919d8ee3aa:42999/jars/kafka-clients-3.3.1.jar to /tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/fetchFileTemp11900285156586203780.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/kafka-clients-3.3.1.jar to class loader default
25/05/02 14:47:13 INFO Executor: Fetching spark://5e919d8ee3aa:42999/jars/commons-pool2-2.11.1.jar with timestamp 1746197214231
25/05/02 14:47:13 INFO Utils: Fetching spark://5e919d8ee3aa:42999/jars/commons-pool2-2.11.1.jar to /tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/fetchFileTemp10381075794505133957.tmp
25/05/02 14:47:13 INFO Executor: Adding file:/tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/userFiles-faf81530-031a-475a-8fff-e7b96c484922/commons-pool2-2.11.1.jar to class loader default
25/05/02 14:47:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34311.
25/05/02 14:47:13 INFO NettyBlockTransferService: Server created on 5e919d8ee3aa:34311
25/05/02 14:47:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/02 14:47:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5e919d8ee3aa, 34311, None)
25/05/02 14:47:13 INFO BlockManagerMasterEndpoint: Registering block manager 5e919d8ee3aa:34311 with 1007.8 MiB RAM, BlockManagerId(driver, 5e919d8ee3aa, 34311, None)
25/05/02 14:47:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5e919d8ee3aa, 34311, None)
25/05/02 14:47:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5e919d8ee3aa, 34311, None)
2025-05-02 14:47:18,228 - AMOUNT_PER_TYPE_WINDOWED - INFO - Session Spark créée.
2025-05-02 14:47:18,229 - AMOUNT_PER_TYPE_WINDOWED - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic AMOUNT_PER_TYPE_WINDOWED.
2025-05-02 14:47:18,229 - AMOUNT_PER_TYPE_WINDOWED - INFO - Schémas définis pour la clé et la valeur.
25/05/02 14:47:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/02 14:47:18 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-02 14:47:39,224 - AMOUNT_PER_TYPE_WINDOWED - INFO - Données chargées avec succès.
2025-05-02 14:47:39,225 - AMOUNT_PER_TYPE_WINDOWED - INFO - Vérification du format initial.
25/05/02 14:47:41 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/02 14:47:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d3a25bcb-3fc0-4ef9-8505-62642d959808. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:42 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-d3a25bcb-3fc0-4ef9-8505-62642d959808 resolved to file:/tmp/temporary-d3a25bcb-3fc0-4ef9-8505-62642d959808.
25/05/02 14:47:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d3a25bcb-3fc0-4ef9-8505-62642d959808/metadata using temp file file:/tmp/temporary-d3a25bcb-3fc0-4ef9-8505-62642d959808/.metadata.9215de3c-0c1c-4183-a054-a6df7d1a1028.tmp
25/05/02 14:47:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d3a25bcb-3fc0-4ef9-8505-62642d959808/.metadata.9215de3c-0c1c-4183-a054-a6df7d1a1028.tmp to file:/tmp/temporary-d3a25bcb-3fc0-4ef9-8505-62642d959808/metadata
25/05/02 14:47:46 INFO MicroBatchExecution: Starting [id = 68845fb3-58ef-4a25-a8a1-f6808463fed5, runId = c887aa5b-8157-460a-9ad1-2ca8838adc1d]. Use file:/tmp/temporary-d3a25bcb-3fc0-4ef9-8505-62642d959808 to store the query checkpoint.
2025-05-02 14:47:46,927 - AMOUNT_PER_TYPE_WINDOWED - INFO - Données en cours de transformation...
25/05/02 14:47:47 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@565bce74] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@48c4ed3]
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:47 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:47 INFO MicroBatchExecution: Stream started from {}
2025-05-02 14:47:53,129 - AMOUNT_PER_TYPE_WINDOWED - INFO - Données parsée avec succès !!!
root
 |-- TRANSACTION_TYPE: string (nullable = true)
 |-- WINDOW_START: long (nullable = true)
 |-- WINDOW_END: long (nullable = true)
 |-- TOTAL_AMOUNT: double (nullable = true)
 |-- ingestion_time: timestamp (nullable = false)
 |-- ingestion_date: date (nullable = false)

2025-05-02 14:47:53,133 - AMOUNT_PER_TYPE_WINDOWED - INFO - Vérification des données chargées :
25/05/02 14:47:53 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ba1574a7-fe81-4ee4-969e-60ec79ae7475. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:54 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-ba1574a7-fe81-4ee4-969e-60ec79ae7475 resolved to file:/tmp/temporary-ba1574a7-fe81-4ee4-969e-60ec79ae7475.
25/05/02 14:47:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-ba1574a7-fe81-4ee4-969e-60ec79ae7475/metadata using temp file file:/tmp/temporary-ba1574a7-fe81-4ee4-969e-60ec79ae7475/.metadata.b91404de-74dc-458e-8a67-4b6ab16fe50f.tmp
25/05/02 14:47:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-ba1574a7-fe81-4ee4-969e-60ec79ae7475/.metadata.b91404de-74dc-458e-8a67-4b6ab16fe50f.tmp to file:/tmp/temporary-ba1574a7-fe81-4ee4-969e-60ec79ae7475/metadata
25/05/02 14:47:55 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:56 INFO MicroBatchExecution: Starting [id = 91685035-fc0f-4afb-ad9c-f63b33760a23, runId = a449c786-32c5-4399-a121-05647d7aaa13]. Use file:/tmp/temporary-ba1574a7-fe81-4ee4-969e-60ec79ae7475 to store the query checkpoint.
2025-05-02 14:47:56,526 - AMOUNT_PER_TYPE_WINDOWED - INFO - Démarrage de l'écriture en console pour le debug.
2025-05-02 14:47:56,526 - AMOUNT_PER_TYPE_WINDOWED - INFO - Initialisation de l'écriture vers postgresql...
25/05/02 14:47:56 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@565bce74] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@48c4ed3]
25/05/02 14:47:56 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:56 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:56 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:56 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:56 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:56 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:56 INFO AppInfoParser: Kafka startTimeMs: 1746197276824
25/05/02 14:47:57 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:57 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:57 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:57 INFO AppInfoParser: Kafka startTimeMs: 1746197277230
25/05/02 14:47:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3fe815fd-5fe7-40b5-b7d2-1a463b846e5c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/02 14:47:57 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-3fe815fd-5fe7-40b5-b7d2-1a463b846e5c resolved to file:/tmp/temporary-3fe815fd-5fe7-40b5-b7d2-1a463b846e5c.
25/05/02 14:47:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/02 14:47:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3fe815fd-5fe7-40b5-b7d2-1a463b846e5c/metadata using temp file file:/tmp/temporary-3fe815fd-5fe7-40b5-b7d2-1a463b846e5c/.metadata.7c6b9ff1-9238-40bf-a74a-b7115d7d3d5a.tmp
25/05/02 14:47:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3fe815fd-5fe7-40b5-b7d2-1a463b846e5c/.metadata.7c6b9ff1-9238-40bf-a74a-b7115d7d3d5a.tmp to file:/tmp/temporary-3fe815fd-5fe7-40b5-b7d2-1a463b846e5c/metadata
25/05/02 14:47:59 INFO MicroBatchExecution: Starting [id = 4cefe596-4fec-4810-98f7-853ebe0ea48b, runId = a31acf14-a67a-4ec0-a8aa-fccd8df2da54]. Use file:/tmp/temporary-3fe815fd-5fe7-40b5-b7d2-1a463b846e5c to store the query checkpoint.
25/05/02 14:47:59 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@565bce74] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@48c4ed3]
25/05/02 14:47:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:59 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/02 14:47:59 INFO MicroBatchExecution: Starting new streaming query.
25/05/02 14:47:59 INFO MicroBatchExecution: Stream started from {}
25/05/02 14:47:59 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:47:59 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:47:59 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:47:59 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:47:59 INFO AppInfoParser: Kafka startTimeMs: 1746197279629
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:01 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
25/05/02 14:48:02 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO Metrics: Metrics reporters closed
25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282524
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282526
25/05/02 14:48:02 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:02 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:02 INFO AppInfoParser: Kafka startTimeMs: 1746197282527
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:02 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-4 unregistered
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-5 unregistered
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283638
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283638
25/05/02 14:48:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-6 unregistered
25/05/02 14:48:03 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:03 INFO Metrics: Metrics reporters closed
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:03 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/02 14:48:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/02 14:48:03 INFO AppInfoParser: Kafka version: 3.3.1
25/05/02 14:48:03 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/02 14:48:03 INFO AppInfoParser: Kafka startTimeMs: 1746197283730
25/05/02 14:48:03 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-8 unregistered
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-7 unregistered
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 INFO AppInfoParser: App info kafka.admin.client for adminclient-9 unregistered
25/05/02 14:48:04 ERROR MicroBatchExecution: Query [id = 68845fb3-58ef-4a25-a8a1-f6808463fed5, runId = c887aa5b-8157-460a-9ad1-2ca8838adc1d] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO Metrics: Metrics scheduler closed
25/05/02 14:48:04 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
25/05/02 14:48:04 INFO Metrics: Metrics reporters closed
25/05/02 14:48:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = 68845fb3-58ef-4a25-a8a1-f6808463fed5, runId = c887aa5b-8157-460a-9ad1-2ca8838adc1d] has been shutdown
25/05/02 14:48:04 ERROR MicroBatchExecution: Query [id = 4cefe596-4fec-4810-98f7-853ebe0ea48b, runId = a31acf14-a67a-4ec0-a8aa-fccd8df2da54] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = 4cefe596-4fec-4810-98f7-853ebe0ea48b, runId = a31acf14-a67a-4ec0-a8aa-fccd8df2da54] has been shutdown
25/05/02 14:48:04 ERROR MicroBatchExecution: Query [id = 91685035-fc0f-4afb-ad9c-f63b33760a23, runId = a449c786-32c5-4399-a121-05647d7aaa13] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchEarliestOffsets(KafkaOffsetReaderAdmin.scala:288)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:244)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:241)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:04 INFO MicroBatchExecution: Async log purge executor pool for query [id = 91685035-fc0f-4afb-ad9c-f63b33760a23, runId = a449c786-32c5-4399-a121-05647d7aaa13] has been shutdown
Traceback (most recent call last):
  File "/app/consumer-table/writter_AMOUNT_PER_TYPE_WINDOWED.py", line 106, in <module>
    df_parsed
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 4cefe596-4fec-4810-98f7-853ebe0ea48b, runId = a31acf14-a67a-4ec0-a8aa-fccd8df2da54] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
25/05/02 14:48:06 INFO SparkContext: Invoking stop() from shutdown hook
25/05/02 14:48:06 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/05/02 14:48:06 INFO SparkUI: Stopped Spark web UI at http://5e919d8ee3aa:4043
25/05/02 14:48:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/05/02 14:48:06 INFO MemoryStore: MemoryStore cleared
25/05/02 14:48:06 INFO BlockManager: BlockManager stopped
25/05/02 14:48:07 INFO BlockManagerMaster: BlockManagerMaster stopped
25/05/02 14:48:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/05/02 14:48:07 INFO SparkContext: Successfully stopped SparkContext
25/05/02 14:48:07 INFO ShutdownHookManager: Shutdown hook called
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-d3a25bcb-3fc0-4ef9-8505-62642d959808
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-55107d47-1e7b-4dee-9990-ae3e6e936f40
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914/pyspark-c5e1dc0f-7504-4cfd-be7c-3ed67659d592
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-3fe815fd-5fe7-40b5-b7d2-1a463b846e5c
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-ba1574a7-fe81-4ee4-969e-60ec79ae7475
25/05/02 14:48:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd77ef83-b5ed-494a-8f87-88f881eb3914
