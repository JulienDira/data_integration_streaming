25/05/11 12:45:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-11 12:45:31,193 - transaction_log - INFO - Lancement de l'application Spark Streaming...
25/05/11 12:45:31 INFO SparkContext: Running Spark version 3.5.0
25/05/11 12:45:31 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/11 12:45:31 INFO SparkContext: Java version 11.0.24
25/05/11 12:45:31 INFO ResourceUtils: ==============================================================
25/05/11 12:45:31 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/11 12:45:31 INFO ResourceUtils: ==============================================================
25/05/11 12:45:31 INFO SparkContext: Submitted application: KafkaConsumer_transaction_log
25/05/11 12:45:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/11 12:45:31 INFO ResourceProfile: Limiting resource is cpu
25/05/11 12:45:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/11 12:45:31 INFO SecurityManager: Changing view acls to: root
25/05/11 12:45:31 INFO SecurityManager: Changing modify acls to: root
25/05/11 12:45:31 INFO SecurityManager: Changing view acls groups to: 
25/05/11 12:45:31 INFO SecurityManager: Changing modify acls groups to: 
25/05/11 12:45:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/11 12:45:31 INFO Utils: Successfully started service 'sparkDriver' on port 38871.
25/05/11 12:45:31 INFO SparkEnv: Registering MapOutputTracker
25/05/11 12:45:31 INFO SparkEnv: Registering BlockManagerMaster
25/05/11 12:45:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/11 12:45:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/11 12:45:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/11 12:45:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7ad7161b-0288-4e42-8a07-b48d5ee51b3f
25/05/11 12:45:31 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
25/05/11 12:45:31 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/11 12:45:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/11 12:45:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/05/11 12:45:32 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://4f1ede6faf1c:38871/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://4f1ede6faf1c:38871/jars/kafka-clients-3.3.1.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://4f1ede6faf1c:38871/jars/commons-pool2-2.11.1.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://4f1ede6faf1c:38871/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://4f1ede6faf1c:38871/jars/postgresql-42.2.23.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO Executor: Starting executor ID driver on host 4f1ede6faf1c
25/05/11 12:45:32 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/11 12:45:32 INFO Executor: Java version 11.0.24
25/05/11 12:45:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/11 12:45:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3ba56f61 for default.
25/05/11 12:45:32 INFO Executor: Fetching spark://4f1ede6faf1c:38871/jars/commons-pool2-2.11.1.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO TransportClientFactory: Successfully created connection to 4f1ede6faf1c/172.18.0.13:38871 after 29 ms (0 ms spent in bootstraps)
25/05/11 12:45:32 INFO Utils: Fetching spark://4f1ede6faf1c:38871/jars/commons-pool2-2.11.1.jar to /tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/fetchFileTemp5046145731215317220.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/commons-pool2-2.11.1.jar to class loader default
25/05/11 12:45:32 INFO Executor: Fetching spark://4f1ede6faf1c:38871/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO Utils: Fetching spark://4f1ede6faf1c:38871/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/fetchFileTemp11916717949796334950.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/11 12:45:32 INFO Executor: Fetching spark://4f1ede6faf1c:38871/jars/postgresql-42.2.23.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO Utils: Fetching spark://4f1ede6faf1c:38871/jars/postgresql-42.2.23.jar to /tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/fetchFileTemp18233411550339620693.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/postgresql-42.2.23.jar to class loader default
25/05/11 12:45:32 INFO Executor: Fetching spark://4f1ede6faf1c:38871/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO Utils: Fetching spark://4f1ede6faf1c:38871/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/fetchFileTemp12108652481108925682.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/11 12:45:32 INFO Executor: Fetching spark://4f1ede6faf1c:38871/jars/kafka-clients-3.3.1.jar with timestamp 1746967531248
25/05/11 12:45:32 INFO Utils: Fetching spark://4f1ede6faf1c:38871/jars/kafka-clients-3.3.1.jar to /tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/fetchFileTemp10420084373009160615.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-39d352ee-b90c-449d-91ed-336fbd2c251c/userFiles-d3e642a8-db5e-4d53-ad66-9b8220544874/kafka-clients-3.3.1.jar to class loader default
25/05/11 12:45:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43081.
25/05/11 12:45:32 INFO NettyBlockTransferService: Server created on 4f1ede6faf1c:43081
25/05/11 12:45:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/11 12:45:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4f1ede6faf1c, 43081, None)
25/05/11 12:45:32 INFO BlockManagerMasterEndpoint: Registering block manager 4f1ede6faf1c:43081 with 1048.8 MiB RAM, BlockManagerId(driver, 4f1ede6faf1c, 43081, None)
25/05/11 12:45:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4f1ede6faf1c, 43081, None)
25/05/11 12:45:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4f1ede6faf1c, 43081, None)
2025-05-11 12:45:32,641 - transaction_log - INFO - Session Spark créée.
2025-05-11 12:45:32,641 - transaction_log - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic transaction_log.
25/05/11 12:45:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/11 12:45:32 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-11 12:45:34,320 - transaction_log - INFO - Données chargées avec succès.
2025-05-11 12:45:34,320 - transaction_log - INFO - Vérification du format initial.
25/05/11 12:45:34 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/11 12:45:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/11 12:45:34 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b resolved to file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b.
25/05/11 12:45:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/11 12:45:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/metadata using temp file file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/.metadata.81d66baf-94fc-4e33-b7d3-b84b249006fa.tmp
25/05/11 12:45:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/.metadata.81d66baf-94fc-4e33-b7d3-b84b249006fa.tmp to file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/metadata
25/05/11 12:45:34 INFO MicroBatchExecution: Starting [id = c1cead2c-f45e-403a-8576-3edd515e5aff, runId = aca34597-62be-4249-8f5c-d72593cf5f0b]. Use file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b to store the query checkpoint.
2025-05-11 12:45:34,856 - transaction_log - INFO - Données en cours de transformation...
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

25/05/11 12:45:34 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@22182455] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7d5e450b]
25/05/11 12:45:34 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:34 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:34 INFO MicroBatchExecution: Starting new streaming query.
25/05/11 12:45:34 INFO MicroBatchExecution: Stream started from {}
2025-05-11 12:45:35,052 - transaction_log - INFO - Données parsée avec succès !!!
2025-05-11 12:45:35,052 - transaction_log - INFO - Démarrage de l'écriture en console pour le debug.
25/05/11 12:45:35 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/11 12:45:35 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855 resolved to file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855.
25/05/11 12:45:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/metadata using temp file file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/.metadata.5c102194-1e73-4907-ab3f-d388e23c776c.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/.metadata.5c102194-1e73-4907-ab3f-d388e23c776c.tmp to file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/metadata
25/05/11 12:45:35 INFO MicroBatchExecution: Starting [id = 51b10983-49c8-4669-ae30-e2b93b6d6a45, runId = 64f1ae03-696f-413e-8543-0b4678ec5a26]. Use file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855 to store the query checkpoint.
2025-05-11 12:45:35,198 - transaction_log - INFO - Initialisation de l'écriture en Parquet...
25/05/11 12:45:35 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@22182455] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7d5e450b]
25/05/11 12:45:35 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:35 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:35 INFO MicroBatchExecution: Starting new streaming query.
25/05/11 12:45:35 INFO MicroBatchExecution: Stream started from {}
25/05/11 12:45:35 INFO ResolveWriteToStream: Checkpoint root checkpoints/transaction_log resolved to file:/app/checkpoints/transaction_log.
25/05/11 12:45:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/metadata using temp file file:/app/checkpoints/transaction_log/.metadata.a2936366-7318-451a-9760-601c026e6fc9.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/.metadata.a2936366-7318-451a-9760-601c026e6fc9.tmp to file:/app/checkpoints/transaction_log/metadata
25/05/11 12:45:35 INFO MicroBatchExecution: Starting [id = 820e7837-fe20-4e1f-b293-87089831e2e3, runId = ae1eea44-7f65-4c1e-9618-2d32e0221fb9]. Use file:/app/checkpoints/transaction_log to store the query checkpoint.
25/05/11 12:45:35 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@22182455] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7d5e450b]
25/05/11 12:45:35 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:35 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:35 INFO MicroBatchExecution: Starting new streaming query.
25/05/11 12:45:35 INFO MicroBatchExecution: Stream started from {}
25/05/11 12:45:35 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/11 12:45:35 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/11 12:45:35 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/11 12:45:35 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/11 12:45:35 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/11 12:45:35 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/11 12:45:35 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:35 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:35 INFO AppInfoParser: Kafka startTimeMs: 1746967535552
25/05/11 12:45:35 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:35 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:35 INFO AppInfoParser: Kafka startTimeMs: 1746967535552
25/05/11 12:45:35 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:35 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:35 INFO AppInfoParser: Kafka startTimeMs: 1746967535552
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/sources/0/0 using temp file file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/sources/0/.0.4037b0f8-4428-4f0c-8f08-dad922f4dbb8.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/sources/0/0 using temp file file:/app/checkpoints/transaction_log/sources/0/.0.3ccc560f-0693-4097-944b-6ddbd8582192.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/sources/0/0 using temp file file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/sources/0/.0.ec7c9fd5-b045-4914-a904-fa6c6c38e5ef.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/sources/0/.0.4037b0f8-4428-4f0c-8f08-dad922f4dbb8.tmp to file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/sources/0/0
25/05/11 12:45:35 INFO KafkaMicroBatchStream: Initial offsets: {"transaction_log":{"0":0}}
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/sources/0/.0.3ccc560f-0693-4097-944b-6ddbd8582192.tmp to file:/app/checkpoints/transaction_log/sources/0/0
25/05/11 12:45:35 INFO KafkaMicroBatchStream: Initial offsets: {"transaction_log":{"0":0}}
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/sources/0/.0.ec7c9fd5-b045-4914-a904-fa6c6c38e5ef.tmp to file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/sources/0/0
25/05/11 12:45:35 INFO KafkaMicroBatchStream: Initial offsets: {"transaction_log":{"0":0}}
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/offsets/0 using temp file file:/app/checkpoints/transaction_log/offsets/.0.a06d8401-2eed-4106-a07c-2d38ce9f53d1.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/offsets/0 using temp file file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/offsets/.0.739f6a55-3bc6-436b-8324-96655fb52d12.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/offsets/0 using temp file file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/offsets/.0.94b80177-837b-4202-adb9-a0f282b68e88.tmp
25/05/11 12:45:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/offsets/.0.94b80177-837b-4202-adb9-a0f282b68e88.tmp to file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/offsets/0
25/05/11 12:45:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/offsets/.0.739f6a55-3bc6-436b-8324-96655fb52d12.tmp to file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/offsets/0
25/05/11 12:45:36 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/offsets/.0.a06d8401-2eed-4106-a07c-2d38ce9f53d1.tmp to file:/app/checkpoints/transaction_log/offsets/0
25/05/11 12:45:36 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746967535955,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/11 12:45:36 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746967535961,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/11 12:45:36 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746967535955,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535961
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535955
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535955
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535961
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO FileStreamSinkLog: BatchIds found from listing: 
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535961
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 247.081914 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 246.054038 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 251.630026 ms
25/05/11 12:45:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]]. The input RDD has 1 partitions.
25/05/11 12:45:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
25/05/11 12:45:37 INFO SparkContext: Starting job: start at <unknown>:0
25/05/11 12:45:37 INFO SparkContext: Starting job: start at <unknown>:0
25/05/11 12:45:37 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/05/11 12:45:37 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/05/11 12:45:37 INFO DAGScheduler: Parents of final stage: List()
25/05/11 12:45:37 INFO DAGScheduler: Missing parents: List()
25/05/11 12:45:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at start at <unknown>:0), which has no missing parents
25/05/11 12:45:37 INFO SparkContext: Starting job: start at <unknown>:0
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 11.2 KiB, free 1048.8 MiB)
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1048.8 MiB)
25/05/11 12:45:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4f1ede6faf1c:43081 (size: 5.0 KiB, free: 1048.8 MiB)
25/05/11 12:45:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/05/11 12:45:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/11 12:45:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/05/11 12:45:37 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
25/05/11 12:45:37 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
25/05/11 12:45:37 INFO DAGScheduler: Parents of final stage: List()
25/05/11 12:45:37 INFO DAGScheduler: Missing parents: List()
25/05/11 12:45:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at start at <unknown>:0), which has no missing parents
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.6 KiB, free 1048.8 MiB)
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 1048.8 MiB)
25/05/11 12:45:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4f1ede6faf1c:43081 (size: 4.5 KiB, free: 1048.8 MiB)
25/05/11 12:45:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
25/05/11 12:45:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/11 12:45:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/05/11 12:45:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4f1ede6faf1c, executor driver, partition 0, PROCESS_LOCAL, 9065 bytes) 
25/05/11 12:45:37 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
25/05/11 12:45:37 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
25/05/11 12:45:37 INFO DAGScheduler: Parents of final stage: List()
25/05/11 12:45:37 INFO DAGScheduler: Missing parents: List()
25/05/11 12:45:37 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
25/05/11 12:45:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (4f1ede6faf1c, executor driver, partition 0, PROCESS_LOCAL, 9064 bytes) 
25/05/11 12:45:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/05/11 12:45:37 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 228.6 KiB, free 1048.5 MiB)
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 83.3 KiB, free 1048.5 MiB)
25/05/11 12:45:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4f1ede6faf1c:43081 (size: 83.3 KiB, free: 1048.7 MiB)
25/05/11 12:45:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
25/05/11 12:45:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/11 12:45:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
25/05/11 12:45:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (4f1ede6faf1c, executor driver, partition 0, PROCESS_LOCAL, 9065 bytes) 
25/05/11 12:45:37 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 29.825664 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 38.270626 ms
25/05/11 12:45:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=transaction_log-0 fromOffset=0 untilOffset=45, for query queryId=c1cead2c-f45e-403a-8576-3edd515e5aff batchId=0 taskId=1 partitionId=0
25/05/11 12:45:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=transaction_log-0 fromOffset=0 untilOffset=45, for query queryId=51b10983-49c8-4669-ae30-e2b93b6d6a45 batchId=0 taskId=0 partitionId=0
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 36.104382 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 23.536599 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 29.244642 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 24.844892 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 9.680763 ms
25/05/11 12:45:37 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/11 12:45:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=transaction_log-0 fromOffset=0 untilOffset=45, for query queryId=820e7837-fe20-4e1f-b293-87089831e2e3 batchId=0 taskId=2 partitionId=0
25/05/11 12:45:38 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/11 12:45:38 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/11 12:45:38 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:38 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:38 INFO AppInfoParser: Kafka startTimeMs: 1746967538072
25/05/11 12:45:38 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:38 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:38 INFO AppInfoParser: Kafka startTimeMs: 1746967538072
25/05/11 12:45:38 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:38 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:38 INFO AppInfoParser: Kafka startTimeMs: 1746967538072
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor-3, groupId=spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor] Assigned to partition(s): transaction_log-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor-1, groupId=spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor] Assigned to partition(s): transaction_log-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor-2, groupId=spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor] Assigned to partition(s): transaction_log-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor-1, groupId=spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor] Seeking to offset 0 for partition transaction_log-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor-3, groupId=spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor] Seeking to offset 0 for partition transaction_log-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor-2, groupId=spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor] Seeking to offset 0 for partition transaction_log-0
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor-1, groupId=spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor] Resetting the last seen epoch of partition transaction_log-0 to 0 since the associated topicId changed from null to CpmsWxYHQkiVNN6lULeTZg
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor-3, groupId=spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor] Resetting the last seen epoch of partition transaction_log-0 to 0 since the associated topicId changed from null to CpmsWxYHQkiVNN6lULeTZg
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor-2, groupId=spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor] Resetting the last seen epoch of partition transaction_log-0 to 0 since the associated topicId changed from null to CpmsWxYHQkiVNN6lULeTZg
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor-2, groupId=spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor] Cluster ID: -8oBituFRZGcQwC3C1XFrQ
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor-1, groupId=spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor] Cluster ID: -8oBituFRZGcQwC3C1XFrQ
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor-3, groupId=spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor] Cluster ID: -8oBituFRZGcQwC3C1XFrQ
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor-3, groupId=spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor] Seeking to earliest offset of partition transaction_log-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor-2, groupId=spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor] Seeking to earliest offset of partition transaction_log-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor-1, groupId=spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor] Seeking to earliest offset of partition transaction_log-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor-1, groupId=spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor-2, groupId=spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor-3, groupId=spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor-2, groupId=spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor] Seeking to latest offset of partition transaction_log-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor-3, groupId=spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor] Seeking to latest offset of partition transaction_log-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor-1, groupId=spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor] Seeking to latest offset of partition transaction_log-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor-1, groupId=spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor-2, groupId=spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor-3, groupId=spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor] Resetting offset for partition transaction_log-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/11 12:45:38 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/11 12:45:38 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
25/05/11 12:45:38 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/05/11 12:45:38 INFO KafkaDataConsumer: From Kafka topicPartition=transaction_log-0 groupId=spark-kafka-source-c18e9c73-af33-414b-b8a3-4b76b80d1619-1340120973-executor read 45 records through 1 polls (polled  out 45 records), taking 582288416 nanos, during time span of 665180338 nanos.
25/05/11 12:45:38 INFO KafkaDataConsumer: From Kafka topicPartition=transaction_log-0 groupId=spark-kafka-source-9b5bf0b9-5831-4c8f-b146-c1e876d69be7--1699598369-executor read 45 records through 1 polls (polled  out 45 records), taking 582520207 nanos, during time span of 665174289 nanos.
25/05/11 12:45:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 39768 bytes result sent to driver
25/05/11 12:45:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 36876 bytes result sent to driver
25/05/11 12:45:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1206 ms on 4f1ede6faf1c (executor driver) (1/1)
25/05/11 12:45:38 INFO CodeGenerator: Code generated in 13.46118 ms
25/05/11 12:45:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/05/11 12:45:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1265 ms on 4f1ede6faf1c (executor driver) (1/1)
25/05/11 12:45:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/05/11 12:45:38 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 1.282 s
25/05/11 12:45:38 INFO CodeGenerator: Code generated in 7.24679 ms
25/05/11 12:45:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/11 12:45:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/05/11 12:45:38 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 1.579420 s
25/05/11 12:45:38 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 1.492 s
25/05/11 12:45:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/11 12:45:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/05/11 12:45:38 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 1.581398 s
25/05/11 12:45:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] is committing.
25/05/11 12:45:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/11 12:45:38 INFO CodeGenerator: Code generated in 34.931225 ms
25/05/11 12:45:38 INFO CodecConfig: Compression: SNAPPY
25/05/11 12:45:38 INFO CodecConfig: Compression: SNAPPY
25/05/11 12:45:38 INFO CodeGenerator: Code generated in 7.540821 ms
25/05/11 12:45:38 INFO CodeGenerator: Code generated in 8.948105 ms
25/05/11 12:45:38 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
25/05/11 12:45:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "json_value",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "key",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "topic",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "partition",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "timestamp",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ingestion_time",
    "type" : "timestamp",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary json_value (STRING);
  optional binary key (STRING);
  optional binary topic (STRING);
  optional int32 partition;
  optional int64 offset;
  optional int96 timestamp;
  required int96 ingestion_time;
}

       
25/05/11 12:45:39 INFO CodecPool: Got brand-new compressor [.snappy]
25/05/11 12:45:39 INFO KafkaDataConsumer: From Kafka topicPartition=transaction_log-0 groupId=spark-kafka-source-b36de718-9321-4f2d-8e38-bf9fbe503fd5--1102404150-executor read 45 records through 1 polls (polled  out 45 records), taking 582460964 nanos, during time span of 1715851977 nanos.
25/05/11 12:45:39 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3578 bytes result sent to driver
25/05/11 12:45:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2087 ms on 4f1ede6faf1c (executor driver) (1/1)
25/05/11 12:45:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/05/11 12:45:39 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 2.229 s
25/05/11 12:45:39 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/11 12:45:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
25/05/11 12:45:39 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 2.482838 s
25/05/11 12:45:39 INFO FileFormatWriter: Start to commit write Job a3b10a91-d006-447f-a2d0-ae387bbd09bc.
25/05/11 12:45:39 INFO FileStreamSinkLog: Set the compact interval to 10 [defaultCompactInterval: 10]
25/05/11 12:45:39 INFO CheckpointFileManager: Writing atomically to /app/data_lake/transaction_log/_spark_metadata/0 using temp file /app/data_lake/transaction_log/_spark_metadata/.0.cfeeae6d-c79c-43b7-9859-7dcbbe8991a7.tmp
25/05/11 12:45:40 INFO CheckpointFileManager: Renamed temp file /app/data_lake/transaction_log/_spark_metadata/.0.cfeeae6d-c79c-43b7-9859-7dcbbe8991a7.tmp to /app/data_lake/transaction_log/_spark_metadata/0
25/05/11 12:45:40 INFO ManifestFileCommitProtocol: Committed batch 0
25/05/11 12:45:40 INFO FileFormatWriter: Write Job a3b10a91-d006-447f-a2d0-ae387bbd09bc committed. Elapsed time: 198 ms.
25/05/11 12:45:40 INFO FileFormatWriter: Finished processing stats for write job a3b10a91-d006-447f-a2d0-ae387bbd09bc.
25/05/11 12:45:40 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/transaction_log/commits/0 using temp file file:/app/checkpoints/transaction_log/commits/.0.decec233-c3c5-4a26-a765-b4ebc0022639.tmp
25/05/11 12:45:40 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/transaction_log/commits/.0.decec233-c3c5-4a26-a765-b4ebc0022639.tmp to file:/app/checkpoints/transaction_log/commits/0
25/05/11 12:45:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "820e7837-fe20-4e1f-b293-87089831e2e3",
  "runId" : "ae1eea44-7f65-4c1e-9618-2d32e0221fb9",
  "name" : null,
  "timestamp" : "2025-05-11T12:45:35.376Z",
  "batchId" : 0,
  "numInputRows" : 45,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 9.564293304994687,
  "durationMs" : {
    "addBatch" : 3495,
    "commitOffsets" : 43,
    "getBatch" : 31,
    "latestOffset" : 575,
    "queryPlanning" : 478,
    "triggerExecution" : 4705,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[transaction_log]]",
    "startOffset" : null,
    "endOffset" : {
      "transaction_log" : {
        "0" : 45
      }
    },
    "latestOffset" : {
      "transaction_log" : {
        "0" : 45
      }
    },
    "numInputRows" : 45,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 9.564293304994687,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "FileSink[/app/data_lake/transaction_log]",
    "numOutputRows" : -1
  }
}
25/05/11 12:45:40 INFO CodeGenerator: Code generated in 12.164538 ms
25/05/11 12:45:40 INFO CodeGenerator: Code generated in 19.845546 ms
+----+--------------------+
| key|               value|
+----+--------------------+
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
|NULL|{"transaction_id"...|
+----+--------------------+
only showing top 20 rows

+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+---------------+---------+------+-----------------------+-----------------------+--------------+
|json_value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |key |topic          |partition|offset|timestamp              |ingestion_time         |ingestion_date|
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+---------------+---------+------+-----------------------+-----------------------+--------------+
|{"transaction_id": "TXN-436782a4", "timestamp": "2025-04-29T05:05:14.305600Z", "user_id": "USER-7274", "user_name": "Lee", "product_id": "PROD-202", "amount": 790.78, "currency": "EUR", "transaction_type": "refund", "status": "failed", "location": {"city": "Toronto", "country": "Canada"}, "payment_method": "paypal", "product_category": "electronics", "quantity": 2, "shipping_address": {"street": "574 Main St", "zip": "11800", "city": "Toronto", "country": "Canada"}, "device_info": {"os": "MacOS", "browser": "Firefox", "ip_address": "21.54.158.231"}, "customer_rating": 3, "discount_code": null, "tax_amount": 0.0, "thread": 0, "message_number": 0, "timestamp_of_reception_log": "11/05/2025 12:32:55"}                             |NULL|transaction_log|0        |0     |2025-05-11 10:32:55.421|2025-05-11 12:45:35.961|2025-05-11    |
|{"transaction_id": "TXN-430ce1ec", "timestamp": "2025-04-23T12:49:21.305600Z", "user_id": "USER-6914", "user_name": "Penelope", "product_id": "PROD-774", "amount": 552.72, "currency": "CAD", "transaction_type": "refund", "status": "cancelled", "location": {"city": "Phoenix", "country": "USA"}, "payment_method": "cryptocurrency", "product_category": "electronics", "quantity": 9, "shipping_address": {"street": "223 Main St", "zip": "53129", "city": "Phoenix", "country": "USA"}, "device_info": {"os": "Linux", "browser": "Firefox", "ip_address": "139.234.142.6"}, "customer_rating": 5, "discount_code": null, "tax_amount": 0.0, "thread": 1, "message_number": 0, "timestamp_of_reception_log": "11/05/2025 12:32:55"}                   |NULL|transaction_log|0        |1     |2025-05-11 10:32:55.421|2025-05-11 12:45:35.961|2025-05-11    |
|{"transaction_id": "TXN-e7fd6b4e", "timestamp": "2025-04-22T18:02:27.312495Z", "user_id": "USER-5924", "user_name": "King", "product_id": "PROD-916", "amount": 680.81, "currency": "AUD", "transaction_type": "purchase", "status": "cancelled", "location": {"city": "Phoenix", "country": "USA"}, "payment_method": "credit_card", "product_category": "home_goods", "quantity": 6, "shipping_address": {"street": "612 Main St", "zip": "46817", "city": "Phoenix", "country": "USA"}, "device_info": {"os": "MacOS", "browser": "Edge", "ip_address": "164.85.63.37"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 3, "message_number": 0, "timestamp_of_reception_log": "11/05/2025 12:32:55"}                          |NULL|transaction_log|0        |2     |2025-05-11 10:32:55.421|2025-05-11 12:45:35.961|2025-05-11    |
|{"transaction_id": "TXN-a44b4144", "timestamp": "2025-04-20T02:22:55.312495Z", "user_id": "USER-5863", "user_name": "Williams", "product_id": "PROD-839", "amount": 446.64, "currency": "CAD", "transaction_type": "refund", "status": "processing", "location": {"city": "S\u00e3o Paulo", "country": "Brazil"}, "payment_method": "apple_pay", "product_category": "electronics", "quantity": 5, "shipping_address": {"street": "610 Main St", "zip": "91353", "city": "S\u00e3o Paulo", "country": "Brazil"}, "device_info": {"os": "Windows", "browser": "Firefox", "ip_address": "115.136.192.73"}, "customer_rating": 5, "discount_code": null, "tax_amount": 0.0, "thread": 4, "message_number": 0, "timestamp_of_reception_log": "11/05/2025 12:32:55"}|NULL|transaction_log|0        |3     |2025-05-11 10:32:55.421|2025-05-11 12:45:35.961|2025-05-11    |
|{"transaction_id": "TXN-4aeb6c15", "timestamp": "2025-04-19T07:20:12.305600Z", "user_id": "USER-6217", "user_name": "Isabella", "product_id": "PROD-669", "amount": 873.04, "currency": "JPY", "transaction_type": "payment", "status": "pending", "location": {"city": "Salvador", "country": "Brazil"}, "payment_method": "paypal", "product_category": "food", "quantity": 9, "shipping_address": {"street": "980 Main St", "zip": "80949", "city": "Salvador", "country": "Brazil"}, "device_info": {"os": "Linux", "browser": "Safari", "ip_address": "67.112.91.36"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 2, "message_number": 0, "timestamp_of_reception_log": "11/05/2025 12:32:55"}                          |NULL|transaction_log|0        |4     |2025-05-11 10:32:55.421|2025-05-11 12:45:35.961|2025-05-11    |
|{"transaction_id": "TXN-cdf1eb54", "timestamp": "2025-04-17T08:57:51.305600Z", "user_id": "USER-1483", "user_name": "Elijah", "product_id": "PROD-506", "amount": 416.12, "currency": "EUR", "transaction_type": "refund", "status": "pending", "location": {"city": "S\u00e3o Paulo", "country": "Brazil"}, "payment_method": "credit_card", "product_category": "food", "quantity": 9, "shipping_address": {"street": "723 Main St", "zip": "66963", "city": "S\u00e3o Paulo", "country": "Brazil"}, "device_info": {"os": "MacOS", "browser": "Firefox", "ip_address": "102.103.179.83"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 1, "message_number": 1, "timestamp_of_reception_log": "11/05/2025 12:32:55"}         |NULL|transaction_log|0        |5     |2025-05-11 10:32:55.421|2025-05-11 12:45:35.961|2025-05-11    |
|{"transaction_id": "TXN-0d8d70fd", "timestamp": "2025-05-02T13:30:35.312495Z", "user_id": "USER-4803", "user_name": "Olivia", "product_id": "PROD-917", "amount": 217.13, "currency": "AUD", "transaction_type": "refund", "status": "failed", "location": {"city": "Adelaide", "country": "Australia"}, "payment_method": "google_pay", "product_category": "clothing", "quantity": 8, "shipping_address": {"street": "618 Main St", "zip": "50177", "city": "Adelaide", "country": "Australia"}, "device_info": {"os": "Android", "browser": "Chrome", "ip_address": "187.25.224.117"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 3, "message_number": 1, "timestamp_of_reception_log": "11/05/2025 12:32:55"}            |NULL|transaction_log|0        |6     |2025-05-11 10:32:55.421|2025-05-11 12:45:35.961|2025-05-11    |
|{"transaction_id": "TXN-4acf27a5", "timestamp": "2025-05-03T15:04:57.305600Z", "user_id": "USER-9928", "user_name": "Martinez", "product_id": "PROD-637", "amount": 978.15, "currency": "EUR", "transaction_type": "payment", "status": "completed", "location": {"city": "New York", "country": "USA"}, "payment_method": "paypal", "product_category": "electronics", "quantity": 2, "shipping_address": {"street": "577 Main St", "zip": "21437", "city": "New York", "country": "USA"}, "device_info": {"os": "Windows", "browser": "Edge", "ip_address": "69.251.211.244"}, "customer_rating": 4, "discount_code": "DISCOUNT-588", "tax_amount": 72.54, "thread": 0, "message_number": 2, "timestamp_of_reception_log": "11/05/2025 12:32:55"}            |NULL|transaction_log|0        |7     |2025-05-11 10:32:55.433|2025-05-11 12:45:35.961|2025-05-11    |
|{"transaction_id": "TXN-a422c9e7", "timestamp": "2025-04-19T07:02:42.312495Z", "user_id": "USER-2493", "user_name": "Jones", "product_id": "PROD-484", "amount": 717.3, "currency": "GBP", "transaction_type": "payment", "status": "completed", "location": {"city": "Ottawa", "country": "Canada"}, "payment_method": "paypal", "product_category": "clothing", "quantity": 6, "shipping_address": {"street": "773 Main St", "zip": "58645", "city": "Ottawa", "country": "Canada"}, "device_info": {"os": "Linux", "browser": "Chrome", "ip_address": "38.13.155.37"}, "customer_rating": 3, "discount_code": null, "tax_amount": 136.32, "thread": 4, "message_number": 2, "timestamp_of_reception_log": "11/05/2025 12:32:55"}                            |NULL|transaction_log|0        |8     |2025-05-11 10:32:55.439|2025-05-11 12:45:35.961|2025-05-11    |
|{"transaction_id": "TXN-05db4e32", "timestamp": "2025-04-14T00:25:23.305600Z", "user_id": "USER-1921", "user_name": "Scarlett", "product_id": "PROD-933", "amount": 469.41, "currency": "CAD", "transaction_type": "refund", "status": "pending", "location": {"city": "Frankfurt", "country": "Germany"}, "payment_method": "bank_transfer", "product_category": "clothing", "quantity": 8, "shipping_address": {"street": "109 Main St", "zip": "41157", "city": "Frankfurt", "country": "Germany"}, "device_info": {"os": "iOS", "browser": "Firefox", "ip_address": "130.68.46.81"}, "customer_rating": null, "discount_code": null, "tax_amount": 0.0, "thread": 2, "message_number": 2, "timestamp_of_reception_log": "11/05/2025 12:32:55"}             |NULL|transaction_log|0        |9     |2025-05-11 10:32:55.439|2025-05-11 12:45:35.961|2025-05-11    |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+---------------+---------+------+-----------------------+-----------------------+--------------+
only showing top 10 rows

25/05/11 12:45:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
25/05/11 12:45:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] committed.
25/05/11 12:45:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/commits/0 using temp file file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/commits/.0.c8cf008a-63f1-4612-8540-1236d9393f36.tmp
25/05/11 12:45:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/commits/0 using temp file file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/commits/.0.0f4c881f-a0de-4f2f-a2c5-e822e212224e.tmp
25/05/11 12:45:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/commits/.0.c8cf008a-63f1-4612-8540-1236d9393f36.tmp to file:/tmp/temporary-d91ce3cc-4c0b-4b73-8acd-8dcef1533855/commits/0
25/05/11 12:45:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/commits/.0.0f4c881f-a0de-4f2f-a2c5-e822e212224e.tmp to file:/tmp/temporary-fe0f072a-b5f4-4dc9-9d44-90e902cb225b/commits/0
25/05/11 12:45:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "51b10983-49c8-4669-ae30-e2b93b6d6a45",
  "runId" : "64f1ae03-696f-413e-8543-0b4678ec5a26",
  "name" : null,
  "timestamp" : "2025-05-11T12:45:35.201Z",
  "batchId" : 0,
  "numInputRows" : 45,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 8.695652173913043,
  "durationMs" : {
    "addBatch" : 3801,
    "commitOffsets" : 38,
    "getBatch" : 31,
    "latestOffset" : 756,
    "queryPlanning" : 477,
    "triggerExecution" : 5175,
    "walCommit" : 58
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[transaction_log]]",
    "startOffset" : null,
    "endOffset" : {
      "transaction_log" : {
        "0" : 45
      }
    },
    "latestOffset" : {
      "transaction_log" : {
        "0" : 45
      }
    },
    "numInputRows" : 45,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 8.695652173913043,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@7a227111",
    "numOutputRows" : 45
  }
}
25/05/11 12:45:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c1cead2c-f45e-403a-8576-3edd515e5aff",
  "runId" : "aca34597-62be-4249-8f5c-d72593cf5f0b",
  "name" : null,
  "timestamp" : "2025-05-11T12:45:34.898Z",
  "batchId" : 0,
  "numInputRows" : 45,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 8.210180623973727,
  "durationMs" : {
    "addBatch" : 3802,
    "commitOffsets" : 40,
    "getBatch" : 31,
    "latestOffset" : 1033,
    "queryPlanning" : 481,
    "triggerExecution" : 5480,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[transaction_log]]",
    "startOffset" : null,
    "endOffset" : {
      "transaction_log" : {
        "0" : 45
      }
    },
    "latestOffset" : {
      "transaction_log" : {
        "0" : 45
      }
    },
    "numInputRows" : 45,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 8.210180623973727,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@7a227111",
    "numOutputRows" : 45
  }
}
25/05/11 12:45:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:45:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:45:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
