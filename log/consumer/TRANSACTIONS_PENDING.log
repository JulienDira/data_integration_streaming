25/05/11 12:45:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-05-11 12:45:31,163 - TRANSACTIONS_PENDING - INFO - Lancement de l'application Spark Streaming...
25/05/11 12:45:31 INFO SparkContext: Running Spark version 3.5.0
25/05/11 12:45:31 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/11 12:45:31 INFO SparkContext: Java version 11.0.24
25/05/11 12:45:31 INFO ResourceUtils: ==============================================================
25/05/11 12:45:31 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/11 12:45:31 INFO ResourceUtils: ==============================================================
25/05/11 12:45:31 INFO SparkContext: Submitted application: KafkaConsumer_TRANSACTIONS_PENDING
25/05/11 12:45:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/11 12:45:31 INFO ResourceProfile: Limiting resource is cpu
25/05/11 12:45:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/11 12:45:31 INFO SecurityManager: Changing view acls to: root
25/05/11 12:45:31 INFO SecurityManager: Changing modify acls to: root
25/05/11 12:45:31 INFO SecurityManager: Changing view acls groups to: 
25/05/11 12:45:31 INFO SecurityManager: Changing modify acls groups to: 
25/05/11 12:45:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
25/05/11 12:45:31 INFO Utils: Successfully started service 'sparkDriver' on port 35543.
25/05/11 12:45:31 INFO SparkEnv: Registering MapOutputTracker
25/05/11 12:45:31 INFO SparkEnv: Registering BlockManagerMaster
25/05/11 12:45:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/11 12:45:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/11 12:45:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/11 12:45:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-eeeebd36-0c85-46a6-892e-ce60965b86f1
25/05/11 12:45:31 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
25/05/11 12:45:31 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/11 12:45:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/11 12:45:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/05/11 12:45:31 INFO SparkContext: Added JAR file:///spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://6c276c7a973d:35543/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746967531215
25/05/11 12:45:31 INFO SparkContext: Added JAR file:///spark/jars/kafka-clients-3.3.1.jar at spark://6c276c7a973d:35543/jars/kafka-clients-3.3.1.jar with timestamp 1746967531215
25/05/11 12:45:31 INFO SparkContext: Added JAR file:///spark/jars/commons-pool2-2.11.1.jar at spark://6c276c7a973d:35543/jars/commons-pool2-2.11.1.jar with timestamp 1746967531215
25/05/11 12:45:31 INFO SparkContext: Added JAR file:///spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://6c276c7a973d:35543/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746967531215
25/05/11 12:45:31 INFO SparkContext: Added JAR file:///spark/jars/postgresql-42.2.23.jar at spark://6c276c7a973d:35543/jars/postgresql-42.2.23.jar with timestamp 1746967531215
25/05/11 12:45:32 INFO Executor: Starting executor ID driver on host 6c276c7a973d
25/05/11 12:45:32 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/11 12:45:32 INFO Executor: Java version 11.0.24
25/05/11 12:45:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/spark/jars/*,file:/app/*'
25/05/11 12:45:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6028b57f for default.
25/05/11 12:45:32 INFO Executor: Fetching spark://6c276c7a973d:35543/jars/kafka-clients-3.3.1.jar with timestamp 1746967531215
25/05/11 12:45:32 INFO TransportClientFactory: Successfully created connection to 6c276c7a973d/172.18.0.14:35543 after 25 ms (0 ms spent in bootstraps)
25/05/11 12:45:32 INFO Utils: Fetching spark://6c276c7a973d:35543/jars/kafka-clients-3.3.1.jar to /tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/fetchFileTemp7164613711644400539.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/kafka-clients-3.3.1.jar to class loader default
25/05/11 12:45:32 INFO Executor: Fetching spark://6c276c7a973d:35543/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1746967531215
25/05/11 12:45:32 INFO Utils: Fetching spark://6c276c7a973d:35543/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/fetchFileTemp9049074268040578414.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/11 12:45:32 INFO Executor: Fetching spark://6c276c7a973d:35543/jars/postgresql-42.2.23.jar with timestamp 1746967531215
25/05/11 12:45:32 INFO Utils: Fetching spark://6c276c7a973d:35543/jars/postgresql-42.2.23.jar to /tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/fetchFileTemp1700882685569039013.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/postgresql-42.2.23.jar to class loader default
25/05/11 12:45:32 INFO Executor: Fetching spark://6c276c7a973d:35543/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1746967531215
25/05/11 12:45:32 INFO Utils: Fetching spark://6c276c7a973d:35543/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/fetchFileTemp18305964712022287977.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
25/05/11 12:45:32 INFO Executor: Fetching spark://6c276c7a973d:35543/jars/commons-pool2-2.11.1.jar with timestamp 1746967531215
25/05/11 12:45:32 INFO Utils: Fetching spark://6c276c7a973d:35543/jars/commons-pool2-2.11.1.jar to /tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/fetchFileTemp16844931812641006175.tmp
25/05/11 12:45:32 INFO Executor: Adding file:/tmp/spark-7e2a3b8b-a223-472c-be5f-147e4a861ee3/userFiles-171a1bba-11ae-454f-8ccc-3cc7c27e271f/commons-pool2-2.11.1.jar to class loader default
25/05/11 12:45:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46359.
25/05/11 12:45:32 INFO NettyBlockTransferService: Server created on 6c276c7a973d:46359
25/05/11 12:45:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/11 12:45:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6c276c7a973d, 46359, None)
25/05/11 12:45:32 INFO BlockManagerMasterEndpoint: Registering block manager 6c276c7a973d:46359 with 1048.8 MiB RAM, BlockManagerId(driver, 6c276c7a973d, 46359, None)
25/05/11 12:45:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6c276c7a973d, 46359, None)
25/05/11 12:45:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6c276c7a973d, 46359, None)
2025-05-11 12:45:32,601 - TRANSACTIONS_PENDING - INFO - Session Spark créée.
2025-05-11 12:45:32,601 - TRANSACTIONS_PENDING - INFO - Tentative de connexion à Kafka sur broker:29092 et abonnement au topic TRANSACTIONS_PENDING.
25/05/11 12:45:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/11 12:45:32 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
2025-05-11 12:45:34,225 - TRANSACTIONS_PENDING - INFO - Données chargées avec succès.
2025-05-11 12:45:34,225 - TRANSACTIONS_PENDING - INFO - Vérification du format initial.
25/05/11 12:45:34 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/05/11 12:45:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/11 12:45:34 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd resolved to file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd.
25/05/11 12:45:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/11 12:45:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/metadata using temp file file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/.metadata.12346a14-481a-4108-a025-ec71e56b0967.tmp
25/05/11 12:45:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/.metadata.12346a14-481a-4108-a025-ec71e56b0967.tmp to file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/metadata
25/05/11 12:45:34 INFO MicroBatchExecution: Starting [id = b3bd168f-106f-4c93-9c13-3c50cc60c464, runId = e668bc66-eb04-4c11-a0f2-01b7d22141b1]. Use file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd to store the query checkpoint.
2025-05-11 12:45:34,740 - TRANSACTIONS_PENDING - INFO - Schéma du message défini.
2025-05-11 12:45:34,740 - TRANSACTIONS_PENDING - INFO - Données en cours de transformation...
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

25/05/11 12:45:34 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5f5aeb5c] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@718a80cc]
25/05/11 12:45:34 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:34 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:34 INFO MicroBatchExecution: Starting new streaming query.
25/05/11 12:45:34 INFO MicroBatchExecution: Stream started from {}
25/05/11 12:45:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-05-11 12:45:35,252 - TRANSACTIONS_PENDING - INFO - Données parsée avec succès !!!
2025-05-11 12:45:35,252 - TRANSACTIONS_PENDING - INFO - Démarrage de l'écriture en console pour le debug.
25/05/11 12:45:35 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
25/05/11 12:45:35 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2 resolved to file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2.
25/05/11 12:45:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/metadata using temp file file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/.metadata.3a14c43c-0d05-4c42-828c-a4d130a9d858.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/.metadata.3a14c43c-0d05-4c42-828c-a4d130a9d858.tmp to file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/metadata
25/05/11 12:45:35 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/11 12:45:35 INFO MicroBatchExecution: Starting [id = 28e0c039-ca01-45b1-b479-89987819521c, runId = 96c1a706-c5f9-4e56-814e-e360ac2d8067]. Use file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2 to store the query checkpoint.
2025-05-11 12:45:35,399 - TRANSACTIONS_PENDING - INFO - Initialisation de l'écriture en Parquet...
25/05/11 12:45:35 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5f5aeb5c] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@718a80cc]
25/05/11 12:45:35 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:35 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:35 INFO MicroBatchExecution: Starting new streaming query.
25/05/11 12:45:35 INFO MicroBatchExecution: Stream started from {}
25/05/11 12:45:35 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/11 12:45:35 INFO ResolveWriteToStream: Checkpoint root checkpoints/TRANSACTIONS_PENDING resolved to file:/app/checkpoints/TRANSACTIONS_PENDING.
25/05/11 12:45:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
25/05/11 12:45:35 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/11 12:45:35 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/TRANSACTIONS_PENDING/metadata using temp file file:/app/checkpoints/TRANSACTIONS_PENDING/.metadata.7aa294e2-70a5-468f-bffd-aeafac661404.tmp
25/05/11 12:45:35 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:35 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:35 INFO AppInfoParser: Kafka startTimeMs: 1746967535494
25/05/11 12:45:35 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:35 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:35 INFO AppInfoParser: Kafka startTimeMs: 1746967535494
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/TRANSACTIONS_PENDING/.metadata.7aa294e2-70a5-468f-bffd-aeafac661404.tmp to file:/app/checkpoints/TRANSACTIONS_PENDING/metadata
25/05/11 12:45:35 INFO MicroBatchExecution: Starting [id = 605aad7c-5c7b-4fd5-b6fa-1dd99b2006d0, runId = 54c4e136-673c-4d22-b78d-fa0d0da45d04]. Use file:/app/checkpoints/TRANSACTIONS_PENDING to store the query checkpoint.
25/05/11 12:45:35 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5f5aeb5c] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@718a80cc]
25/05/11 12:45:35 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:35 INFO OffsetSeqLog: BatchIds found from listing: 
25/05/11 12:45:35 INFO MicroBatchExecution: Starting new streaming query.
25/05/11 12:45:35 INFO MicroBatchExecution: Stream started from {}
25/05/11 12:45:35 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [broker:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

25/05/11 12:45:35 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
25/05/11 12:45:35 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:35 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:35 INFO AppInfoParser: Kafka startTimeMs: 1746967535550
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/sources/0/0 using temp file file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/sources/0/.0.9e8dec34-78fd-439a-be93-c8566312ffab.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/TRANSACTIONS_PENDING/sources/0/0 using temp file file:/app/checkpoints/TRANSACTIONS_PENDING/sources/0/.0.38ae0d07-8512-424e-b080-02df1607ae7d.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/sources/0/0 using temp file file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/sources/0/.0.8d45384e-e2af-4338-bc51-c8805a7aa14c.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/TRANSACTIONS_PENDING/sources/0/.0.38ae0d07-8512-424e-b080-02df1607ae7d.tmp to file:/app/checkpoints/TRANSACTIONS_PENDING/sources/0/0
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/sources/0/.0.9e8dec34-78fd-439a-be93-c8566312ffab.tmp to file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/sources/0/0
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/sources/0/.0.8d45384e-e2af-4338-bc51-c8805a7aa14c.tmp to file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/sources/0/0
25/05/11 12:45:35 INFO KafkaMicroBatchStream: Initial offsets: {"TRANSACTIONS_PENDING":{"0":0}}
25/05/11 12:45:35 INFO KafkaMicroBatchStream: Initial offsets: {"TRANSACTIONS_PENDING":{"0":0}}
25/05/11 12:45:35 INFO KafkaMicroBatchStream: Initial offsets: {"TRANSACTIONS_PENDING":{"0":0}}
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/TRANSACTIONS_PENDING/offsets/0 using temp file file:/app/checkpoints/TRANSACTIONS_PENDING/offsets/.0.7f4d51e4-e063-4b73-92d5-7d59d7882f51.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/offsets/0 using temp file file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/offsets/.0.e058674a-83a8-42b3-94fd-febcd872f4fa.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/offsets/0 using temp file file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/offsets/.0.6742999e-c91e-4f96-92d7-f9aba8ef96f9.tmp
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/offsets/.0.e058674a-83a8-42b3-94fd-febcd872f4fa.tmp to file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/offsets/0
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/offsets/.0.6742999e-c91e-4f96-92d7-f9aba8ef96f9.tmp to file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/offsets/0
25/05/11 12:45:35 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/TRANSACTIONS_PENDING/offsets/.0.7f4d51e4-e063-4b73-92d5-7d59d7882f51.tmp to file:/app/checkpoints/TRANSACTIONS_PENDING/offsets/0
25/05/11 12:45:35 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746967535844,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/11 12:45:35 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746967535844,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/11 12:45:35 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746967535844,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535844
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535844
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535844
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535844
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO FileStreamSinkLog: BatchIds found from listing: 
25/05/11 12:45:36 INFO IncrementalExecution: Current batch timestamp = 1746967535844
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
25/05/11 12:45:36 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 252.567555 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 292.134747 ms
25/05/11 12:45:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]]. The input RDD has 1 partitions.
25/05/11 12:45:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
25/05/11 12:45:37 INFO SparkContext: Starting job: start at <unknown>:0
25/05/11 12:45:37 INFO SparkContext: Starting job: start at <unknown>:0
25/05/11 12:45:37 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
25/05/11 12:45:37 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
25/05/11 12:45:37 INFO DAGScheduler: Parents of final stage: List()
25/05/11 12:45:37 INFO DAGScheduler: Missing parents: List()
25/05/11 12:45:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
25/05/11 12:45:37 INFO SparkContext: Starting job: start at <unknown>:0
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.6 KiB, free 1048.8 MiB)
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 1048.8 MiB)
25/05/11 12:45:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 6c276c7a973d:46359 (size: 4.5 KiB, free: 1048.8 MiB)
25/05/11 12:45:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
25/05/11 12:45:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/11 12:45:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/05/11 12:45:37 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
25/05/11 12:45:37 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
25/05/11 12:45:37 INFO DAGScheduler: Parents of final stage: List()
25/05/11 12:45:37 INFO DAGScheduler: Missing parents: List()
25/05/11 12:45:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 34.0 KiB, free 1048.8 MiB)
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 1048.7 MiB)
25/05/11 12:45:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 6c276c7a973d:46359 (size: 12.9 KiB, free: 1048.8 MiB)
25/05/11 12:45:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
25/05/11 12:45:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (6c276c7a973d, executor driver, partition 0, PROCESS_LOCAL, 9070 bytes) 
25/05/11 12:45:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/11 12:45:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/05/11 12:45:37 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
25/05/11 12:45:37 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
25/05/11 12:45:37 INFO DAGScheduler: Parents of final stage: List()
25/05/11 12:45:37 INFO DAGScheduler: Missing parents: List()
25/05/11 12:45:37 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0), which has no missing parents
25/05/11 12:45:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (6c276c7a973d, executor driver, partition 0, PROCESS_LOCAL, 9067 bytes) 
25/05/11 12:45:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/05/11 12:45:37 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 250.8 KiB, free 1048.5 MiB)
25/05/11 12:45:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 89.0 KiB, free 1048.4 MiB)
25/05/11 12:45:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 6c276c7a973d:46359 (size: 89.0 KiB, free: 1048.7 MiB)
25/05/11 12:45:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
25/05/11 12:45:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/11 12:45:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
25/05/11 12:45:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (6c276c7a973d, executor driver, partition 0, PROCESS_LOCAL, 9069 bytes) 
25/05/11 12:45:37 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 30.67589 ms
25/05/11 12:45:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TRANSACTIONS_PENDING-0 fromOffset=0 untilOffset=9, for query queryId=b3bd168f-106f-4c93-9c13-3c50cc60c464 batchId=0 taskId=0 partitionId=0
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 40.24848 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 56.71574 ms
25/05/11 12:45:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TRANSACTIONS_PENDING-0 fromOffset=0 untilOffset=9, for query queryId=28e0c039-ca01-45b1-b479-89987819521c batchId=0 taskId=1 partitionId=0
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 12.210058 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 17.876271 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 27.020675 ms
25/05/11 12:45:37 INFO CodeGenerator: Code generated in 9.989456 ms
25/05/11 12:45:37 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/11 12:45:37 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/11 12:45:38 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=TRANSACTIONS_PENDING-0 fromOffset=0 untilOffset=9, for query queryId=605aad7c-5c7b-4fd5-b6fa-1dd99b2006d0 batchId=0 taskId=2 partitionId=0
25/05/11 12:45:38 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [broker:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

25/05/11 12:45:38 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:38 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:38 INFO AppInfoParser: Kafka startTimeMs: 1746967538070
25/05/11 12:45:38 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:38 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:38 INFO AppInfoParser: Kafka startTimeMs: 1746967538072
25/05/11 12:45:38 INFO AppInfoParser: Kafka version: 3.3.1
25/05/11 12:45:38 INFO AppInfoParser: Kafka commitId: e23c59d00e687ff5
25/05/11 12:45:38 INFO AppInfoParser: Kafka startTimeMs: 1746967538072
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor-1, groupId=spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor] Assigned to partition(s): TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor-2, groupId=spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor] Assigned to partition(s): TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor-3, groupId=spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor] Assigned to partition(s): TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor-2, groupId=spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor] Seeking to offset 0 for partition TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor-3, groupId=spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor] Seeking to offset 0 for partition TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor-1, groupId=spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor] Seeking to offset 0 for partition TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor-2, groupId=spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor] Resetting the last seen epoch of partition TRANSACTIONS_PENDING-0 to 0 since the associated topicId changed from null to 8JK_KUdEQTSgCasQ4XEalA
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor-3, groupId=spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor] Resetting the last seen epoch of partition TRANSACTIONS_PENDING-0 to 0 since the associated topicId changed from null to 8JK_KUdEQTSgCasQ4XEalA
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor-1, groupId=spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor] Resetting the last seen epoch of partition TRANSACTIONS_PENDING-0 to 0 since the associated topicId changed from null to 8JK_KUdEQTSgCasQ4XEalA
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor-2, groupId=spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor] Cluster ID: -8oBituFRZGcQwC3C1XFrQ
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor-3, groupId=spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor] Cluster ID: -8oBituFRZGcQwC3C1XFrQ
25/05/11 12:45:38 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor-1, groupId=spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor] Cluster ID: -8oBituFRZGcQwC3C1XFrQ
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor-3, groupId=spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor] Seeking to earliest offset of partition TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor-2, groupId=spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor] Seeking to earliest offset of partition TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor-1, groupId=spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor] Seeking to earliest offset of partition TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor-1, groupId=spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor] Resetting offset for partition TRANSACTIONS_PENDING-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor-3, groupId=spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor] Resetting offset for partition TRANSACTIONS_PENDING-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor-2, groupId=spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor] Resetting offset for partition TRANSACTIONS_PENDING-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor-1, groupId=spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor] Seeking to latest offset of partition TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor-2, groupId=spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor] Seeking to latest offset of partition TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor-3, groupId=spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor] Seeking to latest offset of partition TRANSACTIONS_PENDING-0
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor-3, groupId=spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor] Resetting offset for partition TRANSACTIONS_PENDING-0 to position FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor-1, groupId=spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor] Resetting offset for partition TRANSACTIONS_PENDING-0 to position FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor-2, groupId=spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor] Resetting offset for partition TRANSACTIONS_PENDING-0 to position FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.
25/05/11 12:45:38 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/11 12:45:38 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/05/11 12:45:38 INFO KafkaDataConsumer: From Kafka topicPartition=TRANSACTIONS_PENDING-0 groupId=spark-kafka-source-2da8da31-d3ed-4185-ac97-17f689a74cf2--2033533647-executor read 9 records through 1 polls (polled  out 9 records), taking 582955402 nanos, during time span of 664772935 nanos.
25/05/11 12:45:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 9344 bytes result sent to driver
25/05/11 12:45:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1248 ms on 6c276c7a973d (executor driver) (1/1)
25/05/11 12:45:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/05/11 12:45:38 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 1.479 s
25/05/11 12:45:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/11 12:45:38 INFO DataWritingSparkTask: Writer for partition 0 is committing.
25/05/11 12:45:38 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
25/05/11 12:45:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/05/11 12:45:38 INFO KafkaDataConsumer: From Kafka topicPartition=TRANSACTIONS_PENDING-0 groupId=spark-kafka-source-de2d2f6e-6fe5-4018-a344-db099b5e9b36-97115940-executor read 9 records through 1 polls (polled  out 9 records), taking 583122875 nanos, during time span of 731975153 nanos.
25/05/11 12:45:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 7171 bytes result sent to driver
25/05/11 12:45:38 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 1.581082 s
25/05/11 12:45:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1233 ms on 6c276c7a973d (executor driver) (1/1)
25/05/11 12:45:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/05/11 12:45:38 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 1.298 s
25/05/11 12:45:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/11 12:45:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/05/11 12:45:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
25/05/11 12:45:38 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 1.586958 s
25/05/11 12:45:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
-------------------------------------------
Batch: 0
-------------------------------------------
25/05/11 12:45:38 INFO CodeGenerator: Code generated in 33.14567 ms
25/05/11 12:45:38 INFO CodeGenerator: Code generated in 7.574119 ms
25/05/11 12:45:38 INFO CodeGenerator: Code generated in 8.210159 ms
25/05/11 12:45:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 6c276c7a973d:46359 in memory (size: 4.5 KiB, free: 1048.7 MiB)
25/05/11 12:45:38 INFO CodeGenerator: Code generated in 34.748431 ms
25/05/11 12:45:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 6c276c7a973d:46359 in memory (size: 12.9 KiB, free: 1048.7 MiB)
25/05/11 12:45:38 INFO CodecConfig: Compression: SNAPPY
25/05/11 12:45:39 INFO CodeGenerator: Code generated in 19.579142 ms
25/05/11 12:45:39 INFO CodecConfig: Compression: SNAPPY
25/05/11 12:45:39 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
25/05/11 12:45:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "TRANSACTION_ID",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "TIMESTAMP",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "USER_ID_HASHED",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "USER_NAME_HASHED",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PRODUCT_ID",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AMOUNT",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CURRENCY",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "TRANSACTION_TYPE",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "STATUS",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CITY",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "COUNTRY",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PAYMENT_METHOD",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PRODUCT_CATEGORY",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "QUANTITY",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "SHIPPING_STREET",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "SHIPPING_ZIP",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "SHIPPING_CITY",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "SHIPPING_COUNTRY",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DEVICE_OS",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DEVICE_BROWSER",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MASKED_IP",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CUSTOMER_RATING",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DISCOUNT_CODE",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "TAX_AMOUNT",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "THREAD",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MESSAGE_NUMBER",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "TIMESTAMP_OF_RECEPTION_LOG",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AMOUNT_USD",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ingestion_time",
    "type" : "timestamp",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary TRANSACTION_ID (STRING);
  optional binary TIMESTAMP (STRING);
  optional binary USER_ID_HASHED (STRING);
  optional binary USER_NAME_HASHED (STRING);
  optional binary PRODUCT_ID (STRING);
  optional double AMOUNT;
  optional binary CURRENCY (STRING);
  optional binary TRANSACTION_TYPE (STRING);
  optional binary STATUS (STRING);
  optional binary CITY (STRING);
  optional binary COUNTRY (STRING);
  optional binary PAYMENT_METHOD (STRING);
  optional binary PRODUCT_CATEGORY (STRING);
  optional int32 QUANTITY;
  optional binary SHIPPING_STREET (STRING);
  optional binary SHIPPING_ZIP (STRING);
  optional binary SHIPPING_CITY (STRING);
  optional binary SHIPPING_COUNTRY (STRING);
  optional binary DEVICE_OS (STRING);
  optional binary DEVICE_BROWSER (STRING);
  optional binary MASKED_IP (STRING);
  optional int32 CUSTOMER_RATING;
  optional binary DISCOUNT_CODE (STRING);
  optional double TAX_AMOUNT;
  optional int32 THREAD;
  optional int32 MESSAGE_NUMBER;
  optional binary TIMESTAMP_OF_RECEPTION_LOG (STRING);
  optional double AMOUNT_USD;
  required int96 ingestion_time;
}

       
25/05/11 12:45:39 INFO CodecPool: Got brand-new compressor [.snappy]
25/05/11 12:45:39 INFO KafkaDataConsumer: From Kafka topicPartition=TRANSACTIONS_PENDING-0 groupId=spark-kafka-source-e5827426-46c4-400b-a469-1593fc10c8c6-1776545754-executor read 9 records through 1 polls (polled  out 9 records), taking 582602337 nanos, during time span of 1713735152 nanos.
25/05/11 12:45:39 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3583 bytes result sent to driver
25/05/11 12:45:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2130 ms on 6c276c7a973d (executor driver) (1/1)
25/05/11 12:45:39 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 2.228 s
25/05/11 12:45:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/05/11 12:45:39 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/11 12:45:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
25/05/11 12:45:39 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 2.491375 s
25/05/11 12:45:39 INFO FileFormatWriter: Start to commit write Job 225e230a-219e-40c3-ba78-7d74cc670d79.
25/05/11 12:45:39 INFO FileStreamSinkLog: Set the compact interval to 10 [defaultCompactInterval: 10]
25/05/11 12:45:39 INFO CheckpointFileManager: Writing atomically to /app/data_lake/TRANSACTIONS_PENDING/_spark_metadata/0 using temp file /app/data_lake/TRANSACTIONS_PENDING/_spark_metadata/.0.274e47ae-ddfc-47f2-ab82-03e133f92b64.tmp
25/05/11 12:45:40 INFO CheckpointFileManager: Renamed temp file /app/data_lake/TRANSACTIONS_PENDING/_spark_metadata/.0.274e47ae-ddfc-47f2-ab82-03e133f92b64.tmp to /app/data_lake/TRANSACTIONS_PENDING/_spark_metadata/0
25/05/11 12:45:40 INFO ManifestFileCommitProtocol: Committed batch 0
25/05/11 12:45:40 INFO FileFormatWriter: Write Job 225e230a-219e-40c3-ba78-7d74cc670d79 committed. Elapsed time: 220 ms.
25/05/11 12:45:40 INFO FileFormatWriter: Finished processing stats for write job 225e230a-219e-40c3-ba78-7d74cc670d79.
25/05/11 12:45:40 INFO CheckpointFileManager: Writing atomically to file:/app/checkpoints/TRANSACTIONS_PENDING/commits/0 using temp file file:/app/checkpoints/TRANSACTIONS_PENDING/commits/.0.9e35626b-8088-4b3a-b7df-c835f717bf7d.tmp
25/05/11 12:45:40 INFO CheckpointFileManager: Renamed temp file file:/app/checkpoints/TRANSACTIONS_PENDING/commits/.0.9e35626b-8088-4b3a-b7df-c835f717bf7d.tmp to file:/app/checkpoints/TRANSACTIONS_PENDING/commits/0
25/05/11 12:45:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "605aad7c-5c7b-4fd5-b6fa-1dd99b2006d0",
  "runId" : "54c4e136-673c-4d22-b78d-fa0d0da45d04",
  "name" : null,
  "timestamp" : "2025-05-11T12:45:35.536Z",
  "batchId" : 0,
  "numInputRows" : 9,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 1.971522453450164,
  "durationMs" : {
    "addBatch" : 3507,
    "commitOffsets" : 50,
    "getBatch" : 36,
    "latestOffset" : 303,
    "queryPlanning" : 568,
    "triggerExecution" : 4565,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TRANSACTIONS_PENDING]]",
    "startOffset" : null,
    "endOffset" : {
      "TRANSACTIONS_PENDING" : {
        "0" : 9
      }
    },
    "latestOffset" : {
      "TRANSACTIONS_PENDING" : {
        "0" : 9
      }
    },
    "numInputRows" : 9,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 1.971522453450164,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "FileSink[/app/data_lake/TRANSACTIONS_PENDING]",
    "numOutputRows" : -1
  }
}
25/05/11 12:45:40 INFO CodeGenerator: Code generated in 11.116533 ms
25/05/11 12:45:40 INFO CodeGenerator: Code generated in 23.702431 ms
+----+--------------------+
| key|               value|
+----+--------------------+
|NULL|{"TRANSACTION_ID"...|
|NULL|{"TRANSACTION_ID"...|
|NULL|{"TRANSACTION_ID"...|
|NULL|{"TRANSACTION_ID"...|
|NULL|{"TRANSACTION_ID"...|
|NULL|{"TRANSACTION_ID"...|
|NULL|{"TRANSACTION_ID"...|
|NULL|{"TRANSACTION_ID"...|
|NULL|{"TRANSACTION_ID"...|
+----+--------------------+

25/05/11 12:45:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
+--------------+---------------------------+--------------------------------+--------------------------------+----------+------+--------+----------------+-------+--------------+---------+--------------+----------------+--------+---------------+------------+--------------+----------------+---------+--------------+---------------+---------------+-------------+----------+------+--------------+--------------------------+------------------+-----------------------+--------------+
|TRANSACTION_ID|TIMESTAMP                  |USER_ID_HASHED                  |USER_NAME_HASHED                |PRODUCT_ID|AMOUNT|CURRENCY|TRANSACTION_TYPE|STATUS |CITY          |COUNTRY  |PAYMENT_METHOD|PRODUCT_CATEGORY|QUANTITY|SHIPPING_STREET|SHIPPING_ZIP|SHIPPING_CITY |SHIPPING_COUNTRY|DEVICE_OS|DEVICE_BROWSER|MASKED_IP      |CUSTOMER_RATING|DISCOUNT_CODE|TAX_AMOUNT|THREAD|MESSAGE_NUMBER|TIMESTAMP_OF_RECEPTION_LOG|AMOUNT_USD        |ingestion_time         |ingestion_date|
+--------------+---------------------------+--------------------------------+--------------------------------+----------+------+--------+----------------+-------+--------------+---------+--------------+----------------+--------+---------------+------------+--------------+----------------+---------+--------------+---------------+---------------+-------------+----------+------+--------------+--------------------------+------------------+-----------------------+--------------+
|TXN-4aeb6c15  |2025-04-19T07:20:12.305600Z|54f57e09864b54ed59a5cc633673dde4|194cd2f711cebc782ada13e259bf61dd|PROD-669  |873.04|JPY     |payment         |pending|Salvador      |Brazil   |paypal        |food            |9       |980 Main St    |80949       |Salvador      |Brazil          |Linux    |Safari        |67.112.9xxx.xxx|NULL           |NULL         |0.0       |2     |0             |11/05/2025 12:32:55       |6.11128           |2025-05-11 12:45:35.844|2025-05-11    |
|TXN-cdf1eb54  |2025-04-17T08:57:51.305600Z|3ff77438146870ee5dcd120f13dec323|01c3012a4b4c3a3ed2aeefefafa38cc5|PROD-506  |416.12|EUR     |refund          |pending|São Paulo     |Brazil   |credit_card   |food            |9       |723 Main St    |66963       |São Paulo     |Brazil          |MacOS    |Firefox       |102.103.xxx.xxx|NULL           |NULL         |0.0       |1     |1             |11/05/2025 12:32:55       |449.4096          |2025-05-11 12:45:35.844|2025-05-11    |
|TXN-05db4e32  |2025-04-14T00:25:23.305600Z|385f7d23d00928a441e647da673b2c06|88700f697fe98f2bf0370a7114d5bacf|PROD-933  |469.41|CAD     |refund          |pending|Frankfurt     |Germany  |bank_transfer |clothing        |8       |109 Main St    |41157       |Frankfurt     |Germany         |iOS      |Firefox       |130.68.4xxx.xxx|NULL           |NULL         |0.0       |2     |2             |11/05/2025 12:32:55       |352.0575          |2025-05-11 12:45:35.844|2025-05-11    |
|TXN-3b1bb2d2  |2025-05-08T11:38:25.312495Z|7271fbbbc55316d6aab210f25eeda95c|5447aa4026288039fccf95cf270282b3|PROD-609  |776.81|GBP     |payment         |pending|Houston       |USA      |cryptocurrency|food            |3       |619 Main St    |47540       |Houston       |USA             |Linux    |Safari        |239.133.xxx.xxx|NULL           |NULL         |0.0       |3     |3             |11/05/2025 12:32:55       |963.2443999999999 |2025-05-11 12:45:35.844|2025-05-11    |
|TXN-c9f2d7da  |2025-04-27T03:14:24.305600Z|2b4f14972a7ae2a887a3bdaade7bf126|262031397020fd8df478ec13b4b096c5|PROD-711  |970.32|GBP     |withdrawal      |pending|Rio de Janeiro|Brazil   |credit_card   |electronics     |9       |948 Main St    |61337       |Rio de Janeiro|Brazil          |Linux    |Safari        |94.167.1xxx.xxx|NULL           |DISCOUNT-327 |0.0       |2     |3             |11/05/2025 12:32:55       |1203.1968000000002|2025-05-11 12:45:35.844|2025-05-11    |
|TXN-8e7c7031  |2025-04-26T08:05:23.312495Z|71deaaea048fc460df652343efe92b98|72626c2e45e149d005a0d408e80dcfea|PROD-832  |710.38|JPY     |payment         |pending|Chennai       |India    |cryptocurrency|food            |8       |948 Main St    |67917       |Chennai       |India           |iOS      |Chrome        |253.19.2xxx.xxx|3              |NULL         |0.0       |4     |5             |11/05/2025 12:32:55       |4.97266           |2025-05-11 12:45:35.844|2025-05-11    |
|TXN-50d21d0c  |2025-05-07T14:13:09.312495Z|0cf6d14a6247e5ed79930b0d9bf65651|93ea6597c3cbd06e93a46b9f5368732d|PROD-409  |796.47|GBP     |withdrawal      |pending|Brisbane      |Australia|credit_card   |home_goods      |8       |179 Main St    |73836       |Brisbane      |Australia       |Android  |Chrome        |85.130.1xxx.xxx|NULL           |DISCOUNT-406 |0.0       |4     |6             |11/05/2025 12:32:55       |987.6228          |2025-05-11 12:45:35.844|2025-05-11    |
|TXN-492b36c3  |2025-04-24T10:00:22.312495Z|526a367d768fdced0719b275252eb7b4|96db69b0fff794de363fbb73929ac604|PROD-970  |719.11|AUD     |refund          |pending|Beijing       |China    |google_pay    |home_goods      |8       |141 Main St    |87121       |Beijing       |China           |MacOS    |Safari        |2.154.10xxx.xxx|NULL           |DISCOUNT-742 |0.0       |3     |8             |11/05/2025 12:32:55       |474.61260000000004|2025-05-11 12:45:35.844|2025-05-11    |
|TXN-c61925f2  |2025-04-25T06:41:30.305600Z|2efb7d3fc84a4e198fea300197350bcb|61409aa1fd47d4a5332de23cbf59a36f|PROD-359  |281.52|CAD     |refund          |pending|Mumbai        |India    |paypal        |electronics     |7       |345 Main St    |98897       |Mumbai        |India           |iOS      |Edge          |121.211.xxx.xxx|NULL           |DISCOUNT-341 |0.0       |0     |9             |11/05/2025 12:32:55       |211.14            |2025-05-11 12:45:35.844|2025-05-11    |
+--------------+---------------------------+--------------------------------+--------------------------------+----------+------+--------+----------------+-------+--------------+---------+--------------+----------------+--------+---------------+------------+--------------+----------------+---------+--------------+---------------+---------------+-------------+----------+------+--------------+--------------------------+------------------+-----------------------+--------------+

25/05/11 12:45:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=10, truncate=false]] committed.
25/05/11 12:45:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/commits/0 using temp file file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/commits/.0.22725c58-97bd-4390-8d54-e51e68c55be3.tmp
25/05/11 12:45:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/commits/0 using temp file file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/commits/.0.98d1c3d5-147c-4c6c-9d03-fdce3efd2eb0.tmp
25/05/11 12:45:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/commits/.0.98d1c3d5-147c-4c6c-9d03-fdce3efd2eb0.tmp to file:/tmp/temporary-0b13b586-422e-466e-8b2f-fef8e01850b2/commits/0
25/05/11 12:45:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "28e0c039-ca01-45b1-b479-89987819521c",
  "runId" : "96c1a706-c5f9-4e56-814e-e360ac2d8067",
  "name" : null,
  "timestamp" : "2025-05-11T12:45:35.402Z",
  "batchId" : 0,
  "numInputRows" : 9,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 1.8075918859208675,
  "durationMs" : {
    "addBatch" : 3803,
    "commitOffsets" : 42,
    "getBatch" : 36,
    "latestOffset" : 437,
    "queryPlanning" : 568,
    "triggerExecution" : 4979,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TRANSACTIONS_PENDING]]",
    "startOffset" : null,
    "endOffset" : {
      "TRANSACTIONS_PENDING" : {
        "0" : 9
      }
    },
    "latestOffset" : {
      "TRANSACTIONS_PENDING" : {
        "0" : 9
      }
    },
    "numInputRows" : 9,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 1.8075918859208675,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@551631f",
    "numOutputRows" : 9
  }
}
25/05/11 12:45:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/commits/.0.22725c58-97bd-4390-8d54-e51e68c55be3.tmp to file:/tmp/temporary-14223007-3e44-48d6-9135-f56c6837f3bd/commits/0
25/05/11 12:45:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "b3bd168f-106f-4c93-9c13-3c50cc60c464",
  "runId" : "e668bc66-eb04-4c11-a0f2-01b7d22141b1",
  "name" : null,
  "timestamp" : "2025-05-11T12:45:34.783Z",
  "batchId" : 0,
  "numInputRows" : 9,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 1.6068559185859668,
  "durationMs" : {
    "addBatch" : 3802,
    "commitOffsets" : 46,
    "getBatch" : 36,
    "latestOffset" : 1032,
    "queryPlanning" : 570,
    "triggerExecution" : 5599,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[TRANSACTIONS_PENDING]]",
    "startOffset" : null,
    "endOffset" : {
      "TRANSACTIONS_PENDING" : {
        "0" : 9
      }
    },
    "latestOffset" : {
      "TRANSACTIONS_PENDING" : {
        "0" : 9
      }
    },
    "numInputRows" : 9,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 1.6068559185859668,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@551631f",
    "numOutputRows" : 9
  }
}
25/05/11 12:45:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:45:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:45:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
25/05/11 12:46:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
